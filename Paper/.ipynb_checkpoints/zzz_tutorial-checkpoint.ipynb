{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APMTH 207: Advanced Scientific Computing: Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "**Harvard University** <br>\n",
    "**Fall 2018** <br>\n",
    "\n",
    "### Paper Tutorial: Distilling the Knowledge in a Neural Network\n",
    "- Authors: Geoffrey Hinton, Oriol Vinyals, Jeff Dean\n",
    "- Link: https://arxiv.org/pdf/1503.02531.pdf\n",
    "\n",
    "#### Collaborators: Michelle (Chia Chi) Ho, Jiejun Lu, Jiawen Tong\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "<br>\n",
    "\n",
    "**Problem Statement & Motivation**\n",
    "\n",
    "The potential of deep neural net is increasingly realized in various industries, with which comes an increasing need to strategize model training in order to obtain powerful and deployable models. One strategy is to transfer the knowledge learned from a cumbersome but powerful (teacher) model to a smaller (student) model. Hinton et al. proposed an approach to knowledge transfer, which they coined \"distillation\", in \"Distilling the Knowledge in a Neural Network\". In this tutorial, we walk through the implementation and demonstrate the utility of knowledge distillation using the MNIST dataset and a fake dataset generated from the $\\texttt{sklearn}$ library. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Experiments on Full MNIST Data**\n",
    "\n",
    "For the MNIST dataset, we first trained a teacher network generally following the paper specifications, a neural net with 2 Dense layers, each with 1200 nodes. The teacher network made 168 errors on the test set. We then compared the performance of 3 different student networks, each composed of 2 hidden layers with 20 nodes: 1) Non-distilled student baseline, 2) Student distilled with soft targets at T=3 from the teacher; and 3) Student distilled with a weighted average bewteen soft target at T=3 and the cross-entropy loss of ground truth labels. The student networks made 368, 331 and 323 on the test set, respectively. We further trained a teacher network using a CNN model structure. The CNN teacher network showed significant improvement over the Dense teacher, making 97 errors on the test set. However, the student distilled from the CNN teacher performed only comparably to those distilled from the inferior Dense teacher net, making 340 errors.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Experiments on Partial MNIST Data (omitting 3; only keeping 7 & 8)** \n",
    "\n",
    "Having demonstrated the general utility of distillation, we proceeded to distill student networks where the transfer dataset omits certain digits. The idea is that the distillation process should allow the student to learn digits it has never seen before as the knowledge is distailled from a teacher net that has seen all the digits. We first omitted digit 3 from the transfer set and found that our best performing distilled student net was able to get 56% of the test digit 3 samples correct while the non-distilled student net did not get any of these samples right. Additionaly, when only digits 7 and 8 are included in the transfer set, our best performing distilled student net and the non-distilled student net achieved 63% and 20% overall accuracy. Our results recapitulates the paper's findings. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Distillation Applied to Fake Datasets**\n",
    "\n",
    "In the last part of this tutorial, we explored the relationship between data quality and the optimal distillation temperature. We used the $\\texttt{make_classification}$ function from the $\\texttt{sklearn}$ library to generate fake datasets with varying class separability and number of informative features. Data quality is presumed to be higher with higher class separability and higher number of informative features. At varying data quality, we compared the performance of a teacher, non-distilled student and students distilled at various temperatures. We found that, while distilled student nets showed superior model performance over non-distilled student nets in all cases, data quality did not affect the optimal distillation temperature ($T_{opt} \\in (2.5, 5)$). Based on the results reported in the paper: A $T_{opt}$ at 20, 8 and 2.5-4 was found for student nets with 800, 300 and 30 nodes per hidden layer, respectively, we suspect that optimal distillation temperature may be more correlated with student net's architecture complexity, where $T_{opt}$ is higher for more complex structures. This hypothesis remains to be tested. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Flatten, Dense, Dropout, Lambda, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import _pickle as cPickle\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "MAX_BYTES = 2**31 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation in the Math Form\n",
    "\n",
    "In general, knowledge transfer is done by transforming the logits produced by the teacher model at some temperature to a set of “soft targets” for training the student model. The authors proposed the “distillation” approach: raising the temperature of the final softmax until the teacher model produces a suitably smoother set of targets and using the same high temperature when training the student model to learn from these soft targets. \n",
    "\n",
    "$$q_i=\\dfrac{\\exp(\\frac{z_i}{T})}{\\sum_j \\exp(\\frac{z_j}{T})}$$\n",
    "\n",
    "The figure below visualizes the effect of raising temperature on the softmax layer: As temperature increases, the resulting class probability distribution \"softens\" (i.e. becomes flatter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAGECAYAAAAfuqhUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15b7729b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def softmax_temp(logit, temp):\n",
    "    return np.exp(logit/temp) / np.exp(logit/temp).sum()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "p = np.random.rand(10) * 50\n",
    "temps = [1, 10, 20, 50]\n",
    "for t in temps:\n",
    "    plt.plot(softmax_temp(p, t), label=\"T={}\".format(t))\n",
    "ax.set_title(\"Predicted Class Probability at Different Temperatures\", fontsize=16)\n",
    "ax.set_ylabel(\"Probability\", fontsize=12)\n",
    "ax.set_xlabel(\"Class\", fontsize=12)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Full MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# load mnist data & normalize to 0-1\n",
    "# 1D data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28).astype('float32')\n",
    "x_test = x_test.reshape(-1, 28*28).astype('float32')\n",
    "x_train /= 255.\n",
    "x_test /= 255. \n",
    "\n",
    "# 2D data\n",
    "(x_train_2D, _), (x_test_2D, _) = mnist.load_data()\n",
    "x_train_2D = x_train_2D.reshape(x_train_2D.shape[0], 1, 28, 28).astype('float32')\n",
    "x_test_2D = x_test_2D.reshape(x_test_2D.shape[0], 1, 28, 28).astype('float32')\n",
    "x_train_2D /= 255.\n",
    "x_test_2D /= 255.\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teacher Net \n",
    "1. DENSE 1200-1200-10 with HEAVY REGULARIZATION\n",
    "2. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "k_constraint = keras.constraints.MaxNorm(max_value=15, axis=0)\n",
    "\n",
    "mnist_dense = Sequential()\n",
    "mnist_dense.add(Dense(1200, name='hidden_1', input_shape=(28*28, ), activation='relu', kernel_initializer=k_init, kernel_constraint=k_constraint))\n",
    "mnist_dense.add(Dropout(0.7, name='dropout_1'))\n",
    "mnist_dense.add(Dense(1200, name='hidden_2', activation='relu', kernel_initializer=k_init, kernel_constraint=k_constraint))\n",
    "mnist_dense.add(Dropout(0.7, name='dropout_2'))\n",
    "mnist_dense.add(Dense(10, name='logit'))\n",
    "mnist_dense.add(Activation('softmax', name='softmax'))\n",
    "\n",
    "mnist_dense.compile(loss=categorical_crossentropy, optimizer=Adam(0.0001), metrics=['accuracy'])\n",
    "\n",
    "mnist_dense.fit(x_train, y_train, batch_size=100, epochs=20, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "loss, accuracy = mnist_dense.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DENSE TECHERT NET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "\n",
    "mnist_dense.save('./models/mnist_teacher_dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENSE TECHERT NET - On test set:\n",
      "loss = 0.055868843965092674, accuracy = 0.9832, #errors = 168\n"
     ]
    }
   ],
   "source": [
    "mnist_dense = load_model('./models/mnist_teacher_dense.h5')\n",
    "loss, accuracy = mnist_dense.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DENSE TECHERT NET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a CNN teacher net\n",
    "mnist_cnn = Sequential()\n",
    "\n",
    "mnist_cnn.add(Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(1, 28, 28), name='conv_1'))\n",
    "mnist_cnn.add(Conv2D(128, (3, 3), activation='relu', padding='same', name='conv_2'))\n",
    "mnist_cnn.add(MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_1'))\n",
    "mnist_cnn.add(Dropout(0.25, name='dropout_1'))\n",
    "\n",
    "mnist_cnn.add(Flatten())\n",
    "mnist_cnn.add(Dense(10, name='logit'))\n",
    "mnist_cnn.add(Activation('softmax', name='softmax'))\n",
    "\n",
    "mnist_cnn.compile(loss=categorical_crossentropy, optimizer=Adam(lr=0.0005), metrics=['accuracy'])\n",
    "mnist_cnn.fit(x_train_2D, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test_2D, y_test))\n",
    "\n",
    "mnist_cnn.save('./models/mnist_teacher_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN TEACHER NET - On test set:\n",
      "loss = 0.030440742732951186, accuracy = 0.9903, #errors = 97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_cnn = load_model('./models/mnist_teacher_cnn.h5')\n",
    "loss, accuracy = mnist_cnn.evaluate(x_test_2D, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('CNN TEACHER NET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Net 1\n",
    "- NO DISTILLATION BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MNIST_StudentNet(n_hidden, T):\n",
    "    '''\n",
    "    Function to build a studnet net\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "    model.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "    model.add(Dense(10, name='logit'))\n",
    "    model.add(Lambda(lambda x: x / T, name='logit_soft'))\n",
    "    model.add(Activation('softmax', name='softmax'))\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.5879 - acc: 0.8240 - val_loss: 0.2767 - val_acc: 0.9236\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2539 - acc: 0.9275 - val_loss: 0.2207 - val_acc: 0.9364\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2081 - acc: 0.9404 - val_loss: 0.2061 - val_acc: 0.9383\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1822 - acc: 0.9475 - val_loss: 0.1834 - val_acc: 0.9473\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1647 - acc: 0.9527 - val_loss: 0.1707 - val_acc: 0.9512\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1513 - acc: 0.9568 - val_loss: 0.1581 - val_acc: 0.9537\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1416 - acc: 0.9590 - val_loss: 0.1510 - val_acc: 0.9555\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1334 - acc: 0.9610 - val_loss: 0.1466 - val_acc: 0.9585\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1252 - acc: 0.9639 - val_loss: 0.1440 - val_acc: 0.9600\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1189 - acc: 0.9655 - val_loss: 0.1362 - val_acc: 0.9612\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1133 - acc: 0.9667 - val_loss: 0.1392 - val_acc: 0.9615\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1083 - acc: 0.9678 - val_loss: 0.1390 - val_acc: 0.9617\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1036 - acc: 0.9699 - val_loss: 0.1450 - val_acc: 0.9586\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1020 - acc: 0.9694 - val_loss: 0.1338 - val_acc: 0.9632\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0958 - acc: 0.9713 - val_loss: 0.1309 - val_acc: 0.9635\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0941 - acc: 0.9716 - val_loss: 0.1286 - val_acc: 0.9641\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0900 - acc: 0.9736 - val_loss: 0.1339 - val_acc: 0.9625\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0871 - acc: 0.9738 - val_loss: 0.1335 - val_acc: 0.9632\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0853 - acc: 0.9740 - val_loss: 0.1326 - val_acc: 0.9642\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0819 - acc: 0.9753 - val_loss: 0.1453 - val_acc: 0.9601\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0811 - acc: 0.9754 - val_loss: 0.1321 - val_acc: 0.9634\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0784 - acc: 0.9764 - val_loss: 0.1355 - val_acc: 0.9640\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0752 - acc: 0.9770 - val_loss: 0.1341 - val_acc: 0.9651\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0740 - acc: 0.9781 - val_loss: 0.1342 - val_acc: 0.9643\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0721 - acc: 0.9781 - val_loss: 0.1372 - val_acc: 0.9609\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0709 - acc: 0.9786 - val_loss: 0.1365 - val_acc: 0.9641\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0682 - acc: 0.9793 - val_loss: 0.1293 - val_acc: 0.9660\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0675 - acc: 0.9792 - val_loss: 0.1418 - val_acc: 0.9628\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0662 - acc: 0.9801 - val_loss: 0.1349 - val_acc: 0.9649\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0642 - acc: 0.9804 - val_loss: 0.1387 - val_acc: 0.9641\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0638 - acc: 0.9803 - val_loss: 0.1356 - val_acc: 0.9649\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0616 - acc: 0.9810 - val_loss: 0.1305 - val_acc: 0.9645\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0603 - acc: 0.9811 - val_loss: 0.1395 - val_acc: 0.9627\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0588 - acc: 0.9823 - val_loss: 0.1358 - val_acc: 0.9643\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0581 - acc: 0.9817 - val_loss: 0.1384 - val_acc: 0.9634\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0572 - acc: 0.9827 - val_loss: 0.1403 - val_acc: 0.9619\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0558 - acc: 0.9824 - val_loss: 0.1473 - val_acc: 0.9649\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0548 - acc: 0.9831 - val_loss: 0.1422 - val_acc: 0.9655\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0540 - acc: 0.9832 - val_loss: 0.1416 - val_acc: 0.9635\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0533 - acc: 0.9833 - val_loss: 0.1490 - val_acc: 0.9626\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0510 - acc: 0.9844 - val_loss: 0.1540 - val_acc: 0.9628\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0511 - acc: 0.9842 - val_loss: 0.1468 - val_acc: 0.9646\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0507 - acc: 0.9839 - val_loss: 0.1540 - val_acc: 0.9617\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0500 - acc: 0.9845 - val_loss: 0.1505 - val_acc: 0.9632\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0477 - acc: 0.9849 - val_loss: 0.1481 - val_acc: 0.9637\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0471 - acc: 0.9851 - val_loss: 0.1564 - val_acc: 0.9617\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0475 - acc: 0.9858 - val_loss: 0.1571 - val_acc: 0.9627\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0451 - acc: 0.9857 - val_loss: 0.1531 - val_acc: 0.9640\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0452 - acc: 0.9859 - val_loss: 0.1527 - val_acc: 0.9642\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0445 - acc: 0.9862 - val_loss: 0.1535 - val_acc: 0.9632\n",
      "STUDENT BASELINE - On test set:\n",
      "loss = 0.15354005151256278, accuracy = 0.9632, #errors = 368\n"
     ]
    }
   ],
   "source": [
    "# train baseline student net - NO distillation\n",
    "mnist_student_basline = MNIST_StudentNet(n_hidden=20, T=1)\n",
    "mnist_student_basline.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "mnist_student_basline.fit(x_train, y_train, batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# baseline student net model evaluation\n",
    "loss, accuracy = mnist_student_basline.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('STUDENT BASELINE - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Net 2\n",
    "- DISTILLED --- DENSE TEACHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 1.0716 - acc: 0.7981 - val_loss: 0.4087 - val_acc: 0.9109\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.7406 - acc: 0.9192 - val_loss: 0.3516 - val_acc: 0.9244\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6985 - acc: 0.9355 - val_loss: 0.3108 - val_acc: 0.9385\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6782 - acc: 0.9445 - val_loss: 0.2938 - val_acc: 0.9424\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6675 - acc: 0.9500 - val_loss: 0.2873 - val_acc: 0.9466\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6602 - acc: 0.9539 - val_loss: 0.2771 - val_acc: 0.9499\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6537 - acc: 0.9577 - val_loss: 0.2674 - val_acc: 0.9515\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6482 - acc: 0.9609 - val_loss: 0.2622 - val_acc: 0.9542\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6437 - acc: 0.9630 - val_loss: 0.2583 - val_acc: 0.9566\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6400 - acc: 0.9657 - val_loss: 0.2548 - val_acc: 0.9580\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6370 - acc: 0.9672 - val_loss: 0.2530 - val_acc: 0.9579\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6344 - acc: 0.9685 - val_loss: 0.2536 - val_acc: 0.9596\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6326 - acc: 0.9696 - val_loss: 0.2444 - val_acc: 0.9595\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6310 - acc: 0.9707 - val_loss: 0.2426 - val_acc: 0.9602\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6294 - acc: 0.9718 - val_loss: 0.2431 - val_acc: 0.9619\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6283 - acc: 0.9723 - val_loss: 0.2426 - val_acc: 0.9633\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6270 - acc: 0.9731 - val_loss: 0.2434 - val_acc: 0.9630\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6262 - acc: 0.9729 - val_loss: 0.2435 - val_acc: 0.9633\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6252 - acc: 0.9739 - val_loss: 0.2385 - val_acc: 0.9642\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6246 - acc: 0.9742 - val_loss: 0.2331 - val_acc: 0.9662\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6238 - acc: 0.9746 - val_loss: 0.2377 - val_acc: 0.9657\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6232 - acc: 0.9748 - val_loss: 0.2324 - val_acc: 0.9654\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6226 - acc: 0.9751 - val_loss: 0.2359 - val_acc: 0.9647\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6221 - acc: 0.9757 - val_loss: 0.2361 - val_acc: 0.9649\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6214 - acc: 0.9760 - val_loss: 0.2332 - val_acc: 0.9656\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6211 - acc: 0.9765 - val_loss: 0.2355 - val_acc: 0.9654\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6206 - acc: 0.9767 - val_loss: 0.2327 - val_acc: 0.9650\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6202 - acc: 0.9770 - val_loss: 0.2367 - val_acc: 0.9659\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6200 - acc: 0.9771 - val_loss: 0.2314 - val_acc: 0.9661\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6195 - acc: 0.9775 - val_loss: 0.2324 - val_acc: 0.9649\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6195 - acc: 0.9778 - val_loss: 0.2298 - val_acc: 0.9653\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6189 - acc: 0.9776 - val_loss: 0.2333 - val_acc: 0.9654\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6186 - acc: 0.9783 - val_loss: 0.2264 - val_acc: 0.9650\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6183 - acc: 0.9780 - val_loss: 0.2269 - val_acc: 0.9661\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6180 - acc: 0.9779 - val_loss: 0.2290 - val_acc: 0.9652\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6178 - acc: 0.9783 - val_loss: 0.2296 - val_acc: 0.9645\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6175 - acc: 0.9783 - val_loss: 0.2296 - val_acc: 0.9655\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6174 - acc: 0.9783 - val_loss: 0.2316 - val_acc: 0.9657\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6172 - acc: 0.9786 - val_loss: 0.2299 - val_acc: 0.9657\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6170 - acc: 0.9792 - val_loss: 0.2304 - val_acc: 0.9655\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6167 - acc: 0.9791 - val_loss: 0.2293 - val_acc: 0.9661\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6166 - acc: 0.9790 - val_loss: 0.2291 - val_acc: 0.9661\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6163 - acc: 0.9789 - val_loss: 0.2317 - val_acc: 0.9665\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6163 - acc: 0.9793 - val_loss: 0.2289 - val_acc: 0.9661\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6160 - acc: 0.9792 - val_loss: 0.2295 - val_acc: 0.9668\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6160 - acc: 0.9794 - val_loss: 0.2285 - val_acc: 0.9655\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6157 - acc: 0.9796 - val_loss: 0.2269 - val_acc: 0.9667\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6155 - acc: 0.9796 - val_loss: 0.2301 - val_acc: 0.9658\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6154 - acc: 0.9796 - val_loss: 0.2267 - val_acc: 0.9667\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6154 - acc: 0.9796 - val_loss: 0.2285 - val_acc: 0.9669\n",
      "DISTILLED STUDENT - On test set:\n",
      "loss = 0.22846840821504594, accuracy = 0.9669, #errors = 331\n"
     ]
    }
   ],
   "source": [
    "def get_layer_output(model, layer_name):\n",
    "    output = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    return output\n",
    "\n",
    "# compute 'soft target'\n",
    "T = 3\n",
    "teacher_logit = get_layer_output(mnist_dense, 'logit')\n",
    "logit_train = teacher_logit.predict(x_train)\n",
    "y_train_soft = K.softmax(logit_train / T).eval(session=K.get_session())\n",
    "\n",
    "# train student net distilled from the dense teacher net\n",
    "mnist_student_distilled = MNIST_StudentNet(n_hidden=20, T=T)\n",
    "mnist_student_distilled.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "mnist_student_distilled.fit(x_train, y_train_soft, \n",
    "                            batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# distilled student net model evaluation\n",
    "loss, accuracy = mnist_student_distilled.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DISTILLED STUDENT - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Net 3\n",
    "- DISTILLED --- DENSE TEACHER + WEIGHTED HARD/SOFT TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_mix_loss(y_true, y_pred, w, T):    \n",
    "    # split hard & soft targets\n",
    "    y_hard, y_soft = y_true[:, :10], y_true[:, 10:]\n",
    "    \n",
    "    # convert logits to predicted values\n",
    "    y_hard_pred = K.softmax(y_pred) # hard target\n",
    "    y_soft_pred = K.softmax(y_pred / T) # soft target\n",
    "    \n",
    "    # compute weighted avg of the 2 parts of losses\n",
    "    avg_loss = w * categorical_crossentropy(y_hard, y_hard_pred) + categorical_crossentropy(y_soft, y_soft_pred)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 1.0612 - acc: 0.8294 - val_loss: 0.7709 - val_acc: 0.9172\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.7508 - acc: 0.9234 - val_loss: 0.7099 - val_acc: 0.9344\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.7103 - acc: 0.9362 - val_loss: 0.6848 - val_acc: 0.9429\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.6903 - acc: 0.9442 - val_loss: 0.6726 - val_acc: 0.9472\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6769 - acc: 0.9497 - val_loss: 0.6657 - val_acc: 0.9481\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6675 - acc: 0.9533 - val_loss: 0.6559 - val_acc: 0.9543\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6604 - acc: 0.9565 - val_loss: 0.6514 - val_acc: 0.9554\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6549 - acc: 0.9590 - val_loss: 0.6480 - val_acc: 0.9580\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6504 - acc: 0.9612 - val_loss: 0.6445 - val_acc: 0.9614\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.6469 - acc: 0.9632 - val_loss: 0.6416 - val_acc: 0.9627\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.6441 - acc: 0.9647 - val_loss: 0.6406 - val_acc: 0.9614\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6418 - acc: 0.9658 - val_loss: 0.6385 - val_acc: 0.9615\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.6399 - acc: 0.9669 - val_loss: 0.6365 - val_acc: 0.9646\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6383 - acc: 0.9686 - val_loss: 0.6354 - val_acc: 0.9636\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6367 - acc: 0.9690 - val_loss: 0.6353 - val_acc: 0.9644\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6355 - acc: 0.9696 - val_loss: 0.6350 - val_acc: 0.9637\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6342 - acc: 0.9704 - val_loss: 0.6352 - val_acc: 0.9631\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.6332 - acc: 0.9707 - val_loss: 0.6333 - val_acc: 0.9655\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6323 - acc: 0.9712 - val_loss: 0.6325 - val_acc: 0.9656\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6314 - acc: 0.9715 - val_loss: 0.6317 - val_acc: 0.9651\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6308 - acc: 0.9723 - val_loss: 0.6311 - val_acc: 0.9644\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.6300 - acc: 0.9724 - val_loss: 0.6305 - val_acc: 0.9662\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6293 - acc: 0.9729 - val_loss: 0.6297 - val_acc: 0.9663\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6288 - acc: 0.9725 - val_loss: 0.6298 - val_acc: 0.9652\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6281 - acc: 0.9737 - val_loss: 0.6295 - val_acc: 0.9652\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.6277 - acc: 0.9737 - val_loss: 0.6286 - val_acc: 0.9662\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.6271 - acc: 0.9743 - val_loss: 0.6288 - val_acc: 0.9663\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.6267 - acc: 0.9744 - val_loss: 0.6287 - val_acc: 0.9655\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6265 - acc: 0.9747 - val_loss: 0.6286 - val_acc: 0.9673\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6259 - acc: 0.9748 - val_loss: 0.6272 - val_acc: 0.9674\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.6256 - acc: 0.9751 - val_loss: 0.6280 - val_acc: 0.9664\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.6250 - acc: 0.9756 - val_loss: 0.6275 - val_acc: 0.9667\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.6246 - acc: 0.9755 - val_loss: 0.6270 - val_acc: 0.9673\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.6243 - acc: 0.9757 - val_loss: 0.6258 - val_acc: 0.9676\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.6240 - acc: 0.9756 - val_loss: 0.6265 - val_acc: 0.9674\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.6236 - acc: 0.9762 - val_loss: 0.6257 - val_acc: 0.9682\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.6235 - acc: 0.9756 - val_loss: 0.6262 - val_acc: 0.9678\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.6229 - acc: 0.9767 - val_loss: 0.6287 - val_acc: 0.9661\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.6227 - acc: 0.9765 - val_loss: 0.6252 - val_acc: 0.9679\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.6223 - acc: 0.9769 - val_loss: 0.6250 - val_acc: 0.9673\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6221 - acc: 0.9772 - val_loss: 0.6259 - val_acc: 0.9669\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6218 - acc: 0.9769 - val_loss: 0.6258 - val_acc: 0.9667\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6216 - acc: 0.9774 - val_loss: 0.6243 - val_acc: 0.9689\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6213 - acc: 0.9777 - val_loss: 0.6261 - val_acc: 0.9678\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6211 - acc: 0.9779 - val_loss: 0.6247 - val_acc: 0.9673\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.6208 - acc: 0.9773 - val_loss: 0.6243 - val_acc: 0.9673\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6206 - acc: 0.9781 - val_loss: 0.6239 - val_acc: 0.9681\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6204 - acc: 0.9778 - val_loss: 0.6239 - val_acc: 0.9673\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.6202 - acc: 0.9782 - val_loss: 0.6249 - val_acc: 0.9670\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.6200 - acc: 0.9780 - val_loss: 0.6234 - val_acc: 0.9676\n",
      "DISTILLED STUDENT with WEIGHTED HARD/SOFT TARGET - On test set:\n",
      "loss = 0.6234446800231933, accuracy = 0.9676, #errors = 323\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "T = 3\n",
    "w = 0.7 / (T**2)\n",
    "\n",
    "# apply both hard & soft targets to learn\n",
    "logit_test = teacher_logit.predict(x_test)\n",
    "y_test_soft = K.softmax(logit_test / T).eval(session=K.get_session())\n",
    "y_hard_soft_train = np.concatenate((y_train, y_train_soft), axis=1)\n",
    "y_hard_soft_test = np.concatenate((y_test, y_test_soft), axis=1)\n",
    "\n",
    "# fit the student net distilled from the dense teacher net with the hard-soft weighted avg loss\n",
    "mnist_student_mix = Sequential()\n",
    "mnist_student_mix.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "mnist_student_mix.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "mnist_student_mix.add(Dense(10, name='logit'))\n",
    "\n",
    "# y_pred at the end of 10-node dense layer is the logit_pred\n",
    "mnist_student_mix.compile(loss=lambda y_true, y_pred: avg_mix_loss(y_true, y_pred, w, T), \n",
    "                          optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "mnist_student_mix.fit(x_train, y_hard_soft_train, \n",
    "                      batch_size=100, epochs=50, verbose=1, validation_data=(x_test, y_hard_soft_test))\n",
    "\n",
    "\n",
    "# distilled student net with mix-hard-soft loss model evaluation\n",
    "loss, accuracy = mnist_student_mix.evaluate(x_test, y_hard_soft_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DISTILLED STUDENT with WEIGHTED HARD/SOFT TARGET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Net 4\n",
    "- DISTILLED --- CNN TEACHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.9567 - acc: 0.7943 - val_loss: 0.3894 - val_acc: 0.9022\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.6094 - acc: 0.9093 - val_loss: 0.3409 - val_acc: 0.9193\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.5729 - acc: 0.9221 - val_loss: 0.3056 - val_acc: 0.9297\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.5469 - acc: 0.9314 - val_loss: 0.2815 - val_acc: 0.9345\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.5277 - acc: 0.9388 - val_loss: 0.2654 - val_acc: 0.9405\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.5138 - acc: 0.9443 - val_loss: 0.2596 - val_acc: 0.9394\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.5043 - acc: 0.9469 - val_loss: 0.2498 - val_acc: 0.9442\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4966 - acc: 0.9500 - val_loss: 0.2448 - val_acc: 0.9450\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.4899 - acc: 0.9527 - val_loss: 0.2362 - val_acc: 0.9482\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.4847 - acc: 0.9547 - val_loss: 0.2314 - val_acc: 0.9488\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4802 - acc: 0.9568 - val_loss: 0.2281 - val_acc: 0.9496\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4763 - acc: 0.9582 - val_loss: 0.2175 - val_acc: 0.9525\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4728 - acc: 0.9597 - val_loss: 0.2181 - val_acc: 0.9538\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4700 - acc: 0.9607 - val_loss: 0.2139 - val_acc: 0.9541\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4671 - acc: 0.9619 - val_loss: 0.2099 - val_acc: 0.9572\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4647 - acc: 0.9629 - val_loss: 0.2153 - val_acc: 0.9560\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4626 - acc: 0.9633 - val_loss: 0.2038 - val_acc: 0.9585\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4600 - acc: 0.9653 - val_loss: 0.2099 - val_acc: 0.9578\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4589 - acc: 0.9658 - val_loss: 0.1965 - val_acc: 0.9605\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4565 - acc: 0.9666 - val_loss: 0.1949 - val_acc: 0.9591\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4550 - acc: 0.9675 - val_loss: 0.1955 - val_acc: 0.9605\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4533 - acc: 0.9680 - val_loss: 0.2008 - val_acc: 0.9591\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4520 - acc: 0.9688 - val_loss: 0.1958 - val_acc: 0.9623\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4510 - acc: 0.9693 - val_loss: 0.1948 - val_acc: 0.9615\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.4496 - acc: 0.9699 - val_loss: 0.1884 - val_acc: 0.9617\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4485 - acc: 0.9703 - val_loss: 0.1899 - val_acc: 0.9618\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4473 - acc: 0.9711 - val_loss: 0.1880 - val_acc: 0.9616\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4462 - acc: 0.9716 - val_loss: 0.1957 - val_acc: 0.9606\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4456 - acc: 0.9716 - val_loss: 0.1828 - val_acc: 0.9638\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4448 - acc: 0.9725 - val_loss: 0.1899 - val_acc: 0.9607\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4443 - acc: 0.9728 - val_loss: 0.1916 - val_acc: 0.9617\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4432 - acc: 0.9729 - val_loss: 0.1911 - val_acc: 0.9619\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4423 - acc: 0.9729 - val_loss: 0.1863 - val_acc: 0.9610\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4417 - acc: 0.9737 - val_loss: 0.1847 - val_acc: 0.9640\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4413 - acc: 0.9737 - val_loss: 0.1861 - val_acc: 0.9636\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4403 - acc: 0.9746 - val_loss: 0.1855 - val_acc: 0.9633\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4396 - acc: 0.9743 - val_loss: 0.1871 - val_acc: 0.9641\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4391 - acc: 0.9742 - val_loss: 0.1858 - val_acc: 0.9645\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.4386 - acc: 0.9751 - val_loss: 0.1814 - val_acc: 0.9655\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4377 - acc: 0.9752 - val_loss: 0.1776 - val_acc: 0.9671\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4372 - acc: 0.9754 - val_loss: 0.1795 - val_acc: 0.9668\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4368 - acc: 0.9757 - val_loss: 0.1867 - val_acc: 0.9655\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4362 - acc: 0.9762 - val_loss: 0.1837 - val_acc: 0.9647\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4357 - acc: 0.9769 - val_loss: 0.1837 - val_acc: 0.9662\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4353 - acc: 0.9765 - val_loss: 0.1863 - val_acc: 0.9648\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4348 - acc: 0.9770 - val_loss: 0.1758 - val_acc: 0.9672\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4345 - acc: 0.9770 - val_loss: 0.1792 - val_acc: 0.9677\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4340 - acc: 0.9775 - val_loss: 0.1803 - val_acc: 0.9653\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.4336 - acc: 0.9773 - val_loss: 0.1770 - val_acc: 0.9668\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4335 - acc: 0.9776 - val_loss: 0.1806 - val_acc: 0.9660\n",
      "DISTILLED STUDENT from CNN TEACHER - On test set:\n",
      "loss = 0.1805808451652527, accuracy = 0.966, #errors = 340\n"
     ]
    }
   ],
   "source": [
    "# compute 'soft target' of the CNN teacher net\n",
    "T = 3\n",
    "teacher_logit_cnn = get_layer_output(mnist_cnn, 'logit')\n",
    "logit_train_cnn = teacher_logit_cnn.predict(x_train_2D)\n",
    "y_train_soft_cnn = K.softmax(logit_train_cnn / T).eval(session=K.get_session())\n",
    "\n",
    "# train student net distilled from the CNN teacher net\n",
    "mnist_student_distilled_cnn = MNIST_StudentNet(n_hidden=20, T=T)\n",
    "mnist_student_distilled_cnn.fit(x_train, y_train_soft_cnn, \n",
    "                                batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# student net model evalutation\n",
    "loss, accuracy = mnist_student_distilled_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DISTILLED STUDENT from CNN TEACHER - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Student Net Performance on Full Test Set ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.153540</td>\n",
       "      <td>368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_distilled</th>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.180581</td>\n",
       "      <td>340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.233105</td>\n",
       "      <td>331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.9676</td>\n",
       "      <td>0.623445</td>\n",
       "      <td>323.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy      loss  num_error\n",
       "baseline                    0.9632  0.153540      368.0\n",
       "cnn_distilled               0.9660  0.180581      340.0\n",
       "dense_distilled             0.9669  0.233105      331.0\n",
       "dense_distilled_mix_loss    0.9676  0.623445      323.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "student_summary = {\n",
    "    'baseline': {'loss':0.15354005151256278, 'accuracy': 0.9632, 'num_error': 368},\n",
    "    'dense_distilled': {'loss': 0.23310526382923127, 'accuracy':  0.9669, 'num_error': 331},\n",
    "    'dense_distilled_mix_loss': {'loss': 0.6234446800231933, 'accuracy': 0.9676, 'num_error': 323},\n",
    "    'cnn_distilled': {'loss': 0.1805808451652527, 'accuracy': 0.966, 'num_error': 340}\n",
    "}\n",
    "\n",
    "print('=== Student Net Performance on Full Test Set ===')\n",
    "df_student_summary = pd.DataFrame().from_dict(student_summary).T\n",
    "display(df_student_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions - Experiments on Full MNIST Data\n",
    "\n",
    "In the paper, the digits were first jittered by up to two pixels in any direction to make the classification task more difficult so that the teacher and the student would show observable differences in their learning capacities. Without jittering the digits in our implementation, we found that a student net with relatively complex structure (E.g., 800-800-10) can learn from the ground truth labels very well without distillation. Therefore, to better show the effect of distillation, we chose a much simpler student net structure: 20-20-10. \n",
    "\n",
    "For the teacher net, we trained a CNN model in addition to the Dense model (1200-1200-10) reported in the paper because CNN structure is the conventional and more effective method to handle image data. Indeed, we found that the Dense and CNN teacher net achieved a test accuracy = 0.9832 (168 errors) and 0.9903 (97 errors), respectively. While non-distilled student baseline achieved an accuracy = 0.9632 (368 errors), learning from the Dense and the CNN teacher at T=3, the distilled students achieved an accuracy = 0.9669 (331 errors) and 0.9660 (340 errors), respectively. This demonstrates that 1) distillation improves student model performance; and 2) Distilling from a superior model (CNN teacher) does not necessarily translate into superior student model performance. By learning from both the ground truth label and the Dense teacher (with the ground truth weighted more heavily at a weight = 0.7), the performance of the distilled student further improved to a test accuracy = 0.9676 (323 errors).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment on Partial MNIST Data -- omitting digit 3 in the transfer set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we’ve shown that the distilled student net performs better than one without distillation. In the following, our experiments showed that the distilled student can even learn from a transfer set without all of the digits, 0-9. When omitting or keeping certain digits from the transfer set, a non-distilled student net cannot generalize and is unable to understand digits it has never seen. However, distillation presumably enables the student to learn from the teacher's knowledge on how to distinguish all of the digits with the teacher's soft target. The amount of information from the teacher's soft target is controlled by the temperature parameter.\n",
    "\n",
    "In HW7, we built an MLP model on MNIST. We found that on the training set, digit 3 is one of the most easily misclassified label as it is similar with digit 5, 8 and 2. A well trained teacher net on the full training data is expected to have captured the characteristics of digit 3 and its relative similarity with other labels. In the following experiment, we omitted all examples of digit 3 from the transfer set and trained three student nets: 1) non-distilled baseline, 2) distilled at T = 3, and 3) distilled at T = 3 with a mixture of ground truth and teacher soft target losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mis_hist.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = (y_train[:,3] == 1)\n",
    "x_train_omit3 = x_train[~idx,:]\n",
    "y_train_omit3 = y_train[~idx,:]\n",
    "y_train_soft_omit3 = y_train_soft[~idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.5718 - acc: 0.8305 - val_loss: 1.4462 - val_acc: 0.8312\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.2327 - acc: 0.9334 - val_loss: 1.5448 - val_acc: 0.8428\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.1900 - acc: 0.9450 - val_loss: 1.5695 - val_acc: 0.8502\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.1638 - acc: 0.9522 - val_loss: 1.5985 - val_acc: 0.8567\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.1444 - acc: 0.9577 - val_loss: 1.5859 - val_acc: 0.8601\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.1322 - acc: 0.9611 - val_loss: 1.6107 - val_acc: 0.8616\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.1217 - acc: 0.9641 - val_loss: 1.6145 - val_acc: 0.8633\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.1145 - acc: 0.9662 - val_loss: 1.6117 - val_acc: 0.8652\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.1081 - acc: 0.9681 - val_loss: 1.6300 - val_acc: 0.8622\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.1016 - acc: 0.9694 - val_loss: 1.6463 - val_acc: 0.8649\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0965 - acc: 0.9709 - val_loss: 1.6483 - val_acc: 0.8636\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0929 - acc: 0.9717 - val_loss: 1.6629 - val_acc: 0.8655\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0877 - acc: 0.9738 - val_loss: 1.6667 - val_acc: 0.8635\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0847 - acc: 0.9742 - val_loss: 1.6872 - val_acc: 0.8669\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0804 - acc: 0.9756 - val_loss: 1.6984 - val_acc: 0.8665\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0775 - acc: 0.9766 - val_loss: 1.6992 - val_acc: 0.8680\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0747 - acc: 0.9766 - val_loss: 1.7104 - val_acc: 0.8680\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0711 - acc: 0.9783 - val_loss: 1.7139 - val_acc: 0.8679\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0691 - acc: 0.9787 - val_loss: 1.7228 - val_acc: 0.8677\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0660 - acc: 0.9800 - val_loss: 1.7193 - val_acc: 0.8701\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0633 - acc: 0.9799 - val_loss: 1.7208 - val_acc: 0.8686\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0618 - acc: 0.9815 - val_loss: 1.7233 - val_acc: 0.8697\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 18us/step - loss: 0.0598 - acc: 0.9815 - val_loss: 1.7272 - val_acc: 0.8677\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0578 - acc: 0.9818 - val_loss: 1.7293 - val_acc: 0.8677\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0548 - acc: 0.9834 - val_loss: 1.7383 - val_acc: 0.8668\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0545 - acc: 0.9836 - val_loss: 1.7298 - val_acc: 0.8691\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0523 - acc: 0.9842 - val_loss: 1.7430 - val_acc: 0.8657\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0515 - acc: 0.9841 - val_loss: 1.7350 - val_acc: 0.8688\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0493 - acc: 0.9849 - val_loss: 1.7353 - val_acc: 0.8686\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0479 - acc: 0.9853 - val_loss: 1.7355 - val_acc: 0.8700\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0470 - acc: 0.9858 - val_loss: 1.7420 - val_acc: 0.8676\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0452 - acc: 0.9863 - val_loss: 1.7394 - val_acc: 0.8685\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0442 - acc: 0.9868 - val_loss: 1.7468 - val_acc: 0.8670\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0435 - acc: 0.9869 - val_loss: 1.7416 - val_acc: 0.8680\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0409 - acc: 0.9881 - val_loss: 1.7465 - val_acc: 0.8687\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0403 - acc: 0.9881 - val_loss: 1.7444 - val_acc: 0.8693\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0384 - acc: 0.9883 - val_loss: 1.7475 - val_acc: 0.8678\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0375 - acc: 0.9892 - val_loss: 1.7519 - val_acc: 0.8675\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0373 - acc: 0.9888 - val_loss: 1.7482 - val_acc: 0.8689\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0368 - acc: 0.9890 - val_loss: 1.7483 - val_acc: 0.8682\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0352 - acc: 0.9892 - val_loss: 1.7561 - val_acc: 0.8663\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0348 - acc: 0.9893 - val_loss: 1.7549 - val_acc: 0.8667\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0329 - acc: 0.9904 - val_loss: 1.7599 - val_acc: 0.8671\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0323 - acc: 0.9900 - val_loss: 1.7509 - val_acc: 0.8676\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0328 - acc: 0.9902 - val_loss: 1.7590 - val_acc: 0.8666\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0311 - acc: 0.9910 - val_loss: 1.7658 - val_acc: 0.8655\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0299 - acc: 0.9912 - val_loss: 1.7627 - val_acc: 0.8659\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0302 - acc: 0.9908 - val_loss: 1.7592 - val_acc: 0.8679\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0284 - acc: 0.9916 - val_loss: 1.7679 - val_acc: 0.8660\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0279 - acc: 0.9921 - val_loss: 1.7655 - val_acc: 0.8672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1414863c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student net - NO distillation - digit 3 omitted in the transfer set\n",
    "hard_omit3 = MNIST_StudentNet(n_hidden=20, T=1)\n",
    "hard_omit3.fit(x_train_omit3, y_train_omit3, \n",
    "               batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 1.0962 - acc: 0.7889 - val_loss: 0.6145 - val_acc: 0.8243\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.7442 - acc: 0.9269 - val_loss: 0.5496 - val_acc: 0.8405\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.7057 - acc: 0.9430 - val_loss: 0.4999 - val_acc: 0.8513\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6878 - acc: 0.9499 - val_loss: 0.4911 - val_acc: 0.8571\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6771 - acc: 0.9551 - val_loss: 0.4655 - val_acc: 0.8616\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6704 - acc: 0.9578 - val_loss: 0.4524 - val_acc: 0.8645\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6653 - acc: 0.9606 - val_loss: 0.4447 - val_acc: 0.8675\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6608 - acc: 0.9632 - val_loss: 0.4328 - val_acc: 0.8713\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6565 - acc: 0.9650 - val_loss: 0.4193 - val_acc: 0.8753\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6533 - acc: 0.9669 - val_loss: 0.4183 - val_acc: 0.8777\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6504 - acc: 0.9685 - val_loss: 0.4134 - val_acc: 0.8789\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6480 - acc: 0.9699 - val_loss: 0.4059 - val_acc: 0.8821\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6458 - acc: 0.9714 - val_loss: 0.4049 - val_acc: 0.8826\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6441 - acc: 0.9720 - val_loss: 0.3988 - val_acc: 0.8823\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6426 - acc: 0.9730 - val_loss: 0.3940 - val_acc: 0.8848\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6409 - acc: 0.9736 - val_loss: 0.3915 - val_acc: 0.8866\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6398 - acc: 0.9745 - val_loss: 0.3854 - val_acc: 0.8872\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6388 - acc: 0.9750 - val_loss: 0.3863 - val_acc: 0.8886\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6375 - acc: 0.9760 - val_loss: 0.3758 - val_acc: 0.8916\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6365 - acc: 0.9770 - val_loss: 0.3767 - val_acc: 0.8915\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6356 - acc: 0.9770 - val_loss: 0.3728 - val_acc: 0.8965\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6349 - acc: 0.9773 - val_loss: 0.3664 - val_acc: 0.9002\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6341 - acc: 0.9784 - val_loss: 0.3617 - val_acc: 0.9027\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6334 - acc: 0.9782 - val_loss: 0.3583 - val_acc: 0.9031\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6328 - acc: 0.9788 - val_loss: 0.3489 - val_acc: 0.9081\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6319 - acc: 0.9790 - val_loss: 0.3496 - val_acc: 0.9086\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6313 - acc: 0.9794 - val_loss: 0.3508 - val_acc: 0.9084\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6308 - acc: 0.9798 - val_loss: 0.3507 - val_acc: 0.9052\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6302 - acc: 0.9801 - val_loss: 0.3413 - val_acc: 0.9078\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6298 - acc: 0.9807 - val_loss: 0.3487 - val_acc: 0.9068\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6296 - acc: 0.9808 - val_loss: 0.3403 - val_acc: 0.9142\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6289 - acc: 0.9813 - val_loss: 0.3436 - val_acc: 0.9110\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6287 - acc: 0.9813 - val_loss: 0.3386 - val_acc: 0.9121\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6283 - acc: 0.9812 - val_loss: 0.3337 - val_acc: 0.9155\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6280 - acc: 0.9815 - val_loss: 0.3361 - val_acc: 0.9146\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 1s 22us/step - loss: 0.6278 - acc: 0.9815 - val_loss: 0.3369 - val_acc: 0.9131\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6275 - acc: 0.9816 - val_loss: 0.3331 - val_acc: 0.9193\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6272 - acc: 0.9821 - val_loss: 0.3319 - val_acc: 0.9169\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6269 - acc: 0.9825 - val_loss: 0.3359 - val_acc: 0.9155\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6267 - acc: 0.9827 - val_loss: 0.3412 - val_acc: 0.9116\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6264 - acc: 0.9827 - val_loss: 0.3251 - val_acc: 0.9196\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6262 - acc: 0.9828 - val_loss: 0.3253 - val_acc: 0.9208\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6260 - acc: 0.9828 - val_loss: 0.3274 - val_acc: 0.9193\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6258 - acc: 0.9829 - val_loss: 0.3256 - val_acc: 0.9199\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6257 - acc: 0.9829 - val_loss: 0.3275 - val_acc: 0.9194\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6255 - acc: 0.9833 - val_loss: 0.3278 - val_acc: 0.9193\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6252 - acc: 0.9831 - val_loss: 0.3219 - val_acc: 0.9241\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6251 - acc: 0.9835 - val_loss: 0.3236 - val_acc: 0.9204\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6250 - acc: 0.9835 - val_loss: 0.3262 - val_acc: 0.9193\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6249 - acc: 0.9834 - val_loss: 0.3251 - val_acc: 0.9209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x144202080>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student net - WITH distillation - digit 3 omitted in the transfer set\n",
    "soft_omit3 = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "soft_omit3.fit(x_train_omit3, y_train_soft_omit3, \n",
    "               batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 2s 35us/step - loss: 1.1521 - acc: 0.7983 - val_loss: 1.8253 - val_acc: 0.8206\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 1s 22us/step - loss: 0.7695 - acc: 0.9210 - val_loss: 1.7625 - val_acc: 0.8422\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.7313 - acc: 0.9334 - val_loss: 1.7412 - val_acc: 0.8504\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.7094 - acc: 0.9420 - val_loss: 1.7284 - val_acc: 0.8591\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 1s 22us/step - loss: 0.6928 - acc: 0.9492 - val_loss: 1.7250 - val_acc: 0.8649\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6775 - acc: 0.9551 - val_loss: 1.7105 - val_acc: 0.8715\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6686 - acc: 0.9590 - val_loss: 1.7007 - val_acc: 0.8738\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6630 - acc: 0.9617 - val_loss: 1.6886 - val_acc: 0.8790\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6585 - acc: 0.9640 - val_loss: 1.6837 - val_acc: 0.8815\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6550 - acc: 0.9658 - val_loss: 1.6768 - val_acc: 0.8859\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6520 - acc: 0.9674 - val_loss: 1.6704 - val_acc: 0.8914\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6494 - acc: 0.9685 - val_loss: 1.6702 - val_acc: 0.8911\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6472 - acc: 0.9693 - val_loss: 1.6622 - val_acc: 0.8959\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6452 - acc: 0.9702 - val_loss: 1.6643 - val_acc: 0.8959\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6437 - acc: 0.9714 - val_loss: 1.6519 - val_acc: 0.8992\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6421 - acc: 0.9722 - val_loss: 1.6475 - val_acc: 0.9019\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6406 - acc: 0.9725 - val_loss: 1.6404 - val_acc: 0.9053\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6394 - acc: 0.9732 - val_loss: 1.6502 - val_acc: 0.9065\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6385 - acc: 0.9738 - val_loss: 1.6557 - val_acc: 0.9026\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6374 - acc: 0.9744 - val_loss: 1.6493 - val_acc: 0.9068\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6364 - acc: 0.9749 - val_loss: 1.6492 - val_acc: 0.9042\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6356 - acc: 0.9751 - val_loss: 1.6427 - val_acc: 0.9058\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6350 - acc: 0.9752 - val_loss: 1.6436 - val_acc: 0.9111\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6344 - acc: 0.9754 - val_loss: 1.6318 - val_acc: 0.9129\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6337 - acc: 0.9759 - val_loss: 1.6332 - val_acc: 0.9091\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6333 - acc: 0.9760 - val_loss: 1.6398 - val_acc: 0.9104\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6328 - acc: 0.9769 - val_loss: 1.6377 - val_acc: 0.9098\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6322 - acc: 0.9773 - val_loss: 1.6377 - val_acc: 0.9081\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6319 - acc: 0.9772 - val_loss: 1.6340 - val_acc: 0.9119\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6316 - acc: 0.9773 - val_loss: 1.6395 - val_acc: 0.9145\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6311 - acc: 0.9775 - val_loss: 1.6440 - val_acc: 0.9094\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6308 - acc: 0.9777 - val_loss: 1.6340 - val_acc: 0.9124\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6304 - acc: 0.9782 - val_loss: 1.6435 - val_acc: 0.9133\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6302 - acc: 0.9785 - val_loss: 1.6213 - val_acc: 0.9146\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6299 - acc: 0.9788 - val_loss: 1.6412 - val_acc: 0.9125\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6297 - acc: 0.9790 - val_loss: 1.6372 - val_acc: 0.9138\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6292 - acc: 0.9790 - val_loss: 1.6357 - val_acc: 0.9134\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6291 - acc: 0.9794 - val_loss: 1.6270 - val_acc: 0.9179\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6288 - acc: 0.9795 - val_loss: 1.6284 - val_acc: 0.9152\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6286 - acc: 0.9799 - val_loss: 1.6206 - val_acc: 0.9209\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6282 - acc: 0.9797 - val_loss: 1.6288 - val_acc: 0.9181\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6280 - acc: 0.9801 - val_loss: 1.6243 - val_acc: 0.9159\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6279 - acc: 0.9800 - val_loss: 1.6307 - val_acc: 0.9166\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6276 - acc: 0.9804 - val_loss: 1.6276 - val_acc: 0.9208\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6274 - acc: 0.9799 - val_loss: 1.6269 - val_acc: 0.9196\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6271 - acc: 0.9804 - val_loss: 1.6314 - val_acc: 0.9160\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6270 - acc: 0.9804 - val_loss: 1.6295 - val_acc: 0.9166\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6268 - acc: 0.9808 - val_loss: 1.6288 - val_acc: 0.9212\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6267 - acc: 0.9809 - val_loss: 1.6163 - val_acc: 0.9243\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6265 - acc: 0.9810 - val_loss: 1.6204 - val_acc: 0.9251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15535fdd8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "T = 5\n",
    "w = 0.4 / (T**2) \n",
    "\n",
    "# apply both hard & soft targets to learn\n",
    "y_hard_soft_train_omit3 = np.concatenate((y_train_omit3, y_train_soft_omit3), axis=1)\n",
    "\n",
    "# student net - WITH distillation & weighted hard soft loss - digit 3 omitted in the transfer set\n",
    "mix_omit3 = Sequential()\n",
    "mix_omit3.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "mix_omit3.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "mix_omit3.add(Dense(10, name='logit'))\n",
    "\n",
    "# y_pred at the end of 10-node dense layer is the logit_pred\n",
    "mix_omit3.compile(loss=lambda y_true, y_pred: avg_mix_loss(y_true, y_pred, w, T), \n",
    "                  optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "mix_omit3.fit(x_train_omit3, y_hard_soft_train_omit3, \n",
    "              batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_hard_soft_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Accuracy on Test set === \n",
      "\n",
      "NO DISTILLATION\n",
      "loss = 1.7654574642470455, accuracy = 0.8672, #errors = 1328\n",
      "\n",
      "WITH DISTILLATION\n",
      "loss = 0.3250568591117859, accuracy = 0.9209, #errors = 790\n",
      "\n",
      "WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET\n",
      "loss = 1.62039946975708, accuracy = 0.9251, #errors = 748\n"
     ]
    }
   ],
   "source": [
    "print('=== Overall Accuracy on Test set === \\n')\n",
    "\n",
    "loss, accuracy = hard_omit3.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('NO DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION')\n",
    "loss, accuracy = soft_omit3.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET')\n",
    "loss, accuracy = mix_omit3.evaluate(x_test, y_hard_soft_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Digit-3 Only Accuracy on Test set === \n",
      "\n",
      "NO DISTILLATION\n",
      "loss = 16.11809539794922, accuracy = 0.0, #errors = 1010\n",
      "\n",
      "WITH DISTILLATION\n",
      "loss = 1.238349964595077, accuracy = 0.4811881188708957, #errors = 523\n",
      "\n",
      "WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET\n",
      "loss = 1.2482987630485307, accuracy = 0.5217821782473291, #errors = 482\n"
     ]
    }
   ],
   "source": [
    "print('=== Digit-3 Only Accuracy on Test set === \\n')\n",
    "\n",
    "idx2 = y_test[:,3] == 1\n",
    "x_test_3 = x_test[idx2, :]\n",
    "y_test_3 = y_test[idx2, :]\n",
    "y_test_soft_3 = y_test[idx2,:]\n",
    "y_hard_soft_test_3 = np.concatenate((y_test_3, y_test_soft_3), axis=1)\n",
    "\n",
    "loss, accuracy = hard_omit3.evaluate(x_test_3, y_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test_3))\n",
    "print('NO DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "loss, accuracy = soft_omit3.evaluate(x_test_3, y_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test_3))\n",
    "print('WITH DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "loss, accuracy = mix_omit3.evaluate(x_test_3, y_hard_soft_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test_3))\n",
    "print('WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 2s 37us/step - loss: 1.0913 - acc: 0.7864 - val_loss: 0.6302 - val_acc: 0.8288\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.7331 - acc: 0.9324 - val_loss: 0.5332 - val_acc: 0.8457\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6994 - acc: 0.9460 - val_loss: 0.4937 - val_acc: 0.8550\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6826 - acc: 0.9532 - val_loss: 0.4508 - val_acc: 0.8640\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6719 - acc: 0.9577 - val_loss: 0.4296 - val_acc: 0.8719\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6637 - acc: 0.9625 - val_loss: 0.4201 - val_acc: 0.8765\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6577 - acc: 0.9650 - val_loss: 0.4011 - val_acc: 0.8844\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6529 - acc: 0.9670 - val_loss: 0.3869 - val_acc: 0.8913\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6495 - acc: 0.9695 - val_loss: 0.3834 - val_acc: 0.8915\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6468 - acc: 0.9704 - val_loss: 0.3866 - val_acc: 0.8907\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6442 - acc: 0.9716 - val_loss: 0.3738 - val_acc: 0.8960\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6422 - acc: 0.9729 - val_loss: 0.3658 - val_acc: 0.9011\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6403 - acc: 0.9740 - val_loss: 0.3579 - val_acc: 0.9044\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6387 - acc: 0.9742 - val_loss: 0.3533 - val_acc: 0.9056\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6374 - acc: 0.9757 - val_loss: 0.3516 - val_acc: 0.9055\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6360 - acc: 0.9762 - val_loss: 0.3447 - val_acc: 0.9064\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6350 - acc: 0.9767 - val_loss: 0.3408 - val_acc: 0.9086\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6342 - acc: 0.9770 - val_loss: 0.3387 - val_acc: 0.9128\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6332 - acc: 0.9779 - val_loss: 0.3382 - val_acc: 0.9123\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6326 - acc: 0.9781 - val_loss: 0.3418 - val_acc: 0.9077\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6321 - acc: 0.9782 - val_loss: 0.3351 - val_acc: 0.9116\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6314 - acc: 0.9786 - val_loss: 0.3235 - val_acc: 0.9191\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6308 - acc: 0.9794 - val_loss: 0.3309 - val_acc: 0.9188\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6303 - acc: 0.9798 - val_loss: 0.3320 - val_acc: 0.9161\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6299 - acc: 0.9795 - val_loss: 0.3194 - val_acc: 0.9185\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6295 - acc: 0.9802 - val_loss: 0.3204 - val_acc: 0.9229\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6290 - acc: 0.9807 - val_loss: 0.3235 - val_acc: 0.9187\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6287 - acc: 0.9805 - val_loss: 0.3236 - val_acc: 0.9182\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6284 - acc: 0.9811 - val_loss: 0.3200 - val_acc: 0.9191\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6279 - acc: 0.9813 - val_loss: 0.3181 - val_acc: 0.9221\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6277 - acc: 0.9813 - val_loss: 0.3238 - val_acc: 0.9214\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6275 - acc: 0.9807 - val_loss: 0.3182 - val_acc: 0.9252\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6271 - acc: 0.9815 - val_loss: 0.3213 - val_acc: 0.9221\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6269 - acc: 0.9816 - val_loss: 0.3217 - val_acc: 0.9222\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6267 - acc: 0.9817 - val_loss: 0.3196 - val_acc: 0.9223\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6264 - acc: 0.9818 - val_loss: 0.3135 - val_acc: 0.9250\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6262 - acc: 0.9822 - val_loss: 0.3155 - val_acc: 0.9202\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6262 - acc: 0.9822 - val_loss: 0.3167 - val_acc: 0.9218\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6258 - acc: 0.9824 - val_loss: 0.3174 - val_acc: 0.9242\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6258 - acc: 0.9824 - val_loss: 0.3134 - val_acc: 0.9241\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6255 - acc: 0.9824 - val_loss: 0.3152 - val_acc: 0.9237\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6254 - acc: 0.9823 - val_loss: 0.3203 - val_acc: 0.9203\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6251 - acc: 0.9827 - val_loss: 0.3136 - val_acc: 0.9231\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6252 - acc: 0.9827 - val_loss: 0.3107 - val_acc: 0.9272\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6249 - acc: 0.9825 - val_loss: 0.3131 - val_acc: 0.9262\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6247 - acc: 0.9829 - val_loss: 0.3165 - val_acc: 0.9262\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6248 - acc: 0.9831 - val_loss: 0.3137 - val_acc: 0.9291\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6245 - acc: 0.9830 - val_loss: 0.3144 - val_acc: 0.9255\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6244 - acc: 0.9831 - val_loss: 0.3188 - val_acc: 0.9200\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6243 - acc: 0.9833 - val_loss: 0.3122 - val_acc: 0.9293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154a78c88>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune bias\n",
    "soft_omit3_bias = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "bias = soft_omit3_bias.layers[2].get_weights()[1]\n",
    "bias[3] = 3.5\n",
    "K.set_value(soft_omit3_bias.layers[2].bias, bias)\n",
    "\n",
    "soft_omit3_bias.fit(x_train_omit3, y_train_soft_omit3, \n",
    "                    batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BIAS TUNED - Overall Accuracy on Test set === \n",
      "\n",
      "loss = 0.312171789765358, accuracy = 0.9293, #errors = 706\n",
      "\n",
      "=== BIAS TUNED - Digit-3 Only Accuracy on Test set === \n",
      "\n",
      "loss = 1.102321712805493, accuracy = 0.5613861386728759, #errors = 442\n"
     ]
    }
   ],
   "source": [
    "print('=== BIAS TUNED - Overall Accuracy on Test set === \\n')\n",
    "loss, accuracy = soft_omit3_bias.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('=== BIAS TUNED - Digit-3 Only Accuracy on Test set === \\n')\n",
    "loss, accuracy = soft_omit3_bias.evaluate(x_test_3, y_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test_3))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Omit3 Distillation Performance on Full Test Set ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.8672</td>\n",
       "      <td>1.765457</td>\n",
       "      <td>1328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_tuned</th>\n",
       "      <td>0.9293</td>\n",
       "      <td>0.312172</td>\n",
       "      <td>706.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.9209</td>\n",
       "      <td>0.325057</td>\n",
       "      <td>790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.9251</td>\n",
       "      <td>1.620399</td>\n",
       "      <td>748.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy      loss  num_error\n",
       "baseline                    0.8672  1.765457     1328.0\n",
       "bias_tuned                  0.9293  0.312172      706.0\n",
       "dense_distilled             0.9209  0.325057      790.0\n",
       "dense_distilled_mix_loss    0.9251  1.620399      748.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Omit3 Distillation Performance on Digit 3 Test Samples ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.118095</td>\n",
       "      <td>1010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_tuned</th>\n",
       "      <td>0.561386</td>\n",
       "      <td>1.102322</td>\n",
       "      <td>442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.481188</td>\n",
       "      <td>1.238350</td>\n",
       "      <td>523.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.521782</td>\n",
       "      <td>1.248299</td>\n",
       "      <td>482.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy       loss  num_error\n",
       "baseline                  0.000000  16.118095     1010.0\n",
       "bias_tuned                0.561386   1.102322      442.0\n",
       "dense_distilled           0.481188   1.238350      523.0\n",
       "dense_distilled_mix_loss  0.521782   1.248299      482.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "omit3_summary_overall = {\n",
    "    'baseline': {'loss': 1.7654574642470455, 'accuracy': 0.8672, 'num_error': 1328},\n",
    "    'dense_distilled': {'loss': 0.3250568591117859, 'accuracy': 0.9209, 'num_error': 790},\n",
    "    'dense_distilled_mix_loss': {'loss': 1.62039946975708, 'accuracy': 0.9251, 'num_error': 748},\n",
    "    'bias_tuned': {'loss': 0.312171789765358, 'accuracy': 0.9293, 'num_error': 706}\n",
    "}\n",
    "omit3_summary_3only = {\n",
    "    'baseline': {'loss': 16.11809539794922, 'accuracy': 0.0, 'num_error': 1010},\n",
    "    'dense_distilled': {'loss': 1.238349964595077, 'accuracy': 0.4811881188708957, 'num_error': 523},\n",
    "    'dense_distilled_mix_loss': {'loss': 1.2482987630485307, 'accuracy': 0.5217821782473291, 'num_error': 482},\n",
    "    'bias_tuned': {'loss': 1.102321712805493, 'accuracy': 0.5613861386728759, 'num_error': 442}\n",
    "}\n",
    "\n",
    "df_omit3_summary_overall = pd.DataFrame().from_dict(omit3_summary_overall).T\n",
    "df_omit3_summary_3only = pd.DataFrame().from_dict(omit3_summary_3only).T\n",
    "\n",
    "print('=== Omit3 Distillation Performance on Full Test Set ===')\n",
    "display(df_omit3_summary_overall)\n",
    "print()\n",
    "\n",
    "print('=== Omit3 Distillation Performance on Digit 3 Test Samples ===')\n",
    "display(df_omit3_summary_3only)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions - distillation performance of student nets trained on data sets without digit 3\n",
    "\n",
    "Comparing the accuracy on the full test data and on the digit 3 only subset, we observed the worst performance from the non-distilled student baseline test accuracy = 86.8% overall and 0% on 3-only set. With distillation, the student net showed an improved accuracy = 92.1% on full test set and 48.1% on 3-only set. According to the paper, most of the errors are caused by an inappropriate bias for class 3. After adjusting this bias to 3.5 (which optimizes overall accuracy), the distilled model further increased its accuracy to 92.9% and 56.1% on 3-only set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment on Partial MNIST Data -- keeping only digit 7 and 8 in the transfer set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the misclassification distributions of all digits HW7, 7 and 8 with the digits that they are mostly likely being misclassified as cover all class labels. In the following, we kept only digit 7 & 8 in the transfer set and showed results of distillation on 3 student nets: 1) a non-distilled baseline, 2) a distilled student with tuned bias and 3) a distilled student with a mixture of hard and soft losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep 7 and 8 in transfer set\n",
    "idx78 = [True if y[7]==1 or y[8] == 1 else False for y in y_train]\n",
    "\n",
    "x_train_78 = x_train[idx78,:]\n",
    "y_train_soft_78 = y_train_soft[idx78,:]\n",
    "y_train_78 = y_train[idx78,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12116/12116 [==============================] - 2s 161us/step - loss: 0.3170 - acc: 0.9272 - val_loss: 8.2521 - val_acc: 0.1965\n",
      "Epoch 2/50\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.0342 - acc: 0.9889 - val_loss: 9.1068 - val_acc: 0.1973\n",
      "Epoch 3/50\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.0259 - acc: 0.9919 - val_loss: 9.5551 - val_acc: 0.1976\n",
      "Epoch 4/50\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.0220 - acc: 0.9931 - val_loss: 9.8298 - val_acc: 0.1974\n",
      "Epoch 5/50\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.0201 - acc: 0.9931 - val_loss: 10.0648 - val_acc: 0.1980\n",
      "Epoch 6/50\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.0183 - acc: 0.9941 - val_loss: 10.2347 - val_acc: 0.1979\n",
      "Epoch 7/50\n",
      "12116/12116 [==============================] - 1s 99us/step - loss: 0.0177 - acc: 0.9938 - val_loss: 10.3479 - val_acc: 0.1977\n",
      "Epoch 8/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0162 - acc: 0.9948 - val_loss: 10.4804 - val_acc: 0.1980\n",
      "Epoch 9/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0146 - acc: 0.9953 - val_loss: 10.5505 - val_acc: 0.1983\n",
      "Epoch 10/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0140 - acc: 0.9955 - val_loss: 10.6187 - val_acc: 0.1985\n",
      "Epoch 11/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.0148 - acc: 0.9954 - val_loss: 10.6355 - val_acc: 0.1977\n",
      "Epoch 12/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.0123 - acc: 0.9962 - val_loss: 10.7293 - val_acc: 0.1985\n",
      "Epoch 13/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0119 - acc: 0.9960 - val_loss: 10.7953 - val_acc: 0.1981\n",
      "Epoch 14/50\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.0110 - acc: 0.9960 - val_loss: 10.8560 - val_acc: 0.1989\n",
      "Epoch 15/50\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.0102 - acc: 0.9968 - val_loss: 10.8845 - val_acc: 0.1985\n",
      "Epoch 16/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.0084 - acc: 0.9971 - val_loss: 10.9419 - val_acc: 0.1988\n",
      "Epoch 17/50\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 11.0060 - val_acc: 0.1983\n",
      "Epoch 18/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.0073 - acc: 0.9974 - val_loss: 11.0178 - val_acc: 0.1986\n",
      "Epoch 19/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.0067 - acc: 0.9977 - val_loss: 11.0525 - val_acc: 0.1984\n",
      "Epoch 20/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 11.0905 - val_acc: 0.1975\n",
      "Epoch 21/50\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.0059 - acc: 0.9979 - val_loss: 11.1244 - val_acc: 0.1980\n",
      "Epoch 22/50\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 11.1774 - val_acc: 0.1984\n",
      "Epoch 23/50\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.0046 - acc: 0.9982 - val_loss: 11.2052 - val_acc: 0.1985\n",
      "Epoch 24/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 11.2654 - val_acc: 0.1986\n",
      "Epoch 25/50\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 11.2863 - val_acc: 0.1986\n",
      "Epoch 26/50\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.0034 - acc: 0.9988 - val_loss: 11.3389 - val_acc: 0.1983\n",
      "Epoch 27/50\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 11.3731 - val_acc: 0.1986\n",
      "Epoch 28/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 11.3431 - val_acc: 0.1988\n",
      "Epoch 29/50\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.0018 - acc: 0.9998 - val_loss: 11.4389 - val_acc: 0.1984\n",
      "Epoch 30/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 11.4664 - val_acc: 0.1986\n",
      "Epoch 31/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 11.4807 - val_acc: 0.1984\n",
      "Epoch 32/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 11.5301 - val_acc: 0.1985\n",
      "Epoch 33/50\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 9.7775e-04 - acc: 1.0000 - val_loss: 11.5446 - val_acc: 0.1987\n",
      "Epoch 34/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 11.5673 - val_acc: 0.1986\n",
      "Epoch 35/50\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 8.9365e-04 - acc: 1.0000 - val_loss: 11.6144 - val_acc: 0.1987\n",
      "Epoch 36/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 7.0834e-04 - acc: 1.0000 - val_loss: 11.6400 - val_acc: 0.1987\n",
      "Epoch 37/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 5.6447e-04 - acc: 1.0000 - val_loss: 11.6790 - val_acc: 0.1985\n",
      "Epoch 38/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 5.2345e-04 - acc: 1.0000 - val_loss: 11.6894 - val_acc: 0.1987\n",
      "Epoch 39/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 4.6989e-04 - acc: 1.0000 - val_loss: 11.7391 - val_acc: 0.1986\n",
      "Epoch 40/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 5.1136e-04 - acc: 1.0000 - val_loss: 11.7582 - val_acc: 0.1986\n",
      "Epoch 41/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 3.9014e-04 - acc: 1.0000 - val_loss: 11.7727 - val_acc: 0.1987\n",
      "Epoch 42/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 3.4619e-04 - acc: 1.0000 - val_loss: 11.7876 - val_acc: 0.1987\n",
      "Epoch 43/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 3.3569e-04 - acc: 1.0000 - val_loss: 11.7975 - val_acc: 0.1985\n",
      "Epoch 44/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 2.9386e-04 - acc: 1.0000 - val_loss: 11.8095 - val_acc: 0.1985\n",
      "Epoch 45/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 2.8390e-04 - acc: 1.0000 - val_loss: 11.8394 - val_acc: 0.1987\n",
      "Epoch 46/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 2.5653e-04 - acc: 1.0000 - val_loss: 11.8632 - val_acc: 0.1987\n",
      "Epoch 47/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 2.1209e-04 - acc: 1.0000 - val_loss: 11.8771 - val_acc: 0.1987\n",
      "Epoch 48/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 1.8624e-04 - acc: 1.0000 - val_loss: 11.8905 - val_acc: 0.1987\n",
      "Epoch 49/50\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 1.7570e-04 - acc: 1.0000 - val_loss: 11.9116 - val_acc: 0.1987\n",
      "Epoch 50/50\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 1.5753e-04 - acc: 1.0000 - val_loss: 11.9318 - val_acc: 0.1986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153fbe400>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train hard target model with training set omitting digit 3\n",
    "hard_78 = MNIST_StudentNet(n_hidden=20, T=1)\n",
    "hard_78.fit(x_train_78, y_train_78, batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12116/12116 [==============================] - 2s 166us/step - loss: 1.3137 - acc: 0.8350 - val_loss: 2.7221 - val_acc: 0.1943\n",
      "Epoch 2/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.8349 - acc: 0.9833 - val_loss: 2.3942 - val_acc: 0.1964\n",
      "Epoch 3/100\n",
      "12116/12116 [==============================] - 1s 92us/step - loss: 0.7921 - acc: 0.9874 - val_loss: 2.2301 - val_acc: 0.1971\n",
      "Epoch 4/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7810 - acc: 0.9888 - val_loss: 2.1232 - val_acc: 0.1986\n",
      "Epoch 5/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7738 - acc: 0.9893 - val_loss: 2.0344 - val_acc: 0.2018\n",
      "Epoch 6/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7676 - acc: 0.9901 - val_loss: 1.9403 - val_acc: 0.2127\n",
      "Epoch 7/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7628 - acc: 0.9902 - val_loss: 1.8744 - val_acc: 0.2199\n",
      "Epoch 8/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7592 - acc: 0.9904 - val_loss: 1.7867 - val_acc: 0.2264\n",
      "Epoch 9/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7562 - acc: 0.9904 - val_loss: 1.7531 - val_acc: 0.2319\n",
      "Epoch 10/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7533 - acc: 0.9907 - val_loss: 1.7217 - val_acc: 0.2363\n",
      "Epoch 11/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7511 - acc: 0.9912 - val_loss: 1.6749 - val_acc: 0.2551\n",
      "Epoch 12/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7493 - acc: 0.9914 - val_loss: 1.6703 - val_acc: 0.2611\n",
      "Epoch 13/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7476 - acc: 0.9917 - val_loss: 1.6474 - val_acc: 0.2658\n",
      "Epoch 14/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7461 - acc: 0.9917 - val_loss: 1.6107 - val_acc: 0.2834\n",
      "Epoch 15/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7449 - acc: 0.9915 - val_loss: 1.6027 - val_acc: 0.2828\n",
      "Epoch 16/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7438 - acc: 0.9919 - val_loss: 1.5888 - val_acc: 0.2916\n",
      "Epoch 17/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7426 - acc: 0.9922 - val_loss: 1.5615 - val_acc: 0.3042\n",
      "Epoch 18/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7416 - acc: 0.9925 - val_loss: 1.5384 - val_acc: 0.3189\n",
      "Epoch 19/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7408 - acc: 0.9923 - val_loss: 1.5228 - val_acc: 0.3277\n",
      "Epoch 20/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7399 - acc: 0.9923 - val_loss: 1.5058 - val_acc: 0.3397\n",
      "Epoch 21/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7391 - acc: 0.9922 - val_loss: 1.4942 - val_acc: 0.3494\n",
      "Epoch 22/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7383 - acc: 0.9927 - val_loss: 1.4793 - val_acc: 0.3577\n",
      "Epoch 23/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7376 - acc: 0.9929 - val_loss: 1.4694 - val_acc: 0.3685\n",
      "Epoch 24/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7369 - acc: 0.9932 - val_loss: 1.4461 - val_acc: 0.3853\n",
      "Epoch 25/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7361 - acc: 0.9935 - val_loss: 1.4303 - val_acc: 0.3880\n",
      "Epoch 26/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7354 - acc: 0.9938 - val_loss: 1.4123 - val_acc: 0.4031\n",
      "Epoch 27/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7350 - acc: 0.9941 - val_loss: 1.3995 - val_acc: 0.4143\n",
      "Epoch 28/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7343 - acc: 0.9941 - val_loss: 1.3867 - val_acc: 0.4208\n",
      "Epoch 29/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7339 - acc: 0.9940 - val_loss: 1.3710 - val_acc: 0.4240\n",
      "Epoch 30/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7334 - acc: 0.9940 - val_loss: 1.3750 - val_acc: 0.4209\n",
      "Epoch 31/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7331 - acc: 0.9942 - val_loss: 1.3493 - val_acc: 0.4410\n",
      "Epoch 32/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7326 - acc: 0.9945 - val_loss: 1.3333 - val_acc: 0.4527\n",
      "Epoch 33/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7322 - acc: 0.9946 - val_loss: 1.3469 - val_acc: 0.4409\n",
      "Epoch 34/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7320 - acc: 0.9943 - val_loss: 1.3001 - val_acc: 0.4823\n",
      "Epoch 35/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7316 - acc: 0.9943 - val_loss: 1.3247 - val_acc: 0.4549\n",
      "Epoch 36/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7313 - acc: 0.9944 - val_loss: 1.2791 - val_acc: 0.4989\n",
      "Epoch 37/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7310 - acc: 0.9946 - val_loss: 1.2862 - val_acc: 0.4877\n",
      "Epoch 38/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7308 - acc: 0.9944 - val_loss: 1.2711 - val_acc: 0.5010\n",
      "Epoch 39/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7306 - acc: 0.9948 - val_loss: 1.2662 - val_acc: 0.5079\n",
      "Epoch 40/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7304 - acc: 0.9946 - val_loss: 1.2673 - val_acc: 0.5080\n",
      "Epoch 41/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7301 - acc: 0.9948 - val_loss: 1.2551 - val_acc: 0.5102\n",
      "Epoch 42/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7298 - acc: 0.9951 - val_loss: 1.2458 - val_acc: 0.5226\n",
      "Epoch 43/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7297 - acc: 0.9949 - val_loss: 1.2335 - val_acc: 0.5369\n",
      "Epoch 44/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7296 - acc: 0.9950 - val_loss: 1.2453 - val_acc: 0.5225\n",
      "Epoch 45/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7294 - acc: 0.9950 - val_loss: 1.2305 - val_acc: 0.5339\n",
      "Epoch 46/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7292 - acc: 0.9951 - val_loss: 1.2232 - val_acc: 0.5364\n",
      "Epoch 47/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7291 - acc: 0.9952 - val_loss: 1.2312 - val_acc: 0.5298\n",
      "Epoch 48/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7289 - acc: 0.9952 - val_loss: 1.2136 - val_acc: 0.5407\n",
      "Epoch 49/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7289 - acc: 0.9950 - val_loss: 1.2184 - val_acc: 0.5407\n",
      "Epoch 50/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7286 - acc: 0.9951 - val_loss: 1.2098 - val_acc: 0.5449\n",
      "Epoch 51/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7285 - acc: 0.9952 - val_loss: 1.2072 - val_acc: 0.5404\n",
      "Epoch 52/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7284 - acc: 0.9954 - val_loss: 1.1965 - val_acc: 0.5573\n",
      "Epoch 53/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7284 - acc: 0.9955 - val_loss: 1.1991 - val_acc: 0.5518\n",
      "Epoch 54/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7282 - acc: 0.9956 - val_loss: 1.1877 - val_acc: 0.5629\n",
      "Epoch 55/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7281 - acc: 0.9953 - val_loss: 1.1825 - val_acc: 0.5637\n",
      "Epoch 56/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7279 - acc: 0.9958 - val_loss: 1.1849 - val_acc: 0.5545\n",
      "Epoch 57/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7279 - acc: 0.9960 - val_loss: 1.1736 - val_acc: 0.5672\n",
      "Epoch 58/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7277 - acc: 0.9957 - val_loss: 1.1702 - val_acc: 0.5678\n",
      "Epoch 59/100\n",
      "12116/12116 [==============================] - 1s 72us/step - loss: 0.7278 - acc: 0.9957 - val_loss: 1.1507 - val_acc: 0.5857\n",
      "Epoch 60/100\n",
      "12116/12116 [==============================] - 1s 106us/step - loss: 0.7276 - acc: 0.9960 - val_loss: 1.1569 - val_acc: 0.5796\n",
      "Epoch 61/100\n",
      "12116/12116 [==============================] - 1s 94us/step - loss: 0.7274 - acc: 0.9956 - val_loss: 1.1543 - val_acc: 0.5824\n",
      "Epoch 62/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7274 - acc: 0.9960 - val_loss: 1.1489 - val_acc: 0.5867\n",
      "Epoch 63/100\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.7271 - acc: 0.9959 - val_loss: 1.1546 - val_acc: 0.5783\n",
      "Epoch 64/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7272 - acc: 0.9960 - val_loss: 1.1522 - val_acc: 0.5810\n",
      "Epoch 65/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7270 - acc: 0.9960 - val_loss: 1.1494 - val_acc: 0.5796\n",
      "Epoch 66/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7269 - acc: 0.9960 - val_loss: 1.1401 - val_acc: 0.5877\n",
      "Epoch 67/100\n",
      "12116/12116 [==============================] - 1s 60us/step - loss: 0.7268 - acc: 0.9958 - val_loss: 1.1410 - val_acc: 0.5846\n",
      "Epoch 68/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7267 - acc: 0.9960 - val_loss: 1.1471 - val_acc: 0.5758\n",
      "Epoch 69/100\n",
      "12116/12116 [==============================] - 1s 55us/step - loss: 0.7266 - acc: 0.9959 - val_loss: 1.1320 - val_acc: 0.5927\n",
      "Epoch 70/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7266 - acc: 0.9964 - val_loss: 1.1292 - val_acc: 0.5935\n",
      "Epoch 71/100\n",
      "12116/12116 [==============================] - 1s 82us/step - loss: 0.7265 - acc: 0.9964 - val_loss: 1.1204 - val_acc: 0.5972\n",
      "Epoch 72/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7265 - acc: 0.9961 - val_loss: 1.1223 - val_acc: 0.5940\n",
      "Epoch 73/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7263 - acc: 0.9962 - val_loss: 1.1193 - val_acc: 0.5944\n",
      "Epoch 74/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7262 - acc: 0.9965 - val_loss: 1.1082 - val_acc: 0.6142\n",
      "Epoch 75/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7261 - acc: 0.9962 - val_loss: 1.1069 - val_acc: 0.6071\n",
      "Epoch 76/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7261 - acc: 0.9963 - val_loss: 1.1091 - val_acc: 0.5999\n",
      "Epoch 77/100\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.7261 - acc: 0.9967 - val_loss: 1.1006 - val_acc: 0.6103\n",
      "Epoch 78/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7259 - acc: 0.9961 - val_loss: 1.0991 - val_acc: 0.6141\n",
      "Epoch 79/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7259 - acc: 0.9964 - val_loss: 1.1042 - val_acc: 0.6061\n",
      "Epoch 80/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7258 - acc: 0.9960 - val_loss: 1.1070 - val_acc: 0.6044\n",
      "Epoch 81/100\n",
      "12116/12116 [==============================] - 1s 73us/step - loss: 0.7259 - acc: 0.9960 - val_loss: 1.1087 - val_acc: 0.6010\n",
      "Epoch 82/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7256 - acc: 0.9963 - val_loss: 1.0914 - val_acc: 0.6188\n",
      "Epoch 83/100\n",
      "12116/12116 [==============================] - 1s 86us/step - loss: 0.7256 - acc: 0.9965 - val_loss: 1.1047 - val_acc: 0.6018\n",
      "Epoch 84/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7254 - acc: 0.9962 - val_loss: 1.0916 - val_acc: 0.6130\n",
      "Epoch 85/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7254 - acc: 0.9965 - val_loss: 1.1055 - val_acc: 0.5986\n",
      "Epoch 86/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7255 - acc: 0.9965 - val_loss: 1.0815 - val_acc: 0.6292\n",
      "Epoch 87/100\n",
      "12116/12116 [==============================] - 1s 62us/step - loss: 0.7254 - acc: 0.9965 - val_loss: 1.1044 - val_acc: 0.6028\n",
      "Epoch 88/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7253 - acc: 0.9964 - val_loss: 1.0909 - val_acc: 0.6130\n",
      "Epoch 89/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7251 - acc: 0.9967 - val_loss: 1.0916 - val_acc: 0.6120\n",
      "Epoch 90/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7250 - acc: 0.9965 - val_loss: 1.0944 - val_acc: 0.6038\n",
      "Epoch 91/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7251 - acc: 0.9960 - val_loss: 1.0875 - val_acc: 0.6145\n",
      "Epoch 92/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7251 - acc: 0.9965 - val_loss: 1.0903 - val_acc: 0.6117\n",
      "Epoch 93/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7250 - acc: 0.9965 - val_loss: 1.0882 - val_acc: 0.6089\n",
      "Epoch 94/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7250 - acc: 0.9965 - val_loss: 1.0912 - val_acc: 0.6106\n",
      "Epoch 95/100\n",
      "12116/12116 [==============================] - 1s 52us/step - loss: 0.7249 - acc: 0.9965 - val_loss: 1.0715 - val_acc: 0.6286\n",
      "Epoch 96/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7249 - acc: 0.9962 - val_loss: 1.0791 - val_acc: 0.6202\n",
      "Epoch 97/100\n",
      "12116/12116 [==============================] - 1s 62us/step - loss: 0.7248 - acc: 0.9966 - val_loss: 1.0793 - val_acc: 0.6188\n",
      "Epoch 98/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7247 - acc: 0.9962 - val_loss: 1.0711 - val_acc: 0.6213\n",
      "Epoch 99/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7248 - acc: 0.9964 - val_loss: 1.0795 - val_acc: 0.6143\n",
      "Epoch 100/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7246 - acc: 0.9964 - val_loss: 1.0752 - val_acc: 0.6182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15681ada0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train soft target model with training set omitting digit 3\n",
    "soft_78 = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "soft_78.fit(x_train_78, y_train_soft_78, batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12116/12116 [==============================] - 4s 314us/step - loss: 1.4717 - acc: 0.8966 - val_loss: 3.1774 - val_acc: 0.1925\n",
      "Epoch 2/100\n",
      "12116/12116 [==============================] - 1s 110us/step - loss: 0.8514 - acc: 0.9808 - val_loss: 2.7144 - val_acc: 0.1955\n",
      "Epoch 3/100\n",
      "12116/12116 [==============================] - 1s 82us/step - loss: 0.8034 - acc: 0.9891 - val_loss: 2.5803 - val_acc: 0.1967\n",
      "Epoch 4/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7909 - acc: 0.9908 - val_loss: 2.5080 - val_acc: 0.1974\n",
      "Epoch 5/100\n",
      "12116/12116 [==============================] - 1s 75us/step - loss: 0.7839 - acc: 0.9916 - val_loss: 2.4242 - val_acc: 0.1986\n",
      "Epoch 6/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7786 - acc: 0.9922 - val_loss: 2.4065 - val_acc: 0.2003\n",
      "Epoch 7/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7741 - acc: 0.9929 - val_loss: 2.3520 - val_acc: 0.2033\n",
      "Epoch 8/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7705 - acc: 0.9931 - val_loss: 2.3089 - val_acc: 0.2072\n",
      "Epoch 9/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7675 - acc: 0.9936 - val_loss: 2.2714 - val_acc: 0.2103\n",
      "Epoch 10/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7648 - acc: 0.9936 - val_loss: 2.2362 - val_acc: 0.2143\n",
      "Epoch 11/100\n",
      "12116/12116 [==============================] - 1s 70us/step - loss: 0.7627 - acc: 0.9939 - val_loss: 2.2003 - val_acc: 0.2175\n",
      "Epoch 12/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7606 - acc: 0.9941 - val_loss: 2.1660 - val_acc: 0.2246\n",
      "Epoch 13/100\n",
      "12116/12116 [==============================] - 1s 72us/step - loss: 0.7583 - acc: 0.9944 - val_loss: 2.1081 - val_acc: 0.2313\n",
      "Epoch 14/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7564 - acc: 0.9943 - val_loss: 2.0878 - val_acc: 0.2324\n",
      "Epoch 15/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7546 - acc: 0.9946 - val_loss: 2.0283 - val_acc: 0.2476\n",
      "Epoch 16/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7530 - acc: 0.9951 - val_loss: 2.0129 - val_acc: 0.2533\n",
      "Epoch 17/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7512 - acc: 0.9953 - val_loss: 1.9817 - val_acc: 0.2617\n",
      "Epoch 18/100\n",
      "12116/12116 [==============================] - 1s 52us/step - loss: 0.7497 - acc: 0.9954 - val_loss: 1.9586 - val_acc: 0.2733\n",
      "Epoch 19/100\n",
      "12116/12116 [==============================] - 1s 83us/step - loss: 0.7484 - acc: 0.9950 - val_loss: 1.9090 - val_acc: 0.2905\n",
      "Epoch 20/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7470 - acc: 0.9954 - val_loss: 1.8885 - val_acc: 0.3021\n",
      "Epoch 21/100\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.7461 - acc: 0.9960 - val_loss: 1.8663 - val_acc: 0.3118\n",
      "Epoch 22/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7448 - acc: 0.9956 - val_loss: 1.8397 - val_acc: 0.3300\n",
      "Epoch 23/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7438 - acc: 0.9959 - val_loss: 1.8317 - val_acc: 0.3287\n",
      "Epoch 24/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7426 - acc: 0.9957 - val_loss: 1.7921 - val_acc: 0.3515\n",
      "Epoch 25/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7414 - acc: 0.9959 - val_loss: 1.7836 - val_acc: 0.3571\n",
      "Epoch 26/100\n",
      "12116/12116 [==============================] - 1s 52us/step - loss: 0.7402 - acc: 0.9960 - val_loss: 1.7695 - val_acc: 0.3587\n",
      "Epoch 27/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7392 - acc: 0.9957 - val_loss: 1.7414 - val_acc: 0.3790\n",
      "Epoch 28/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7384 - acc: 0.9957 - val_loss: 1.7280 - val_acc: 0.3808\n",
      "Epoch 29/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7377 - acc: 0.9961 - val_loss: 1.7074 - val_acc: 0.3899\n",
      "Epoch 30/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7371 - acc: 0.9963 - val_loss: 1.6971 - val_acc: 0.3960\n",
      "Epoch 31/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7364 - acc: 0.9960 - val_loss: 1.6845 - val_acc: 0.4029\n",
      "Epoch 32/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7361 - acc: 0.9957 - val_loss: 1.6755 - val_acc: 0.4029\n",
      "Epoch 33/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7355 - acc: 0.9964 - val_loss: 1.6645 - val_acc: 0.4110\n",
      "Epoch 34/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7352 - acc: 0.9961 - val_loss: 1.6446 - val_acc: 0.4230\n",
      "Epoch 35/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7347 - acc: 0.9961 - val_loss: 1.6405 - val_acc: 0.4182\n",
      "Epoch 36/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7345 - acc: 0.9965 - val_loss: 1.6334 - val_acc: 0.4176\n",
      "Epoch 37/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7342 - acc: 0.9963 - val_loss: 1.6181 - val_acc: 0.4303\n",
      "Epoch 38/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7339 - acc: 0.9964 - val_loss: 1.6123 - val_acc: 0.4320\n",
      "Epoch 39/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7335 - acc: 0.9964 - val_loss: 1.5968 - val_acc: 0.4409\n",
      "Epoch 40/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7333 - acc: 0.9966 - val_loss: 1.5890 - val_acc: 0.4447\n",
      "Epoch 41/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7330 - acc: 0.9965 - val_loss: 1.5703 - val_acc: 0.4587\n",
      "Epoch 42/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7328 - acc: 0.9961 - val_loss: 1.5716 - val_acc: 0.4483\n",
      "Epoch 43/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7325 - acc: 0.9967 - val_loss: 1.5599 - val_acc: 0.4587\n",
      "Epoch 44/100\n",
      "12116/12116 [==============================] - 2s 137us/step - loss: 0.7323 - acc: 0.9965 - val_loss: 1.5419 - val_acc: 0.4716\n",
      "Epoch 45/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7319 - acc: 0.9967 - val_loss: 1.5337 - val_acc: 0.4799\n",
      "Epoch 46/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7317 - acc: 0.9965 - val_loss: 1.5360 - val_acc: 0.4721\n",
      "Epoch 47/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7314 - acc: 0.9965 - val_loss: 1.5236 - val_acc: 0.4796\n",
      "Epoch 48/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7313 - acc: 0.9966 - val_loss: 1.5133 - val_acc: 0.4893\n",
      "Epoch 49/100\n",
      "12116/12116 [==============================] - 1s 93us/step - loss: 0.7312 - acc: 0.9966 - val_loss: 1.5166 - val_acc: 0.4867\n",
      "Epoch 50/100\n",
      "12116/12116 [==============================] - 1s 64us/step - loss: 0.7310 - acc: 0.9966 - val_loss: 1.5087 - val_acc: 0.4923\n",
      "Epoch 51/100\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.7308 - acc: 0.9968 - val_loss: 1.5069 - val_acc: 0.4873\n",
      "Epoch 52/100\n",
      "12116/12116 [==============================] - 1s 92us/step - loss: 0.7307 - acc: 0.9971 - val_loss: 1.5043 - val_acc: 0.4873\n",
      "Epoch 53/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7306 - acc: 0.9969 - val_loss: 1.4903 - val_acc: 0.5030\n",
      "Epoch 54/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7304 - acc: 0.9967 - val_loss: 1.4883 - val_acc: 0.5033\n",
      "Epoch 55/100\n",
      "12116/12116 [==============================] - 1s 60us/step - loss: 0.7302 - acc: 0.9966 - val_loss: 1.4820 - val_acc: 0.5019\n",
      "Epoch 56/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7302 - acc: 0.9965 - val_loss: 1.4820 - val_acc: 0.5042\n",
      "Epoch 57/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7299 - acc: 0.9967 - val_loss: 1.4840 - val_acc: 0.5008\n",
      "Epoch 58/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7298 - acc: 0.9969 - val_loss: 1.4815 - val_acc: 0.5040\n",
      "Epoch 59/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7298 - acc: 0.9969 - val_loss: 1.4737 - val_acc: 0.5095\n",
      "Epoch 60/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7296 - acc: 0.9967 - val_loss: 1.4726 - val_acc: 0.5099\n",
      "Epoch 61/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7295 - acc: 0.9968 - val_loss: 1.4675 - val_acc: 0.5117\n",
      "Epoch 62/100\n",
      "12116/12116 [==============================] - 1s 98us/step - loss: 0.7294 - acc: 0.9966 - val_loss: 1.4583 - val_acc: 0.5168\n",
      "Epoch 63/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7292 - acc: 0.9969 - val_loss: 1.4433 - val_acc: 0.5330\n",
      "Epoch 64/100\n",
      "12116/12116 [==============================] - 1s 96us/step - loss: 0.7290 - acc: 0.9970 - val_loss: 1.4454 - val_acc: 0.5273\n",
      "Epoch 65/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7289 - acc: 0.9970 - val_loss: 1.4350 - val_acc: 0.5345\n",
      "Epoch 66/100\n",
      "12116/12116 [==============================] - 1s 78us/step - loss: 0.7287 - acc: 0.9969 - val_loss: 1.4380 - val_acc: 0.5307\n",
      "Epoch 67/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7287 - acc: 0.9967 - val_loss: 1.4179 - val_acc: 0.5501\n",
      "Epoch 68/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7283 - acc: 0.9967 - val_loss: 1.4233 - val_acc: 0.5421\n",
      "Epoch 69/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7283 - acc: 0.9969 - val_loss: 1.4262 - val_acc: 0.5348\n",
      "Epoch 70/100\n",
      "12116/12116 [==============================] - 1s 73us/step - loss: 0.7281 - acc: 0.9970 - val_loss: 1.4106 - val_acc: 0.5548\n",
      "Epoch 71/100\n",
      "12116/12116 [==============================] - 1s 84us/step - loss: 0.7278 - acc: 0.9971 - val_loss: 1.3971 - val_acc: 0.5590\n",
      "Epoch 72/100\n",
      "12116/12116 [==============================] - 1s 75us/step - loss: 0.7277 - acc: 0.9970 - val_loss: 1.3877 - val_acc: 0.5645\n",
      "Epoch 73/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7276 - acc: 0.9969 - val_loss: 1.3855 - val_acc: 0.5631\n",
      "Epoch 74/100\n",
      "12116/12116 [==============================] - 1s 88us/step - loss: 0.7275 - acc: 0.9970 - val_loss: 1.3854 - val_acc: 0.5640\n",
      "Epoch 75/100\n",
      "12116/12116 [==============================] - 1s 91us/step - loss: 0.7273 - acc: 0.9971 - val_loss: 1.3775 - val_acc: 0.5673\n",
      "Epoch 76/100\n",
      "12116/12116 [==============================] - 1s 80us/step - loss: 0.7272 - acc: 0.9970 - val_loss: 1.3728 - val_acc: 0.5705\n",
      "Epoch 77/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7271 - acc: 0.9969 - val_loss: 1.3608 - val_acc: 0.5816\n",
      "Epoch 78/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7270 - acc: 0.9969 - val_loss: 1.3653 - val_acc: 0.5753\n",
      "Epoch 79/100\n",
      "12116/12116 [==============================] - 1s 84us/step - loss: 0.7269 - acc: 0.9971 - val_loss: 1.3564 - val_acc: 0.5770\n",
      "Epoch 80/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7267 - acc: 0.9973 - val_loss: 1.3536 - val_acc: 0.5835\n",
      "Epoch 81/100\n",
      "12116/12116 [==============================] - 1s 95us/step - loss: 0.7267 - acc: 0.9971 - val_loss: 1.3424 - val_acc: 0.5913\n",
      "Epoch 82/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7265 - acc: 0.9971 - val_loss: 1.3438 - val_acc: 0.5857\n",
      "Epoch 83/100\n",
      "12116/12116 [==============================] - 1s 94us/step - loss: 0.7265 - acc: 0.9973 - val_loss: 1.3459 - val_acc: 0.5789\n",
      "Epoch 84/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7265 - acc: 0.9974 - val_loss: 1.3412 - val_acc: 0.5856\n",
      "Epoch 85/100\n",
      "12116/12116 [==============================] - 1s 75us/step - loss: 0.7263 - acc: 0.9974 - val_loss: 1.3393 - val_acc: 0.5878\n",
      "Epoch 86/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7264 - acc: 0.9970 - val_loss: 1.3284 - val_acc: 0.5929\n",
      "Epoch 87/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7262 - acc: 0.9972 - val_loss: 1.3272 - val_acc: 0.5973\n",
      "Epoch 88/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7262 - acc: 0.9973 - val_loss: 1.3272 - val_acc: 0.5950\n",
      "Epoch 89/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7262 - acc: 0.9974 - val_loss: 1.3172 - val_acc: 0.6009\n",
      "Epoch 90/100\n",
      "12116/12116 [==============================] - 1s 71us/step - loss: 0.7260 - acc: 0.9972 - val_loss: 1.3208 - val_acc: 0.5956\n",
      "Epoch 91/100\n",
      "12116/12116 [==============================] - 1s 64us/step - loss: 0.7259 - acc: 0.9974 - val_loss: 1.3108 - val_acc: 0.6095\n",
      "Epoch 92/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7259 - acc: 0.9973 - val_loss: 1.3163 - val_acc: 0.5970\n",
      "Epoch 93/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7259 - acc: 0.9973 - val_loss: 1.3147 - val_acc: 0.6037\n",
      "Epoch 94/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7258 - acc: 0.9970 - val_loss: 1.3103 - val_acc: 0.6069\n",
      "Epoch 95/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7257 - acc: 0.9975 - val_loss: 1.3087 - val_acc: 0.6069\n",
      "Epoch 96/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7256 - acc: 0.9972 - val_loss: 1.2975 - val_acc: 0.6186\n",
      "Epoch 97/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7257 - acc: 0.9971 - val_loss: 1.3142 - val_acc: 0.6010\n",
      "Epoch 98/100\n",
      "12116/12116 [==============================] - 1s 62us/step - loss: 0.7256 - acc: 0.9970 - val_loss: 1.3085 - val_acc: 0.6089\n",
      "Epoch 99/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7255 - acc: 0.9974 - val_loss: 1.3045 - val_acc: 0.6123\n",
      "Epoch 100/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7254 - acc: 0.9974 - val_loss: 1.2919 - val_acc: 0.6241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15519a668>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "T = 5\n",
    "w = 0.5 / (T**2) \n",
    "\n",
    "# apply both hard & soft targets to learn\n",
    "y_hard_soft_train_78 = np.concatenate((y_train_78, y_train_soft_78), axis=1)\n",
    "\n",
    "# student net - WITH distillation & weighted hard soft loss - digit 3 omitted in the transfer set\n",
    "mix_78 = Sequential()\n",
    "mix_78.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "mix_78.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "mix_78.add(Dense(10, name='logit'))\n",
    "\n",
    "# y_pred at the end of 10-node dense layer is the logit_pred\n",
    "mix_78.compile(loss=lambda y_true, y_pred: avg_mix_loss(y_true, y_pred, w, T), \n",
    "               optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "mix_78.fit(x_train_78, y_hard_soft_train_78, \n",
    "           batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_hard_soft_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12116/12116 [==============================] - 2s 158us/step - loss: 1.1857 - acc: 0.9439 - val_loss: 2.7235 - val_acc: 0.1951\n",
      "Epoch 2/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.8168 - acc: 0.9835 - val_loss: 2.4055 - val_acc: 0.1966\n",
      "Epoch 3/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7902 - acc: 0.9879 - val_loss: 2.2500 - val_acc: 0.1972\n",
      "Epoch 4/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7781 - acc: 0.9889 - val_loss: 2.1407 - val_acc: 0.1979\n",
      "Epoch 5/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7688 - acc: 0.9896 - val_loss: 1.9956 - val_acc: 0.1990\n",
      "Epoch 6/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7621 - acc: 0.9907 - val_loss: 1.9205 - val_acc: 0.2064\n",
      "Epoch 7/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7574 - acc: 0.9910 - val_loss: 1.8677 - val_acc: 0.2155\n",
      "Epoch 8/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7539 - acc: 0.9914 - val_loss: 1.8046 - val_acc: 0.2334\n",
      "Epoch 9/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7506 - acc: 0.9917 - val_loss: 1.7750 - val_acc: 0.2409\n",
      "Epoch 10/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7482 - acc: 0.9920 - val_loss: 1.7308 - val_acc: 0.2534\n",
      "Epoch 11/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7460 - acc: 0.9920 - val_loss: 1.6992 - val_acc: 0.2588\n",
      "Epoch 12/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7443 - acc: 0.9926 - val_loss: 1.6775 - val_acc: 0.2617\n",
      "Epoch 13/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7426 - acc: 0.9925 - val_loss: 1.6517 - val_acc: 0.2693\n",
      "Epoch 14/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7417 - acc: 0.9931 - val_loss: 1.6111 - val_acc: 0.2863\n",
      "Epoch 15/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7401 - acc: 0.9931 - val_loss: 1.5931 - val_acc: 0.2908\n",
      "Epoch 16/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7391 - acc: 0.9933 - val_loss: 1.5818 - val_acc: 0.2927\n",
      "Epoch 17/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7383 - acc: 0.9932 - val_loss: 1.5364 - val_acc: 0.3159\n",
      "Epoch 18/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7373 - acc: 0.9936 - val_loss: 1.5101 - val_acc: 0.3225\n",
      "Epoch 19/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7366 - acc: 0.9937 - val_loss: 1.4798 - val_acc: 0.3437\n",
      "Epoch 20/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7358 - acc: 0.9941 - val_loss: 1.4584 - val_acc: 0.3529\n",
      "Epoch 21/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7352 - acc: 0.9940 - val_loss: 1.4392 - val_acc: 0.3604\n",
      "Epoch 22/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7344 - acc: 0.9941 - val_loss: 1.4249 - val_acc: 0.3673\n",
      "Epoch 23/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7338 - acc: 0.9944 - val_loss: 1.4007 - val_acc: 0.3840\n",
      "Epoch 24/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7333 - acc: 0.9944 - val_loss: 1.3750 - val_acc: 0.4071\n",
      "Epoch 25/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7328 - acc: 0.9943 - val_loss: 1.3739 - val_acc: 0.3989\n",
      "Epoch 26/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7324 - acc: 0.9947 - val_loss: 1.3493 - val_acc: 0.4254\n",
      "Epoch 27/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7319 - acc: 0.9946 - val_loss: 1.3388 - val_acc: 0.4246\n",
      "Epoch 28/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7314 - acc: 0.9944 - val_loss: 1.3160 - val_acc: 0.4396\n",
      "Epoch 29/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7311 - acc: 0.9946 - val_loss: 1.3113 - val_acc: 0.4490\n",
      "Epoch 30/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7307 - acc: 0.9944 - val_loss: 1.3019 - val_acc: 0.4496\n",
      "Epoch 31/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7303 - acc: 0.9945 - val_loss: 1.2916 - val_acc: 0.4579\n",
      "Epoch 32/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7300 - acc: 0.9947 - val_loss: 1.2824 - val_acc: 0.4624\n",
      "Epoch 33/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7298 - acc: 0.9945 - val_loss: 1.2597 - val_acc: 0.4820\n",
      "Epoch 34/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7296 - acc: 0.9950 - val_loss: 1.2578 - val_acc: 0.4777\n",
      "Epoch 35/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7292 - acc: 0.9953 - val_loss: 1.2347 - val_acc: 0.5060\n",
      "Epoch 36/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7291 - acc: 0.9951 - val_loss: 1.2388 - val_acc: 0.4992\n",
      "Epoch 37/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7287 - acc: 0.9951 - val_loss: 1.2439 - val_acc: 0.4876\n",
      "Epoch 38/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7286 - acc: 0.9951 - val_loss: 1.2358 - val_acc: 0.4965\n",
      "Epoch 39/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7284 - acc: 0.9947 - val_loss: 1.2146 - val_acc: 0.5168\n",
      "Epoch 40/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7282 - acc: 0.9953 - val_loss: 1.2164 - val_acc: 0.5105\n",
      "Epoch 41/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7281 - acc: 0.9954 - val_loss: 1.2115 - val_acc: 0.5148\n",
      "Epoch 42/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7279 - acc: 0.9957 - val_loss: 1.1935 - val_acc: 0.5381\n",
      "Epoch 43/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7279 - acc: 0.9953 - val_loss: 1.1997 - val_acc: 0.5235\n",
      "Epoch 44/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7276 - acc: 0.9950 - val_loss: 1.1819 - val_acc: 0.5383\n",
      "Epoch 45/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7274 - acc: 0.9953 - val_loss: 1.1809 - val_acc: 0.5380\n",
      "Epoch 46/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7273 - acc: 0.9955 - val_loss: 1.1855 - val_acc: 0.5288\n",
      "Epoch 47/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7272 - acc: 0.9954 - val_loss: 1.1608 - val_acc: 0.5568\n",
      "Epoch 48/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7271 - acc: 0.9951 - val_loss: 1.1711 - val_acc: 0.5418\n",
      "Epoch 49/100\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.7270 - acc: 0.9954 - val_loss: 1.1627 - val_acc: 0.5519\n",
      "Epoch 50/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7268 - acc: 0.9954 - val_loss: 1.1500 - val_acc: 0.5686\n",
      "Epoch 51/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7267 - acc: 0.9957 - val_loss: 1.1531 - val_acc: 0.5630\n",
      "Epoch 52/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7267 - acc: 0.9954 - val_loss: 1.1511 - val_acc: 0.5603\n",
      "Epoch 53/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7265 - acc: 0.9951 - val_loss: 1.1462 - val_acc: 0.5631\n",
      "Epoch 54/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7264 - acc: 0.9955 - val_loss: 1.1280 - val_acc: 0.5838\n",
      "Epoch 55/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7263 - acc: 0.9955 - val_loss: 1.1308 - val_acc: 0.5820\n",
      "Epoch 56/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7262 - acc: 0.9957 - val_loss: 1.1296 - val_acc: 0.5783\n",
      "Epoch 57/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7261 - acc: 0.9958 - val_loss: 1.1392 - val_acc: 0.5654\n",
      "Epoch 58/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7259 - acc: 0.9959 - val_loss: 1.1230 - val_acc: 0.5842\n",
      "Epoch 59/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7258 - acc: 0.9957 - val_loss: 1.1268 - val_acc: 0.5737\n",
      "Epoch 60/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7258 - acc: 0.9956 - val_loss: 1.1162 - val_acc: 0.5952\n",
      "Epoch 61/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7257 - acc: 0.9960 - val_loss: 1.1146 - val_acc: 0.5917\n",
      "Epoch 62/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7256 - acc: 0.9958 - val_loss: 1.1011 - val_acc: 0.6008\n",
      "Epoch 63/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7255 - acc: 0.9960 - val_loss: 1.1063 - val_acc: 0.5917\n",
      "Epoch 64/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7255 - acc: 0.9960 - val_loss: 1.1095 - val_acc: 0.5871\n",
      "Epoch 65/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7254 - acc: 0.9961 - val_loss: 1.0997 - val_acc: 0.6020\n",
      "Epoch 66/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7253 - acc: 0.9962 - val_loss: 1.0974 - val_acc: 0.6036\n",
      "Epoch 67/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7252 - acc: 0.9961 - val_loss: 1.1014 - val_acc: 0.5965\n",
      "Epoch 68/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7251 - acc: 0.9965 - val_loss: 1.0910 - val_acc: 0.6101\n",
      "Epoch 69/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7251 - acc: 0.9960 - val_loss: 1.0862 - val_acc: 0.6142\n",
      "Epoch 70/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7250 - acc: 0.9963 - val_loss: 1.0899 - val_acc: 0.6071\n",
      "Epoch 71/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7249 - acc: 0.9963 - val_loss: 1.0895 - val_acc: 0.6066\n",
      "Epoch 72/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7249 - acc: 0.9966 - val_loss: 1.0904 - val_acc: 0.6071\n",
      "Epoch 73/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7248 - acc: 0.9965 - val_loss: 1.0833 - val_acc: 0.6128\n",
      "Epoch 74/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7248 - acc: 0.9965 - val_loss: 1.0821 - val_acc: 0.6143\n",
      "Epoch 75/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7247 - acc: 0.9966 - val_loss: 1.0708 - val_acc: 0.6232\n",
      "Epoch 76/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7246 - acc: 0.9967 - val_loss: 1.0802 - val_acc: 0.6112\n",
      "Epoch 77/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7245 - acc: 0.9963 - val_loss: 1.0816 - val_acc: 0.6074\n",
      "Epoch 78/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7246 - acc: 0.9966 - val_loss: 1.0781 - val_acc: 0.6122\n",
      "Epoch 79/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7244 - acc: 0.9965 - val_loss: 1.0716 - val_acc: 0.6245\n",
      "Epoch 80/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7246 - acc: 0.9965 - val_loss: 1.0750 - val_acc: 0.6102\n",
      "Epoch 81/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7244 - acc: 0.9965 - val_loss: 1.0701 - val_acc: 0.6215\n",
      "Epoch 82/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7243 - acc: 0.9965 - val_loss: 1.0655 - val_acc: 0.6270\n",
      "Epoch 83/100\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.7242 - acc: 0.9966 - val_loss: 1.0671 - val_acc: 0.6170\n",
      "Epoch 84/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7242 - acc: 0.9968 - val_loss: 1.0557 - val_acc: 0.6298\n",
      "Epoch 85/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7242 - acc: 0.9965 - val_loss: 1.0649 - val_acc: 0.6263\n",
      "Epoch 86/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7243 - acc: 0.9965 - val_loss: 1.0625 - val_acc: 0.6266\n",
      "Epoch 87/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7242 - acc: 0.9962 - val_loss: 1.0639 - val_acc: 0.6226\n",
      "Epoch 88/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7241 - acc: 0.9968 - val_loss: 1.0573 - val_acc: 0.6279\n",
      "Epoch 89/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7241 - acc: 0.9967 - val_loss: 1.0604 - val_acc: 0.6205\n",
      "Epoch 90/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7241 - acc: 0.9968 - val_loss: 1.0581 - val_acc: 0.6277\n",
      "Epoch 91/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7240 - acc: 0.9965 - val_loss: 1.0475 - val_acc: 0.6412\n",
      "Epoch 92/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7239 - acc: 0.9964 - val_loss: 1.0507 - val_acc: 0.6291\n",
      "Epoch 93/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7239 - acc: 0.9969 - val_loss: 1.0493 - val_acc: 0.6455\n",
      "Epoch 94/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7239 - acc: 0.9969 - val_loss: 1.0559 - val_acc: 0.6333\n",
      "Epoch 95/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7238 - acc: 0.9967 - val_loss: 1.0523 - val_acc: 0.6356\n",
      "Epoch 96/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7238 - acc: 0.9965 - val_loss: 1.0415 - val_acc: 0.6445\n",
      "Epoch 97/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7238 - acc: 0.9967 - val_loss: 1.0436 - val_acc: 0.6428\n",
      "Epoch 98/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7237 - acc: 0.9969 - val_loss: 1.0450 - val_acc: 0.6358\n",
      "Epoch 99/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7237 - acc: 0.9969 - val_loss: 1.0400 - val_acc: 0.6423\n",
      "Epoch 100/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7237 - acc: 0.9969 - val_loss: 1.0510 - val_acc: 0.6302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1591e5f60>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_78_bias = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "bias = soft_78_bias.layers[2].get_weights()[1]\n",
    "bias[7] = 1\n",
    "bias[8] = 1\n",
    "K.set_value(soft_78_bias.layers[2].bias, bias)\n",
    "\n",
    "soft_78_bias.fit(x_train_78, y_train_soft_78, \n",
    "                 batch_size=128, epochs=100,verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Accuracy on Test set === \n",
      "\n",
      "NO DISTILLATION\n",
      "loss = 11.931765022277832, accuracy = 0.1986, #errors = 8014\n",
      "\n",
      "WITH DISTILLATION\n",
      "loss = 1.07515452003479, accuracy = 0.6182, #errors = 3818\n",
      "\n",
      "WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET\n",
      "loss = 1.2918906003952026, accuracy = 0.6241, #errors = 3759\n",
      "\n",
      "WITH DISTILLATION & BIAS TUNED\n",
      "loss = 1.050957187271118, accuracy = 0.6302, #errors = 3698\n"
     ]
    }
   ],
   "source": [
    "print('=== Overall Accuracy on Test set === \\n')\n",
    "\n",
    "loss, accuracy = hard_78.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('NO DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION')\n",
    "loss, accuracy = soft_78.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET')\n",
    "loss, accuracy = mix_78.evaluate(x_test, y_hard_soft_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION & BIAS TUNED')\n",
    "loss, accuracy = soft_78_bias.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Only 7 & 8 Distillation Performance on Full Test Set ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.1986</td>\n",
       "      <td>11.931765</td>\n",
       "      <td>8014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_tuned</th>\n",
       "      <td>0.6302</td>\n",
       "      <td>1.050957</td>\n",
       "      <td>3698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.6182</td>\n",
       "      <td>1.075155</td>\n",
       "      <td>3818.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.6241</td>\n",
       "      <td>1.291891</td>\n",
       "      <td>3759.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy       loss  num_error\n",
       "baseline                    0.1986  11.931765     8014.0\n",
       "bias_tuned                  0.6302   1.050957     3698.0\n",
       "dense_distilled             0.6182   1.075155     3818.0\n",
       "dense_distilled_mix_loss    0.6241   1.291891     3759.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep78_summary = {\n",
    "    'baseline': {'loss': 11.931765022277832, 'accuracy': 0.1986, 'num_error': 8014},\n",
    "    'dense_distilled': {'loss': 1.07515452003479, 'accuracy': 0.6182, 'num_error': 3818},\n",
    "    'dense_distilled_mix_loss': {'loss': 1.2918906003952026, 'accuracy': 0.6241, 'num_error': 3759},\n",
    "    'bias_tuned': {'loss': 1.050957187271118, 'accuracy': 0.6302, 'num_error': 3698}\n",
    "}\n",
    "\n",
    "print('=== Only 7 & 8 Distillation Performance on Full Test Set ===')\n",
    "df_keep78_summary = pd.DataFrame().from_dict(keep78_summary).T\n",
    "display(df_keep78_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion - distillation performances of student nets on transfer sets of digit 7 & 8 only\n",
    "\n",
    "The non-distilled student baseline achieved the worst accuracy = 19.9%. All other distilled models performed stunningly better, with accuracies over 60%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Distillation Applied to Fake Datasets\n",
    "\n",
    "As shown in the first figure of this tutorial, increasing temperature leads to increasing target \"softness\". Therefore, we hypothesized that datasets with different data quality (i.e. degree of class separability and the number of informative features) may require different temperature for optimal distillation outcome. In the following experiment, we used the $\\texttt{sklearn}$ library to synthesize 9 datasets with varying degrees of data quality. Specifically, the datasets were synthesized using the follwoing parameters: $n\\_samples$ = 50000, $n\\_features$ = 100, $n\\_classes$ = 10, $n\\_clusters\\_per\\_class$ = 1. In addition, data quailty was synthesized using $class\\_sep \\in (0.2, 0.6, 1)$ and $p\\_informative \\in (0.5, 0.8, 1)$ where $n\\_informative$ = $p\\_informative$ x $n\\_features$. The teacher net has 3 hidden layers, each with 100 nodes; the student net has 2 hidden layers, each with 50 nodes. Distillation was done at $T \\in (2, 2.5, 3, 4, 5, 10, 15, 20)$. Both teacher and students were trained for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fake dataset class\n",
    "class FakeDataGenerator():\n",
    "    def __init__(self, n_samples, n_features, n_classes, n_clusters_per_class):\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.n_clusters_per_class = n_clusters_per_class\n",
    "        \n",
    "    def generate_data(self, p_informative, class_sep):\n",
    "        self.n_informative = round(p_informative*self.n_features)\n",
    "        self.n_redundant = self.n_features - self.n_informative\n",
    "        self.class_sep = class_sep\n",
    "        self.X, self.Y = make_classification(n_samples=self.n_samples, \n",
    "                                             n_features=self.n_features,\n",
    "                                             n_redundant=self.n_redundant,\n",
    "                                             n_informative=self.n_informative,\n",
    "                                             n_clusters_per_class=self.n_clusters_per_class, \n",
    "                                             n_classes=self.n_classes,\n",
    "                                             class_sep=self.class_sep)\n",
    "        return self.X, self.Y\n",
    "\n",
    "# Teacher net structure\n",
    "def FakeTeacherNet(input_shape, n_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, name='hidden_1', input_shape=input_shape, activation='relu'))\n",
    "    model.add(Dropout(0.2, name='dropout_1'))\n",
    "    model.add(Dense(100, name='hidden_2', activation='relu'))\n",
    "    model.add(Dropout(0.2, name='dropout_2'))\n",
    "    model.add(Dense(100, name='hidden_3', activation='relu'))\n",
    "    model.add(Dropout(0.2, name='dropout_3'))\n",
    "    model.add(Dense(n_classes, name='logit'))\n",
    "    model.add(Activation('softmax', name='softmax'))\n",
    "    return model\n",
    "\n",
    "# Student net structure\n",
    "def FakeStudentNet(input_shape, n_classes, n_hidden, T=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_hidden, name='hidden_1', input_shape=input_shape, activation='relu'))\n",
    "    model.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "    model.add(Dense(10, name='logit'))\n",
    "    model.add(Lambda(lambda x: x / T, name='logit_soft'))\n",
    "    model.add(Activation('softmax', name='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x_train, x_test):\n",
    "    x_train_mean = x_train.mean(axis=0)\n",
    "    x_train_std = x_train.std(axis=0)\n",
    "    x_train_norm = (x_train - x_train_mean) / x_train_std\n",
    "    x_test_norm = (x_test - x_train_mean) / x_train_std\n",
    "    return x_train_norm, x_test_norm\n",
    "\n",
    "def train_teacher(x_train, y_train, x_test, y_test, n_classes, \n",
    "                  batch_size=100, epochs=50, save_model=False, model_name=None):\n",
    "    '''\n",
    "    Function to build a cumbersome teacher net using hard cross-entropy loss\n",
    "    ''' \n",
    "    model = FakeTeacherNet(input_shape=(x_train.shape[1],), n_classes=n_classes)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    # train teacher net\n",
    "    model.fit(x_train, y_train, \n",
    "              batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    \n",
    "    # teacher net model evaluation\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    loss, accuracy = score[0], score[1]\n",
    "    num_errors = int((1-score[1])*len(x_test))\n",
    "    print('On test set:')\n",
    "    print('loss = {}, accuracy = {}'.format(loss, accuracy))\n",
    "    print('#errors = {}'.format(num_errors))\n",
    "    \n",
    "    # save the fitted model\n",
    "    if save_model:\n",
    "        directory = './models/fakeset'\n",
    "        filename = os.path.join(directory, 'fake_teacher.h5')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        if model_name:\n",
    "            filename = os.path.join(directory, model_name)\n",
    "        model.save(filename)\n",
    "        print(\"model saved at {}\".format(filename))\n",
    "    \n",
    "    return model, loss, accuracy, num_errors\n",
    "\n",
    "def train_student(x_train, y_train, x_test, y_test, n_classes, n_hidden, \n",
    "                  batch_size=100, epochs=50, save_model=False, model_name=None):\n",
    "    '''\n",
    "    Function to build a smaller student net with T = 1 as baseline for distillation performance\n",
    "    ''' \n",
    "    model = FakeStudentNet(input_shape=(x_train.shape[1],), n_classes=n_classes, n_hidden=n_hidden, T=1)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    # train student\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    \n",
    "    # student net model evaluation\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    loss, accuracy = score[0], score[1]\n",
    "    num_errors = int((1-score[1])*len(x_test))\n",
    "    print('On test set:')\n",
    "    print('loss = {}, accuracy = {}'.format(loss, accuracy))\n",
    "    print('#errors = {}'.format(num_errors))\n",
    "    \n",
    "    # save the fitted model\n",
    "    if save_model:\n",
    "        directory = './models/fakeset'\n",
    "        filename = os.path.join(directory, 'fake_student.h5')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        if model_name:\n",
    "            filename = os.path.join(directory, model_name)\n",
    "        model.save(filename)\n",
    "        print(\"model saved at {}\".format(filename))\n",
    "    \n",
    "    return model, loss, accuracy, num_errors\n",
    "            \n",
    "def distill_student(x_train, y_train, x_test, y_test, n_classes, n_hidden, T, teacher_model, \n",
    "                    batch_size=100, epochs=50, save_model=False, model_name=None):\n",
    "    '''\n",
    "    Function to distill knowledge from the fitted 'teacher_model' to a student net under some T\n",
    "    ''' \n",
    "    # make the 'soft target' to train the student net\n",
    "    teacher_logit = Model(inputs=teacher_model.input, outputs=teacher_model.get_layer('logit').output)\n",
    "    logit_train = teacher_logit.predict(x_train)\n",
    "    y_train_soft = K.softmax(logit_train / T).eval(session=K.get_session())\n",
    "    \n",
    "    # build a baseline student net with T specified\n",
    "    model = FakeStudentNet(input_shape=(x_train.shape[1],), n_classes=n_classes, n_hidden=n_hidden, T=T)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    # train the student net with the 'soft target'\n",
    "    model.fit(x_train, y_train_soft, \n",
    "              batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    \n",
    "    # distilled student net model evaluation\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    loss, accuracy = score[0], score[1]\n",
    "    num_errors = int((1-score[1])*len(x_test))\n",
    "    print('On test set:')\n",
    "    print('loss = {}, accuracy = {}'.format(loss, accuracy))\n",
    "    print('#errors = {}'.format(num_errors))\n",
    "    \n",
    "    # save the fitted model\n",
    "    if save_model:\n",
    "        directory = './models/fakeset'\n",
    "        filename = os.path.join(directory, 'fake_distill_student.h5')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        if model_name:\n",
    "            filename = os.path.join(directory, model_name)\n",
    "        model.save(filename)\n",
    "        print(\"model saved at {}\".format(filename))\n",
    "    \n",
    "    return model, loss, accuracy, num_errors\n",
    "\n",
    "## function to perform 1 experiment at 1 data quality and varying temperatures\n",
    "def experiment(class_sep, p_informative, epochs, n_hidden, Ts):\n",
    "    '''\n",
    "    Function to run experiments on fake datasets generated from different hyper parameter sets\n",
    "    '''\n",
    "    # constant params for the fake dataset\n",
    "    n_samples = 50000\n",
    "    n_features = 100\n",
    "    n_classes = 10\n",
    "    n_clusters_per_class = 1\n",
    "    \n",
    "    # constant params for training\n",
    "    test_size = 0.2\n",
    "    random_state = 12345\n",
    "    batch_size=100\n",
    "    \n",
    "    # dictionaries for results\n",
    "    results = {\n",
    "        'experiment_meta': {},\n",
    "        'teacher': {},\n",
    "        'student': {},\n",
    "        'distilled_student': {}  \n",
    "    }\n",
    "    \n",
    "    # record results\n",
    "    results['experiment_meta']['n_samples'] = n_samples\n",
    "    results['experiment_meta']['n_features'] = n_features\n",
    "    results['experiment_meta']['n_classes'] = n_classes\n",
    "    results['experiment_meta']['n_clusters_per_class'] = n_clusters_per_class\n",
    "    results['experiment_meta']['test_size'] = test_size\n",
    "    results['experiment_meta']['random_state'] = random_state\n",
    "    results['experiment_meta']['batch_size'] = batch_size\n",
    "    \n",
    "    results['experiment_meta']['class_sep'] = class_sep\n",
    "    results['experiment_meta']['p_informative'] = p_informative\n",
    "    results['experiment_meta']['epochs'] = epochs\n",
    "    results['experiment_meta']['n_hidden'] = n_hidden\n",
    "    results['experiment_meta']['Ts'] = Ts\n",
    "    \n",
    "    # generate dataset\n",
    "    FakeDataset = FakeDataGenerator(n_samples, n_features, n_classes, n_clusters_per_class)\n",
    "    X, y = FakeDataset.generate_data(p_informative=p_informative, class_sep=class_sep)\n",
    "\n",
    "    # test train split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # standardize features\n",
    "    x_train, x_test = standardize(x_train, x_test)\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "    \n",
    "    # train teacher\n",
    "    print(\"training teacher...\")\n",
    "    teacher_model_filename = 'teacher_classsep={}_pinform={}.h5'.format(class_sep, p_informative)\n",
    "    teacher_model, teacher_loss, teacher_acc, teacher_nerr = train_teacher(x_train, y_train, x_test, y_test, \n",
    "                                                                           n_classes, batch_size=batch_size, \n",
    "                                                                           epochs=epochs, save_model=True, \n",
    "                                                                           model_name=teacher_model_filename)\n",
    "    \n",
    "    results['teacher']['loss'] = teacher_loss\n",
    "    results['teacher']['accuracy'] = teacher_acc\n",
    "    results['teacher']['num_err'] = teacher_nerr\n",
    "    \n",
    "    \n",
    "    # train basline student -- NO distillation\n",
    "    print(\"training student...\")\n",
    "    student_model_filename = 'student_classsep={}_pinform={}_nhidden={}.h5'.format(class_sep, p_informative, n_hidden)\n",
    "    student_model, student_loss, student_acc, student_nerr = train_student(x_train, y_train, x_test, y_test, \n",
    "                                                                           n_classes, n_hidden=n_hidden, \n",
    "                                                                           batch_size=batch_size, \n",
    "                                                                           epochs=epochs, save_model=True, \n",
    "                                                                           model_name=student_model_filename)\n",
    "    \n",
    "    results['student']['loss'] = student_loss\n",
    "    results['student']['accuracy'] = student_acc\n",
    "    results['student']['num_err'] = student_nerr\n",
    "    \n",
    "    # distill student -- WITH distillation\n",
    "    print(\"distilling student...\")\n",
    "    for T in Ts:\n",
    "        print(\"At T={}\".format(T))\n",
    "        distill_student_model_filename = 'distill_student_classsep={}_pinform={}_nhidden={}_T={}.h5'.format(class_sep, p_informative, n_hidden, T)\n",
    "        distill_student_model, d_stu_loss, d_stu_acc, d_stu_nerr = distill_student(x_train, y_train, x_test, y_test, \n",
    "                                                                                   n_classes, n_hidden=n_hidden, T=T, \n",
    "                                                                                   teacher_model=teacher_model, \n",
    "                                                                                   batch_size=batch_size, \n",
    "                                                                                   epochs=epochs, save_model=True, \n",
    "                                                                                   model_name=distill_student_model_filename)\n",
    "        results['distilled_student'][T] = {}\n",
    "        results['distilled_student'][T]['loss'] = d_stu_loss\n",
    "        results['distilled_student'][T]['accuracy'] = d_stu_acc\n",
    "        results['distilled_student'][T]['num_err'] = d_stu_nerr\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params for training\n",
    "epochs = 50\n",
    "n_hidden = 50 # num hidden nodes for student network\n",
    "\n",
    "# T to experiment for distillation\n",
    "Ts = [2, 2.5, 3, 4, 5, 10, 15, 20]\n",
    "class_sep_list = [0.2, 0.6, 1]\n",
    "p_informative_list = [0.5, 0.8, 1]\n",
    "var_combo = list(itertools.product(class_sep_list, p_informative_list))\n",
    "\n",
    "results_list = []\n",
    "for cs, pi in var_combo:\n",
    "    result = experiment(cs, pi, epochs, n_hidden, Ts)\n",
    "    results_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving results\n",
    "def dump_pkl(output_path, data):\n",
    "    bytes_out = cPickle.dumps(data)\n",
    "    with open(output_path, 'wb') as f_out:\n",
    "        for idx in range(0, len(bytes_out), MAX_BYTES):\n",
    "            f_out.write(bytes_out[idx:idx+MAX_BYTES])\n",
    "    print('data dumped to %s' % output_path)\n",
    "            \n",
    "for result in results_list:\n",
    "    class_sep = result['experiment_meta']['class_sep']\n",
    "    p_informative = result['experiment_meta']['p_informative']\n",
    "    \n",
    "    # filepath\n",
    "    directory = './results/fakeset'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filepath = os.path.join(directory, 'class_sep={}_p_info={}_results.json'.format(class_sep, p_informative))\n",
    "    dump_pkl(filepath, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading results\n",
    "def load_pkl(file_path):\n",
    "    bytes_in = bytearray(0)\n",
    "    input_size = os.path.getsize(file_path)\n",
    "    with open(file_path, 'rb') as f_in:\n",
    "        for _ in range(0, input_size, MAX_BYTES):\n",
    "            bytes_in += f_in.read(MAX_BYTES)\n",
    "    data = cPickle.loads(bytes_in)\n",
    "\n",
    "    return data\n",
    "\n",
    "directory = './results/fakeset'\n",
    "results_df = pd.DataFrame(columns=['class_sep', 'p_informative', 'model', \n",
    "                                  'temp', 'num_error', 'loss', 'accuracy'])\n",
    "\n",
    "# extract teacher information\n",
    "counter = 0\n",
    "for idx, (class_sep, p_informative) in enumerate(var_combo):\n",
    "    filepath = os.path.join(directory, 'class_sep={}_p_info={}_results.json'.format(class_sep, p_informative))\n",
    "    result = load_pkl(filepath)\n",
    "    teacher_entry = pd.Series({'class_sep': class_sep, 'p_informative': p_informative, 'model': 'teacher', \n",
    "             'temp': 1, 'num_error': result['teacher']['num_err'], 'loss': result['teacher']['loss'], \n",
    "             'accuracy': result['teacher']['accuracy']}, name= counter)\n",
    "    results_df = results_df.append(teacher_entry)\n",
    "    counter += 1\n",
    "    \n",
    "    student_entry = pd.Series({'class_sep': class_sep, 'p_informative': p_informative, 'model': 'student', \n",
    "             'temp': 1, 'num_error': result['student']['num_err'], 'loss': result['student']['loss'], \n",
    "             'accuracy': result['student']['accuracy']}, name= counter)\n",
    "    results_df = results_df.append(student_entry)\n",
    "    counter += 1\n",
    "    \n",
    "    for temp, data in result['distilled_student'].items():\n",
    "        entry = pd.Series({'class_sep': class_sep, 'p_informative': p_informative, 'model': 'distilled_student', \n",
    "             'temp': temp, 'num_error': data['num_err'], 'loss': data['loss'], \n",
    "             'accuracy': data['accuracy']}, name= counter)\n",
    "        results_df = results_df.append(entry)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>num_error</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_sep</th>\n",
       "      <th>p_informative</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.2</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.5</th>\n",
       "      <th>teacher</th>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>0.100470</td>\n",
       "      <td>0.9882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>0.256389</td>\n",
       "      <td>0.9412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilled_student</th>\n",
       "      <td>2</td>\n",
       "      <td>199</td>\n",
       "      <td>0.230405</td>\n",
       "      <td>0.9801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilled_student</th>\n",
       "      <td>2.5</td>\n",
       "      <td>213</td>\n",
       "      <td>0.362845</td>\n",
       "      <td>0.9786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilled_student</th>\n",
       "      <td>3</td>\n",
       "      <td>231</td>\n",
       "      <td>0.511031</td>\n",
       "      <td>0.9769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          temp num_error      loss  accuracy\n",
       "class_sep p_informative model                                               \n",
       "0.2       0.5           teacher              1       118  0.100470    0.9882\n",
       "                        student              1       587  0.256389    0.9412\n",
       "                        distilled_student    2       199  0.230405    0.9801\n",
       "                        distilled_student  2.5       213  0.362845    0.9786\n",
       "                        distilled_student    3       231  0.511031    0.9769"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.set_index(['class_sep', 'p_informative', 'model'], inplace=True)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABQcAAADlCAYAAADuiUEYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEuIfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15665b080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABQoAAADlCAYAAAAbd8qoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1565f2828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing results\n",
    "def viz_result(measure, v_min, v_max):\n",
    "    l = len(p_informative_list)\n",
    "    fig, ax = plt.subplots(1, 9, figsize=(20, 3))\n",
    "    plt.suptitle('Distillation {} vs Temperature'.format(measure), fontsize=16, weight='heavy')\n",
    "    plt.subplots_adjust(top=0.7, hspace=0.5, wspace=0.5)\n",
    "\n",
    "    for i, cs in enumerate(class_sep_list):\n",
    "        for j, pi in enumerate(p_informative_list):\n",
    "            ax[j+i*l].axhline(results_df.loc[(cs, pi, 'teacher'), measure].values[0], c='r', label='teacher')\n",
    "            ax[j+i*l].axhline(results_df.loc[(cs, pi, 'student'), measure].values[0], c='g', label='student_baseline')\n",
    "            ax[j+i*l].plot(results_df.loc[(cs, pi, 'distilled_student'), 'temp'].values, \n",
    "                           results_df.loc[(cs, pi, 'distilled_student'), measure].values, label='student_distilled')\n",
    "            ax[j+i*l].set_xlabel('T')\n",
    "            ax[j+i*l].set_ylabel(measure)\n",
    "            ax[j+i*l].set_ylim(v_min, v_max)\n",
    "            ax[j+i*l].set_title('class_sep = {}, \\np_informative = {}'.format(cs, pi))\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "viz_result('accuracy', 0.9, 1)\n",
    "viz_result('num_error', 100, 1050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion - Distillation Results on Fake Datasets\n",
    "\n",
    "We found that, while distilled student nets showed superior model performance over non-distilled student nets in all cases, data quality did not affect the optimal distillation temperature ($T_{opt} \\in (2.5, 5)$ in all cases). Based on the results reported in the paper: A $T_{opt}$ at 20, 8 and 2.5-4 was found for student nets with 800, 300 and 30 nodes per hidden layer, respectively, we suspect that optimal distillation temperature may be more correlated with student net's architecture complexity, where $T_{opt}$ is higher for more complex structures. This hypothesis remains to be tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
