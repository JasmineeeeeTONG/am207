{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APMTH 207: Advanced Scientific Computing: Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "**Harvard University** <br>\n",
    "**Fall 2018** <br>\n",
    "\n",
    "### Paper Tutorial: Distilling the Knowledge in a Neural Network\n",
    "- Authors: Geoffrey Hinton, Oriol Vinyals, Jeff Dean\n",
    "- Link: https://arxiv.org/pdf/1503.02531.pdf\n",
    "\n",
    "### Collaborators: Michelle (Chia Chi) Ho, Jiejun Lu, Jiawen Tong\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "<br>\n",
    "\n",
    "**Problem Statement & Motivation**\n",
    "\n",
    "The potential of deep neural net is increasingly realized in various industries, with which comes an increasing need to strategize model training in order to obtain powerful and deployable models. One strategy is to transfer the knowledge learned from a cumbersome but powerful (teacher) model to a smaller (student) model. Hinton et al. proposed an approach to knowledge transfer, which they coined \"distillation\", in \"Distilling the Knowledge in a Neural Network\". In this tutorial, we walk through the implementation and demonstrate the utility of knowledge distillation using the MNIST dataset and a fake dataset generated from the $\\texttt{sklearn}$ library. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Experiments on Full MNIST Data**\n",
    "\n",
    "For the MNIST dataset, we first trained a teacher network generally following the paper specifications, a neural net with 2 Dense layers, each with 1200 nodes. The teacher network made 168 errors on the test set. We then compared the performance of 3 different student networks, each composed of 2 hidden layers with 20 nodes: 1) Non-distilled student baseline, 2) Student distilled with soft targets at T=3 from the teacher; and 3) Student distilled with a weighted average bewteen soft target at T=3 and the cross-entropy loss of ground truth labels. The student networks made 368, 331 and 323 on the test set, respectively. We further trained a teacher network using a CNN model structure. The CNN teacher network showed significant improvement over the Dense teacher, making 97 errors on the test set. However, the student distilled from the CNN teacher performed only comparably to those distilled from the inferior Dense teacher net, making 340 errors.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Experiments on Partial MNIST Data (omitting 3; only keeping 7 & 8)** \n",
    "\n",
    "Having demonstrated the general utility of distillation, we proceeded to distill student networks where the transfer dataset omits certain digits. The idea is that the distillation process should allow the student to learn digits it has never seen before as the knowledge is distailled from a teacher net that has seen all the digits. We first omitted digit 3 from the transfer set and found that our best performing distilled student net was able to get 56% of the test digit 3 samples correct while the non-distilled student net did not get any of these samples right. Additionaly, when only digits 7 and 8 are included in the transfer set, our best performing distilled student net and the non-distilled student net achieved 63% and 20% overall accuracy. Our results recapitulates the paper's findings. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Distillation Applied to Fake Datasets**\n",
    "\n",
    "In the last part of this tutorial, we explored the relationship between data quality and the optimal distillation temperature. We used the $\\texttt{make_classification}$ function from the $\\texttt{sklearn}$ library to generate fake datasets with varying class separability and number of informative features. Data quality is presumed to be higher with higher class separability and higher number of informative features. At varying data quality, we compared the performance of a teacher, non-distilled student and students distilled at various temperatures. We found that, while distilled student nets showed superior model performance over non-distilled student nets in all cases, data quality did not affect the optimal distillation temperature ($T_{opt} \\in (2.5, 5)$). Based on the results reported in the paper: A $T_{opt}$ at 20, 8 and 2.5-4 was found for student nets with 800, 300 and 30 nodes per hidden layer, respectively, we suspect that optimal distillation temperature may be more correlated with student net's architecture complexity, where $T_{opt}$ is higher for more complex structures. This hypothesis remains to be tested. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Flatten, Dense, Dropout, Lambda, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import _pickle as cPickle\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_BYTES = 2**31 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Training a powerful model that accurately extracts structure from data is often at odds with training a deployable model as the former objective tends to result in large and cumbersome models while the latter has much more stringent requirements on latency and computational resources. Thus, while the potential of deep neural net is increasingly realized in various industries, there is also increasing need to strategize model training in order to obtain powerful and deployable models. The general goal is to transfer the knowledge of a cumbersome but powerful (teacher) model to a smaller (student) model for deployment in real applications. To do so, the authors of this paper proposed the idea of \"knowledge distillation\". In this tutorial, we will go through the implementation for knowledge distillation and demonstrate its effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation in the Math Form\n",
    "\n",
    "In general, knowledge transfer is done by transforming the logits produced by the teacher model at some temperature to a set of “soft targets” for training the student model. The authors proposed the “distillation” approach: raising the temperature of the final softmax until the teacher model produces a suitably smoother set of targets and using the same high temperature when training the student model to learn from these soft targets. \n",
    "\n",
    "$$q_i=\\dfrac{\\exp(\\frac{z_i}{T})}{\\sum_j \\exp(\\frac{z_j}{T})}$$\n",
    "\n",
    "The figure below visualizes the effect of raising temperature on the softmax layer: As temperature increases, the resulting class probability distribution \"softens\" (i.e. becomes flatter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a59feb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def softmax_temp(logit, temp):\n",
    "    return np.exp(logit/temp) / np.exp(logit/temp).sum()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "p = np.random.rand(10) * 50\n",
    "temps = [1, 10, 20, 50]\n",
    "for t in temps:\n",
    "    plt.plot(softmax_temp(p, t), label=\"T={}\".format(t))\n",
    "ax.set_title(\"Predicted Class Probability at Different Temperatures\", fontsize=16)\n",
    "ax.set_ylabel(\"Probability\", fontsize=12)\n",
    "ax.set_xlabel(\"Class\", fontsize=12)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments Part I on Full MNIST Data\n",
    "\n",
    "In this part of the tutorial, we used the full MNIST dataset to train and compare:\n",
    "\n",
    "- 2 teacher nets (a Dense 1200-1200-10 with heavy regularization and a CNN) \n",
    "- 4 student nets (Dense 20-20-10)\n",
    "    - Baseline (non-distilled)\n",
    "    - Distilled at T = 3, using cross-entropy loss with Dense teacher soft targets\n",
    "    - Distilled at T = 3, using weighted average between cross-entropy loss with Dense teacher soft targets and cross-entropy loss with the true target label\n",
    "    - Distilled at T = 3, using cross-entropy loss with CNN teacher\n",
    "\n",
    "The main objective for this part of the tutorial is to demonstrate the general utility of distillation (i.e. distilled student nets are expected to perform better than the baseline (non-distilled) student net. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load the MNIST data. There are 60000 and 10000 training and test samples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# load mnist data & normalize to 0-1\n",
    "# 1D data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28).astype('float32')\n",
    "x_test = x_test.reshape(-1, 28*28).astype('float32')\n",
    "x_train /= 255.\n",
    "x_test /= 255. \n",
    "\n",
    "# 2D data\n",
    "(x_train_2D, _), (x_test_2D, _) = mnist.load_data()\n",
    "x_train_2D = x_train_2D.reshape(x_train_2D.shape[0], 1, 28, 28).astype('float32')\n",
    "x_test_2D = x_test_2D.reshape(x_test_2D.shape[0], 1, 28, 28).astype('float32')\n",
    "x_train_2D /= 255.\n",
    "x_test_2D /= 255.\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Teacher Nets\n",
    "The two teacher nets we train are:\n",
    "\n",
    "1. DENSE 1200-1200-10 with HEAVY REGULARIZATION\n",
    "2. CNN\n",
    "\n",
    "The first Dense teacher closely follows the specifications of the teacher net described in the paper. Since image data is typically handled using CNNs, we also trained the second CNN teacher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Teacher Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "k_constraint = keras.constraints.MaxNorm(max_value=15, axis=0)\n",
    "\n",
    "# Dense teacher model\n",
    "mnist_dense = Sequential()\n",
    "mnist_dense.add(Dense(1200, name='hidden_1', input_shape=(28*28, ), activation='relu', kernel_initializer=k_init, kernel_constraint=k_constraint))\n",
    "mnist_dense.add(Dropout(0.7, name='dropout_1'))\n",
    "mnist_dense.add(Dense(1200, name='hidden_2', activation='relu', kernel_initializer=k_init, kernel_constraint=k_constraint))\n",
    "mnist_dense.add(Dropout(0.7, name='dropout_2'))\n",
    "mnist_dense.add(Dense(10, name='logit'))\n",
    "mnist_dense.add(Activation('softmax', name='softmax'))\n",
    "\n",
    "mnist_dense.compile(loss=categorical_crossentropy, optimizer=Adam(0.0001), metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "mnist_dense.fit(x_train, y_train, batch_size=100, epochs=20, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "loss, accuracy = mnist_dense.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DENSE TECHERT NET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "\n",
    "# save model\n",
    "mnist_dense.save('./models/mnist_teacher_dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENSE TECHERT NET - On test set:\n",
      "loss = 0.055868843965092674, accuracy = 0.9832, #errors = 168\n"
     ]
    }
   ],
   "source": [
    "# load model and print test performance\n",
    "mnist_dense = load_model('./models/mnist_teacher_dense.h5')\n",
    "loss, accuracy = mnist_dense.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DENSE TECHERT NET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Teacher net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN teacher model\n",
    "mnist_cnn = Sequential()\n",
    "\n",
    "mnist_cnn.add(Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(1, 28, 28), name='conv_1'))\n",
    "mnist_cnn.add(Conv2D(128, (3, 3), activation='relu', padding='same', name='conv_2'))\n",
    "mnist_cnn.add(MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_1'))\n",
    "mnist_cnn.add(Dropout(0.25, name='dropout_1'))\n",
    "\n",
    "mnist_cnn.add(Flatten())\n",
    "mnist_cnn.add(Dense(10, name='logit'))\n",
    "mnist_cnn.add(Activation('softmax', name='softmax'))\n",
    "\n",
    "mnist_cnn.compile(loss=categorical_crossentropy, optimizer=Adam(lr=0.0005), metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "mnist_cnn.fit(x_train_2D, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test_2D, y_test))\n",
    "\n",
    "# save model\n",
    "mnist_cnn.save('./models/mnist_teacher_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN TEACHER NET - On test set:\n",
      "loss = 0.030440742732951186, accuracy = 0.9903, #errors = 97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model and print test performance\n",
    "mnist_cnn = load_model('./models/mnist_teacher_cnn.h5')\n",
    "loss, accuracy = mnist_cnn.evaluate(x_test_2D, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('CNN TEACHER NET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Net\n",
    "\n",
    "All the student nets we trained share similar model architecture: \n",
    "- 2 dense layers, each with 20 hidden nodes\n",
    "- the logit outputs before the original softmax transformation are normalized over a pre-specified temperature `T`\n",
    "> - `T = 1` for hard targets\n",
    "> - `T > 1` for distillation\n",
    "\n",
    "This model architecture is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MNIST_StudentNet(n_hidden, T):\n",
    "    '''\n",
    "    Function to build a studnet net\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "    model.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "    model.add(Dense(10, name='logit'))\n",
    "    model.add(Lambda(lambda x: x / T, name='logit_soft'))\n",
    "    model.add(Activation('softmax', name='softmax'))\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Student Net 1 - Baseline, No Distillation (T = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.5879 - acc: 0.8240 - val_loss: 0.2767 - val_acc: 0.9236\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2539 - acc: 0.9275 - val_loss: 0.2207 - val_acc: 0.9364\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2081 - acc: 0.9404 - val_loss: 0.2061 - val_acc: 0.9383\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1822 - acc: 0.9475 - val_loss: 0.1834 - val_acc: 0.9473\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1647 - acc: 0.9527 - val_loss: 0.1707 - val_acc: 0.9512\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1513 - acc: 0.9568 - val_loss: 0.1581 - val_acc: 0.9537\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1416 - acc: 0.9590 - val_loss: 0.1510 - val_acc: 0.9555\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1334 - acc: 0.9610 - val_loss: 0.1466 - val_acc: 0.9585\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1252 - acc: 0.9639 - val_loss: 0.1440 - val_acc: 0.9600\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1189 - acc: 0.9655 - val_loss: 0.1362 - val_acc: 0.9612\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1133 - acc: 0.9667 - val_loss: 0.1392 - val_acc: 0.9615\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1083 - acc: 0.9678 - val_loss: 0.1390 - val_acc: 0.9617\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1036 - acc: 0.9699 - val_loss: 0.1450 - val_acc: 0.9586\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1020 - acc: 0.9694 - val_loss: 0.1338 - val_acc: 0.9632\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0958 - acc: 0.9713 - val_loss: 0.1309 - val_acc: 0.9635\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0941 - acc: 0.9716 - val_loss: 0.1286 - val_acc: 0.9641\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0900 - acc: 0.9736 - val_loss: 0.1339 - val_acc: 0.9625\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0871 - acc: 0.9738 - val_loss: 0.1335 - val_acc: 0.9632\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0853 - acc: 0.9740 - val_loss: 0.1326 - val_acc: 0.9642\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0819 - acc: 0.9753 - val_loss: 0.1453 - val_acc: 0.9601\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0811 - acc: 0.9754 - val_loss: 0.1321 - val_acc: 0.9634\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0784 - acc: 0.9764 - val_loss: 0.1355 - val_acc: 0.9640\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0752 - acc: 0.9770 - val_loss: 0.1341 - val_acc: 0.9651\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0740 - acc: 0.9781 - val_loss: 0.1342 - val_acc: 0.9643\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0721 - acc: 0.9781 - val_loss: 0.1372 - val_acc: 0.9609\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0709 - acc: 0.9786 - val_loss: 0.1365 - val_acc: 0.9641\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0682 - acc: 0.9793 - val_loss: 0.1293 - val_acc: 0.9660\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0675 - acc: 0.9792 - val_loss: 0.1418 - val_acc: 0.9628\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0662 - acc: 0.9801 - val_loss: 0.1349 - val_acc: 0.9649\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0642 - acc: 0.9804 - val_loss: 0.1387 - val_acc: 0.9641\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0638 - acc: 0.9803 - val_loss: 0.1356 - val_acc: 0.9649\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0616 - acc: 0.9810 - val_loss: 0.1305 - val_acc: 0.9645\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0603 - acc: 0.9811 - val_loss: 0.1395 - val_acc: 0.9627\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0588 - acc: 0.9823 - val_loss: 0.1358 - val_acc: 0.9643\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0581 - acc: 0.9817 - val_loss: 0.1384 - val_acc: 0.9634\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0572 - acc: 0.9827 - val_loss: 0.1403 - val_acc: 0.9619\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0558 - acc: 0.9824 - val_loss: 0.1473 - val_acc: 0.9649\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0548 - acc: 0.9831 - val_loss: 0.1422 - val_acc: 0.9655\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0540 - acc: 0.9832 - val_loss: 0.1416 - val_acc: 0.9635\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0533 - acc: 0.9833 - val_loss: 0.1490 - val_acc: 0.9626\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0510 - acc: 0.9844 - val_loss: 0.1540 - val_acc: 0.9628\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0511 - acc: 0.9842 - val_loss: 0.1468 - val_acc: 0.9646\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0507 - acc: 0.9839 - val_loss: 0.1540 - val_acc: 0.9617\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0500 - acc: 0.9845 - val_loss: 0.1505 - val_acc: 0.9632\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0477 - acc: 0.9849 - val_loss: 0.1481 - val_acc: 0.9637\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0471 - acc: 0.9851 - val_loss: 0.1564 - val_acc: 0.9617\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0475 - acc: 0.9858 - val_loss: 0.1571 - val_acc: 0.9627\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0451 - acc: 0.9857 - val_loss: 0.1531 - val_acc: 0.9640\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0452 - acc: 0.9859 - val_loss: 0.1527 - val_acc: 0.9642\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0445 - acc: 0.9862 - val_loss: 0.1535 - val_acc: 0.9632\n",
      "STUDENT BASELINE - On test set:\n",
      "loss = 0.15354005151256278, accuracy = 0.9632, #errors = 368\n"
     ]
    }
   ],
   "source": [
    "# train baseline student net - NO distillation\n",
    "mnist_student_basline = MNIST_StudentNet(n_hidden=20, T=1)\n",
    "mnist_student_basline.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "mnist_student_basline.fit(x_train, y_train, batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# baseline student net model evaluation\n",
    "loss, accuracy = mnist_student_basline.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('STUDENT BASELINE - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Student Net 2 - distilling from Dense teacher at T = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 1.0716 - acc: 0.7981 - val_loss: 0.4087 - val_acc: 0.9109\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.7406 - acc: 0.9192 - val_loss: 0.3516 - val_acc: 0.9244\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6985 - acc: 0.9355 - val_loss: 0.3108 - val_acc: 0.9385\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6782 - acc: 0.9445 - val_loss: 0.2938 - val_acc: 0.9424\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6675 - acc: 0.9500 - val_loss: 0.2873 - val_acc: 0.9466\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6602 - acc: 0.9539 - val_loss: 0.2771 - val_acc: 0.9499\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6537 - acc: 0.9577 - val_loss: 0.2674 - val_acc: 0.9515\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6482 - acc: 0.9609 - val_loss: 0.2622 - val_acc: 0.9542\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6437 - acc: 0.9630 - val_loss: 0.2583 - val_acc: 0.9566\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6400 - acc: 0.9657 - val_loss: 0.2548 - val_acc: 0.9580\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6370 - acc: 0.9672 - val_loss: 0.2530 - val_acc: 0.9579\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6344 - acc: 0.9685 - val_loss: 0.2536 - val_acc: 0.9596\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6326 - acc: 0.9696 - val_loss: 0.2444 - val_acc: 0.9595\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6310 - acc: 0.9707 - val_loss: 0.2426 - val_acc: 0.9602\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6294 - acc: 0.9718 - val_loss: 0.2431 - val_acc: 0.9619\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6283 - acc: 0.9723 - val_loss: 0.2426 - val_acc: 0.9633\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6270 - acc: 0.9731 - val_loss: 0.2434 - val_acc: 0.9630\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6262 - acc: 0.9729 - val_loss: 0.2435 - val_acc: 0.9633\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6252 - acc: 0.9739 - val_loss: 0.2385 - val_acc: 0.9642\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6246 - acc: 0.9742 - val_loss: 0.2331 - val_acc: 0.9662\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6238 - acc: 0.9746 - val_loss: 0.2377 - val_acc: 0.9657\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6232 - acc: 0.9748 - val_loss: 0.2324 - val_acc: 0.9654\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6226 - acc: 0.9751 - val_loss: 0.2359 - val_acc: 0.9647\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6221 - acc: 0.9757 - val_loss: 0.2361 - val_acc: 0.9649\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6214 - acc: 0.9760 - val_loss: 0.2332 - val_acc: 0.9656\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6211 - acc: 0.9765 - val_loss: 0.2355 - val_acc: 0.9654\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6206 - acc: 0.9767 - val_loss: 0.2327 - val_acc: 0.9650\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6202 - acc: 0.9770 - val_loss: 0.2367 - val_acc: 0.9659\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6200 - acc: 0.9771 - val_loss: 0.2314 - val_acc: 0.9661\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6195 - acc: 0.9775 - val_loss: 0.2324 - val_acc: 0.9649\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6195 - acc: 0.9778 - val_loss: 0.2298 - val_acc: 0.9653\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6189 - acc: 0.9776 - val_loss: 0.2333 - val_acc: 0.9654\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6186 - acc: 0.9783 - val_loss: 0.2264 - val_acc: 0.9650\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.6183 - acc: 0.9780 - val_loss: 0.2269 - val_acc: 0.9661\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6180 - acc: 0.9779 - val_loss: 0.2290 - val_acc: 0.9652\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6178 - acc: 0.9783 - val_loss: 0.2296 - val_acc: 0.9645\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6175 - acc: 0.9783 - val_loss: 0.2296 - val_acc: 0.9655\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6174 - acc: 0.9783 - val_loss: 0.2316 - val_acc: 0.9657\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6172 - acc: 0.9786 - val_loss: 0.2299 - val_acc: 0.9657\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6170 - acc: 0.9792 - val_loss: 0.2304 - val_acc: 0.9655\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6167 - acc: 0.9791 - val_loss: 0.2293 - val_acc: 0.9661\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6166 - acc: 0.9790 - val_loss: 0.2291 - val_acc: 0.9661\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6163 - acc: 0.9789 - val_loss: 0.2317 - val_acc: 0.9665\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6163 - acc: 0.9793 - val_loss: 0.2289 - val_acc: 0.9661\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6160 - acc: 0.9792 - val_loss: 0.2295 - val_acc: 0.9668\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6160 - acc: 0.9794 - val_loss: 0.2285 - val_acc: 0.9655\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6157 - acc: 0.9796 - val_loss: 0.2269 - val_acc: 0.9667\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6155 - acc: 0.9796 - val_loss: 0.2301 - val_acc: 0.9658\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.6154 - acc: 0.9796 - val_loss: 0.2267 - val_acc: 0.9667\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.6154 - acc: 0.9796 - val_loss: 0.2285 - val_acc: 0.9669\n",
      "DISTILLED STUDENT - On test set:\n",
      "loss = 0.22846840821504594, accuracy = 0.9669, #errors = 331\n"
     ]
    }
   ],
   "source": [
    "def get_layer_output(model, layer_name):\n",
    "    output = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    return output\n",
    "\n",
    "# compute 'soft target'\n",
    "T = 3\n",
    "teacher_logit = get_layer_output(mnist_dense, 'logit')\n",
    "logit_train = teacher_logit.predict(x_train)\n",
    "y_train_soft = K.softmax(logit_train / T).eval(session=K.get_session())\n",
    "\n",
    "# train student net distilled from the dense teacher net\n",
    "mnist_student_distilled = MNIST_StudentNet(n_hidden=20, T=T)\n",
    "mnist_student_distilled.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "mnist_student_distilled.fit(x_train, y_train_soft, \n",
    "                            batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# distilled student net model evaluation\n",
    "loss, accuracy = mnist_student_distilled.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DISTILLED STUDENT - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Student Net 3 - Distilling from weighted loss of both the soft target and the true labels' cross-entropy at T = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_mix_loss(y_true, y_pred, w, T):    \n",
    "    # split hard & soft targets\n",
    "    y_hard, y_soft = y_true[:, :10], y_true[:, 10:]\n",
    "    \n",
    "    # convert logits to predicted values\n",
    "    y_hard_pred = K.softmax(y_pred) # hard target\n",
    "    y_soft_pred = K.softmax(y_pred / T) # soft target\n",
    "    \n",
    "    # compute weighted avg of the 2 parts of losses\n",
    "    avg_loss = w * categorical_crossentropy(y_hard, y_hard_pred) + categorical_crossentropy(y_soft, y_soft_pred)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 1.0612 - acc: 0.8294 - val_loss: 0.7709 - val_acc: 0.9172\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.7508 - acc: 0.9234 - val_loss: 0.7099 - val_acc: 0.9344\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.7103 - acc: 0.9362 - val_loss: 0.6848 - val_acc: 0.9429\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.6903 - acc: 0.9442 - val_loss: 0.6726 - val_acc: 0.9472\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6769 - acc: 0.9497 - val_loss: 0.6657 - val_acc: 0.9481\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6675 - acc: 0.9533 - val_loss: 0.6559 - val_acc: 0.9543\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6604 - acc: 0.9565 - val_loss: 0.6514 - val_acc: 0.9554\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6549 - acc: 0.9590 - val_loss: 0.6480 - val_acc: 0.9580\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6504 - acc: 0.9612 - val_loss: 0.6445 - val_acc: 0.9614\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.6469 - acc: 0.9632 - val_loss: 0.6416 - val_acc: 0.9627\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.6441 - acc: 0.9647 - val_loss: 0.6406 - val_acc: 0.9614\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6418 - acc: 0.9658 - val_loss: 0.6385 - val_acc: 0.9615\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.6399 - acc: 0.9669 - val_loss: 0.6365 - val_acc: 0.9646\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6383 - acc: 0.9686 - val_loss: 0.6354 - val_acc: 0.9636\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6367 - acc: 0.9690 - val_loss: 0.6353 - val_acc: 0.9644\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6355 - acc: 0.9696 - val_loss: 0.6350 - val_acc: 0.9637\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6342 - acc: 0.9704 - val_loss: 0.6352 - val_acc: 0.9631\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.6332 - acc: 0.9707 - val_loss: 0.6333 - val_acc: 0.9655\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6323 - acc: 0.9712 - val_loss: 0.6325 - val_acc: 0.9656\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6314 - acc: 0.9715 - val_loss: 0.6317 - val_acc: 0.9651\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6308 - acc: 0.9723 - val_loss: 0.6311 - val_acc: 0.9644\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.6300 - acc: 0.9724 - val_loss: 0.6305 - val_acc: 0.9662\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6293 - acc: 0.9729 - val_loss: 0.6297 - val_acc: 0.9663\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6288 - acc: 0.9725 - val_loss: 0.6298 - val_acc: 0.9652\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6281 - acc: 0.9737 - val_loss: 0.6295 - val_acc: 0.9652\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.6277 - acc: 0.9737 - val_loss: 0.6286 - val_acc: 0.9662\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.6271 - acc: 0.9743 - val_loss: 0.6288 - val_acc: 0.9663\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.6267 - acc: 0.9744 - val_loss: 0.6287 - val_acc: 0.9655\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6265 - acc: 0.9747 - val_loss: 0.6286 - val_acc: 0.9673\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6259 - acc: 0.9748 - val_loss: 0.6272 - val_acc: 0.9674\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.6256 - acc: 0.9751 - val_loss: 0.6280 - val_acc: 0.9664\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.6250 - acc: 0.9756 - val_loss: 0.6275 - val_acc: 0.9667\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.6246 - acc: 0.9755 - val_loss: 0.6270 - val_acc: 0.9673\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.6243 - acc: 0.9757 - val_loss: 0.6258 - val_acc: 0.9676\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.6240 - acc: 0.9756 - val_loss: 0.6265 - val_acc: 0.9674\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.6236 - acc: 0.9762 - val_loss: 0.6257 - val_acc: 0.9682\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.6235 - acc: 0.9756 - val_loss: 0.6262 - val_acc: 0.9678\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.6229 - acc: 0.9767 - val_loss: 0.6287 - val_acc: 0.9661\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.6227 - acc: 0.9765 - val_loss: 0.6252 - val_acc: 0.9679\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.6223 - acc: 0.9769 - val_loss: 0.6250 - val_acc: 0.9673\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6221 - acc: 0.9772 - val_loss: 0.6259 - val_acc: 0.9669\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6218 - acc: 0.9769 - val_loss: 0.6258 - val_acc: 0.9667\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6216 - acc: 0.9774 - val_loss: 0.6243 - val_acc: 0.9689\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6213 - acc: 0.9777 - val_loss: 0.6261 - val_acc: 0.9678\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6211 - acc: 0.9779 - val_loss: 0.6247 - val_acc: 0.9673\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.6208 - acc: 0.9773 - val_loss: 0.6243 - val_acc: 0.9673\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6206 - acc: 0.9781 - val_loss: 0.6239 - val_acc: 0.9681\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6204 - acc: 0.9778 - val_loss: 0.6239 - val_acc: 0.9673\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.6202 - acc: 0.9782 - val_loss: 0.6249 - val_acc: 0.9670\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.6200 - acc: 0.9780 - val_loss: 0.6234 - val_acc: 0.9676\n",
      "DISTILLED STUDENT with WEIGHTED HARD/SOFT TARGET - On test set:\n",
      "loss = 0.6234446800231933, accuracy = 0.9676, #errors = 323\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "T = 3\n",
    "w = 0.7 / (T**2)\n",
    "\n",
    "# apply both hard & soft targets to learn\n",
    "logit_test = teacher_logit.predict(x_test)\n",
    "y_test_soft = K.softmax(logit_test / T).eval(session=K.get_session())\n",
    "y_hard_soft_train = np.concatenate((y_train, y_train_soft), axis=1)\n",
    "y_hard_soft_test = np.concatenate((y_test, y_test_soft), axis=1)\n",
    "\n",
    "# fit the student net distilled from the dense teacher net with the hard-soft weighted avg loss\n",
    "mnist_student_mix = Sequential()\n",
    "mnist_student_mix.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "mnist_student_mix.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "mnist_student_mix.add(Dense(10, name='logit'))\n",
    "\n",
    "# y_pred at the end of 10-node dense layer is the logit_pred\n",
    "mnist_student_mix.compile(loss=lambda y_true, y_pred: avg_mix_loss(y_true, y_pred, w, T), \n",
    "                          optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "mnist_student_mix.fit(x_train, y_hard_soft_train, \n",
    "                      batch_size=100, epochs=50, verbose=1, validation_data=(x_test, y_hard_soft_test))\n",
    "\n",
    "\n",
    "# distilled student net with mix-hard-soft loss model evaluation\n",
    "loss, accuracy = mnist_student_mix.evaluate(x_test, y_hard_soft_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DISTILLED STUDENT with WEIGHTED HARD/SOFT TARGET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Student Net 4 - Distilling from CNN teacher at T = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.9567 - acc: 0.7943 - val_loss: 0.3894 - val_acc: 0.9022\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.6094 - acc: 0.9093 - val_loss: 0.3409 - val_acc: 0.9193\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.5729 - acc: 0.9221 - val_loss: 0.3056 - val_acc: 0.9297\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.5469 - acc: 0.9314 - val_loss: 0.2815 - val_acc: 0.9345\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.5277 - acc: 0.9388 - val_loss: 0.2654 - val_acc: 0.9405\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.5138 - acc: 0.9443 - val_loss: 0.2596 - val_acc: 0.9394\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.5043 - acc: 0.9469 - val_loss: 0.2498 - val_acc: 0.9442\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4966 - acc: 0.9500 - val_loss: 0.2448 - val_acc: 0.9450\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.4899 - acc: 0.9527 - val_loss: 0.2362 - val_acc: 0.9482\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.4847 - acc: 0.9547 - val_loss: 0.2314 - val_acc: 0.9488\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4802 - acc: 0.9568 - val_loss: 0.2281 - val_acc: 0.9496\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4763 - acc: 0.9582 - val_loss: 0.2175 - val_acc: 0.9525\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4728 - acc: 0.9597 - val_loss: 0.2181 - val_acc: 0.9538\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4700 - acc: 0.9607 - val_loss: 0.2139 - val_acc: 0.9541\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4671 - acc: 0.9619 - val_loss: 0.2099 - val_acc: 0.9572\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4647 - acc: 0.9629 - val_loss: 0.2153 - val_acc: 0.9560\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4626 - acc: 0.9633 - val_loss: 0.2038 - val_acc: 0.9585\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4600 - acc: 0.9653 - val_loss: 0.2099 - val_acc: 0.9578\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4589 - acc: 0.9658 - val_loss: 0.1965 - val_acc: 0.9605\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4565 - acc: 0.9666 - val_loss: 0.1949 - val_acc: 0.9591\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4550 - acc: 0.9675 - val_loss: 0.1955 - val_acc: 0.9605\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4533 - acc: 0.9680 - val_loss: 0.2008 - val_acc: 0.9591\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4520 - acc: 0.9688 - val_loss: 0.1958 - val_acc: 0.9623\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4510 - acc: 0.9693 - val_loss: 0.1948 - val_acc: 0.9615\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.4496 - acc: 0.9699 - val_loss: 0.1884 - val_acc: 0.9617\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4485 - acc: 0.9703 - val_loss: 0.1899 - val_acc: 0.9618\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4473 - acc: 0.9711 - val_loss: 0.1880 - val_acc: 0.9616\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4462 - acc: 0.9716 - val_loss: 0.1957 - val_acc: 0.9606\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4456 - acc: 0.9716 - val_loss: 0.1828 - val_acc: 0.9638\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4448 - acc: 0.9725 - val_loss: 0.1899 - val_acc: 0.9607\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4443 - acc: 0.9728 - val_loss: 0.1916 - val_acc: 0.9617\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4432 - acc: 0.9729 - val_loss: 0.1911 - val_acc: 0.9619\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4423 - acc: 0.9729 - val_loss: 0.1863 - val_acc: 0.9610\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4417 - acc: 0.9737 - val_loss: 0.1847 - val_acc: 0.9640\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4413 - acc: 0.9737 - val_loss: 0.1861 - val_acc: 0.9636\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4403 - acc: 0.9746 - val_loss: 0.1855 - val_acc: 0.9633\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4396 - acc: 0.9743 - val_loss: 0.1871 - val_acc: 0.9641\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4391 - acc: 0.9742 - val_loss: 0.1858 - val_acc: 0.9645\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.4386 - acc: 0.9751 - val_loss: 0.1814 - val_acc: 0.9655\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4377 - acc: 0.9752 - val_loss: 0.1776 - val_acc: 0.9671\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4372 - acc: 0.9754 - val_loss: 0.1795 - val_acc: 0.9668\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4368 - acc: 0.9757 - val_loss: 0.1867 - val_acc: 0.9655\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4362 - acc: 0.9762 - val_loss: 0.1837 - val_acc: 0.9647\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4357 - acc: 0.9769 - val_loss: 0.1837 - val_acc: 0.9662\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4353 - acc: 0.9765 - val_loss: 0.1863 - val_acc: 0.9648\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4348 - acc: 0.9770 - val_loss: 0.1758 - val_acc: 0.9672\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4345 - acc: 0.9770 - val_loss: 0.1792 - val_acc: 0.9677\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4340 - acc: 0.9775 - val_loss: 0.1803 - val_acc: 0.9653\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.4336 - acc: 0.9773 - val_loss: 0.1770 - val_acc: 0.9668\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4335 - acc: 0.9776 - val_loss: 0.1806 - val_acc: 0.9660\n",
      "DISTILLED STUDENT from CNN TEACHER - On test set:\n",
      "loss = 0.1805808451652527, accuracy = 0.966, #errors = 340\n"
     ]
    }
   ],
   "source": [
    "# compute 'soft target' of the CNN teacher net\n",
    "T = 3\n",
    "teacher_logit_cnn = get_layer_output(mnist_cnn, 'logit')\n",
    "logit_train_cnn = teacher_logit_cnn.predict(x_train_2D)\n",
    "y_train_soft_cnn = K.softmax(logit_train_cnn / T).eval(session=K.get_session())\n",
    "\n",
    "# train student net distilled from the CNN teacher net\n",
    "mnist_student_distilled_cnn = MNIST_StudentNet(n_hidden=20, T=T)\n",
    "mnist_student_distilled_cnn.fit(x_train, y_train_soft_cnn, \n",
    "                                batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# student net model evalutation\n",
    "loss, accuracy = mnist_student_distilled_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DISTILLED STUDENT from CNN TEACHER - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of student net performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Student Net Performance on Full Test Set ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.153540</td>\n",
       "      <td>368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_distilled</th>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.180581</td>\n",
       "      <td>340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.228468</td>\n",
       "      <td>331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.9676</td>\n",
       "      <td>0.623445</td>\n",
       "      <td>323.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy      loss  num_error\n",
       "baseline                    0.9632  0.153540      368.0\n",
       "cnn_distilled               0.9660  0.180581      340.0\n",
       "dense_distilled             0.9669  0.228468      331.0\n",
       "dense_distilled_mix_loss    0.9676  0.623445      323.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "student_summary = {\n",
    "    'baseline': {'loss': 0.15354005151256278, 'accuracy': 0.9632, 'num_error': 368},\n",
    "    'dense_distilled': {'loss': 0.22846840821504594, 'accuracy':  0.9669, 'num_error': 331},\n",
    "    'dense_distilled_mix_loss': {'loss': 0.6234446800231933, 'accuracy': 0.9676, 'num_error': 323},\n",
    "    'cnn_distilled': {'loss': 0.1805808451652527, 'accuracy': 0.966, 'num_error': 340}\n",
    "}\n",
    "\n",
    "print('=== Student Net Performance on Full Test Set ===')\n",
    "df_student_summary = pd.DataFrame().from_dict(student_summary).T\n",
    "display(df_student_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions - Experiments on Full MNIST Data\n",
    "\n",
    "In the paper, the digits were first jittered by up to two pixels in any direction to make the classification task more difficult so that the teacher and the student would show observable differences in their learning capacities. Without jittering the digits in our implementation, we found that a student net with relatively complex structure (E.g., 800-800-10) can learn from the ground truth labels very well without distillation. Therefore, to better show the effect of distillation, we chose a much simpler student net structure: 20-20-10. \n",
    "\n",
    "For the teacher net, we trained a CNN model in addition to the Dense model (1200-1200-10) reported in the paper because CNN structure is the conventional and more effective method to handle image data. Indeed, we found that the Dense and CNN teacher net achieved a test accuracy = 0.9832 (168 errors) and 0.9903 (97 errors), respectively. While non-distilled student baseline achieved an accuracy = 0.9632 (368 errors), learning from the Dense and the CNN teacher at T=3, the distilled students achieved an accuracy = 0.9669 (331 errors) and 0.9660 (340 errors), respectively. This demonstrates that 1) distillation improves student model performance; and 2) Distilling from a superior model (CNN teacher) does not necessarily translate into superior student model performance. By learning from both the ground truth label and the Dense teacher (with the ground truth weighted more heavily at a weight = 0.7), the performance of the distilled student further improved to a test accuracy = 0.9676 (323 errors).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Part II-a on Partial MNIST Data -- omitting digit 3 in the transfer set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we’ve shown that the distilled student net performs better than one without distillation. In the following, our experiments showed that the distilled student can even learn from a transfer set without all of the digits, 0-9. When omitting or keeping certain digits from the transfer set, a non-distilled student net cannot generalize and is unable to understand digits it has never seen. However, distillation presumably enables the student to learn from the teacher's knowledge on how to distinguish all of the digits with the teacher's soft target. The amount of information from the teacher's soft target is controlled by the temperature parameter.\n",
    "\n",
    "In HW7, we built an MLP model on MNIST. We found that on the training set, digit 3 is one of the most easily misclassified label as it is similar with digit 5, 8 and 2. A well trained teacher net on the full training data is expected to have captured the characteristics of digit 3 and its relative similarity with other labels. In the following experiment, we omitted all examples of digit 3 from the transfer set and trained three student nets: 1) non-distilled baseline, 2) distilled at T = 3, and 3) distilled at T = 3 with a mixture of ground truth and teacher soft target losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mis_hist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide code to omit the digit 3 from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = (y_train[:,3] == 1)\n",
    "x_train_omit3 = x_train[~idx,:]\n",
    "y_train_omit3 = y_train[~idx,:]\n",
    "y_train_soft_omit3 = y_train_soft[~idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Student Net 1 - Baseline, No Distillation (T = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.5718 - acc: 0.8305 - val_loss: 1.4462 - val_acc: 0.8312\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.2327 - acc: 0.9334 - val_loss: 1.5448 - val_acc: 0.8428\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.1900 - acc: 0.9450 - val_loss: 1.5695 - val_acc: 0.8502\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.1638 - acc: 0.9522 - val_loss: 1.5985 - val_acc: 0.8567\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.1444 - acc: 0.9577 - val_loss: 1.5859 - val_acc: 0.8601\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.1322 - acc: 0.9611 - val_loss: 1.6107 - val_acc: 0.8616\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.1217 - acc: 0.9641 - val_loss: 1.6145 - val_acc: 0.8633\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.1145 - acc: 0.9662 - val_loss: 1.6117 - val_acc: 0.8652\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.1081 - acc: 0.9681 - val_loss: 1.6300 - val_acc: 0.8622\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.1016 - acc: 0.9694 - val_loss: 1.6463 - val_acc: 0.8649\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0965 - acc: 0.9709 - val_loss: 1.6483 - val_acc: 0.8636\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0929 - acc: 0.9717 - val_loss: 1.6629 - val_acc: 0.8655\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0877 - acc: 0.9738 - val_loss: 1.6667 - val_acc: 0.8635\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0847 - acc: 0.9742 - val_loss: 1.6872 - val_acc: 0.8669\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0804 - acc: 0.9756 - val_loss: 1.6984 - val_acc: 0.8665\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0775 - acc: 0.9766 - val_loss: 1.6992 - val_acc: 0.8680\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0747 - acc: 0.9766 - val_loss: 1.7104 - val_acc: 0.8680\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0711 - acc: 0.9783 - val_loss: 1.7139 - val_acc: 0.8679\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0691 - acc: 0.9787 - val_loss: 1.7228 - val_acc: 0.8677\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0660 - acc: 0.9800 - val_loss: 1.7193 - val_acc: 0.8701\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0633 - acc: 0.9799 - val_loss: 1.7208 - val_acc: 0.8686\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0618 - acc: 0.9815 - val_loss: 1.7233 - val_acc: 0.8697\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 18us/step - loss: 0.0598 - acc: 0.9815 - val_loss: 1.7272 - val_acc: 0.8677\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0578 - acc: 0.9818 - val_loss: 1.7293 - val_acc: 0.8677\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0548 - acc: 0.9834 - val_loss: 1.7383 - val_acc: 0.8668\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0545 - acc: 0.9836 - val_loss: 1.7298 - val_acc: 0.8691\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0523 - acc: 0.9842 - val_loss: 1.7430 - val_acc: 0.8657\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0515 - acc: 0.9841 - val_loss: 1.7350 - val_acc: 0.8688\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0493 - acc: 0.9849 - val_loss: 1.7353 - val_acc: 0.8686\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0479 - acc: 0.9853 - val_loss: 1.7355 - val_acc: 0.8700\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0470 - acc: 0.9858 - val_loss: 1.7420 - val_acc: 0.8676\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0452 - acc: 0.9863 - val_loss: 1.7394 - val_acc: 0.8685\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0442 - acc: 0.9868 - val_loss: 1.7468 - val_acc: 0.8670\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0435 - acc: 0.9869 - val_loss: 1.7416 - val_acc: 0.8680\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0409 - acc: 0.9881 - val_loss: 1.7465 - val_acc: 0.8687\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0403 - acc: 0.9881 - val_loss: 1.7444 - val_acc: 0.8693\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0384 - acc: 0.9883 - val_loss: 1.7475 - val_acc: 0.8678\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0375 - acc: 0.9892 - val_loss: 1.7519 - val_acc: 0.8675\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0373 - acc: 0.9888 - val_loss: 1.7482 - val_acc: 0.8689\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0368 - acc: 0.9890 - val_loss: 1.7483 - val_acc: 0.8682\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0352 - acc: 0.9892 - val_loss: 1.7561 - val_acc: 0.8663\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0348 - acc: 0.9893 - val_loss: 1.7549 - val_acc: 0.8667\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0329 - acc: 0.9904 - val_loss: 1.7599 - val_acc: 0.8671\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0323 - acc: 0.9900 - val_loss: 1.7509 - val_acc: 0.8676\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0328 - acc: 0.9902 - val_loss: 1.7590 - val_acc: 0.8666\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0311 - acc: 0.9910 - val_loss: 1.7658 - val_acc: 0.8655\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0299 - acc: 0.9912 - val_loss: 1.7627 - val_acc: 0.8659\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0302 - acc: 0.9908 - val_loss: 1.7592 - val_acc: 0.8679\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.0284 - acc: 0.9916 - val_loss: 1.7679 - val_acc: 0.8660\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.0279 - acc: 0.9921 - val_loss: 1.7655 - val_acc: 0.8672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1414863c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student net - NO distillation - digit 3 omitted in the transfer set\n",
    "hard_omit3 = MNIST_StudentNet(n_hidden=20, T=1)\n",
    "hard_omit3.fit(x_train_omit3, y_train_omit3, \n",
    "               batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Student Net 2 - distilling from Dense teacher at T = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 1.0962 - acc: 0.7889 - val_loss: 0.6145 - val_acc: 0.8243\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.7442 - acc: 0.9269 - val_loss: 0.5496 - val_acc: 0.8405\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.7057 - acc: 0.9430 - val_loss: 0.4999 - val_acc: 0.8513\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6878 - acc: 0.9499 - val_loss: 0.4911 - val_acc: 0.8571\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6771 - acc: 0.9551 - val_loss: 0.4655 - val_acc: 0.8616\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6704 - acc: 0.9578 - val_loss: 0.4524 - val_acc: 0.8645\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6653 - acc: 0.9606 - val_loss: 0.4447 - val_acc: 0.8675\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6608 - acc: 0.9632 - val_loss: 0.4328 - val_acc: 0.8713\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6565 - acc: 0.9650 - val_loss: 0.4193 - val_acc: 0.8753\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6533 - acc: 0.9669 - val_loss: 0.4183 - val_acc: 0.8777\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6504 - acc: 0.9685 - val_loss: 0.4134 - val_acc: 0.8789\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6480 - acc: 0.9699 - val_loss: 0.4059 - val_acc: 0.8821\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6458 - acc: 0.9714 - val_loss: 0.4049 - val_acc: 0.8826\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6441 - acc: 0.9720 - val_loss: 0.3988 - val_acc: 0.8823\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6426 - acc: 0.9730 - val_loss: 0.3940 - val_acc: 0.8848\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6409 - acc: 0.9736 - val_loss: 0.3915 - val_acc: 0.8866\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6398 - acc: 0.9745 - val_loss: 0.3854 - val_acc: 0.8872\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6388 - acc: 0.9750 - val_loss: 0.3863 - val_acc: 0.8886\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6375 - acc: 0.9760 - val_loss: 0.3758 - val_acc: 0.8916\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6365 - acc: 0.9770 - val_loss: 0.3767 - val_acc: 0.8915\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6356 - acc: 0.9770 - val_loss: 0.3728 - val_acc: 0.8965\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6349 - acc: 0.9773 - val_loss: 0.3664 - val_acc: 0.9002\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6341 - acc: 0.9784 - val_loss: 0.3617 - val_acc: 0.9027\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6334 - acc: 0.9782 - val_loss: 0.3583 - val_acc: 0.9031\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6328 - acc: 0.9788 - val_loss: 0.3489 - val_acc: 0.9081\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6319 - acc: 0.9790 - val_loss: 0.3496 - val_acc: 0.9086\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6313 - acc: 0.9794 - val_loss: 0.3508 - val_acc: 0.9084\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6308 - acc: 0.9798 - val_loss: 0.3507 - val_acc: 0.9052\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6302 - acc: 0.9801 - val_loss: 0.3413 - val_acc: 0.9078\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6298 - acc: 0.9807 - val_loss: 0.3487 - val_acc: 0.9068\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6296 - acc: 0.9808 - val_loss: 0.3403 - val_acc: 0.9142\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6289 - acc: 0.9813 - val_loss: 0.3436 - val_acc: 0.9110\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6287 - acc: 0.9813 - val_loss: 0.3386 - val_acc: 0.9121\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6283 - acc: 0.9812 - val_loss: 0.3337 - val_acc: 0.9155\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6280 - acc: 0.9815 - val_loss: 0.3361 - val_acc: 0.9146\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 1s 22us/step - loss: 0.6278 - acc: 0.9815 - val_loss: 0.3369 - val_acc: 0.9131\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6275 - acc: 0.9816 - val_loss: 0.3331 - val_acc: 0.9193\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6272 - acc: 0.9821 - val_loss: 0.3319 - val_acc: 0.9169\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6269 - acc: 0.9825 - val_loss: 0.3359 - val_acc: 0.9155\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6267 - acc: 0.9827 - val_loss: 0.3412 - val_acc: 0.9116\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6264 - acc: 0.9827 - val_loss: 0.3251 - val_acc: 0.9196\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6262 - acc: 0.9828 - val_loss: 0.3253 - val_acc: 0.9208\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6260 - acc: 0.9828 - val_loss: 0.3274 - val_acc: 0.9193\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6258 - acc: 0.9829 - val_loss: 0.3256 - val_acc: 0.9199\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6257 - acc: 0.9829 - val_loss: 0.3275 - val_acc: 0.9194\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6255 - acc: 0.9833 - val_loss: 0.3278 - val_acc: 0.9193\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 1s 20us/step - loss: 0.6252 - acc: 0.9831 - val_loss: 0.3219 - val_acc: 0.9241\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6251 - acc: 0.9835 - val_loss: 0.3236 - val_acc: 0.9204\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6250 - acc: 0.9835 - val_loss: 0.3262 - val_acc: 0.9193\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 1s 19us/step - loss: 0.6249 - acc: 0.9834 - val_loss: 0.3251 - val_acc: 0.9209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x144202080>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student net - WITH distillation - digit 3 omitted in the transfer set\n",
    "soft_omit3 = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "soft_omit3.fit(x_train_omit3, y_train_soft_omit3, \n",
    "               batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Student Net 3 - Distilling from the weighted loss of both the soft target and the true labels' cross-entropy at T = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 2s 35us/step - loss: 1.1521 - acc: 0.7983 - val_loss: 1.8253 - val_acc: 0.8206\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 1s 22us/step - loss: 0.7695 - acc: 0.9210 - val_loss: 1.7625 - val_acc: 0.8422\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.7313 - acc: 0.9334 - val_loss: 1.7412 - val_acc: 0.8504\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.7094 - acc: 0.9420 - val_loss: 1.7284 - val_acc: 0.8591\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 1s 22us/step - loss: 0.6928 - acc: 0.9492 - val_loss: 1.7250 - val_acc: 0.8649\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6775 - acc: 0.9551 - val_loss: 1.7105 - val_acc: 0.8715\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6686 - acc: 0.9590 - val_loss: 1.7007 - val_acc: 0.8738\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6630 - acc: 0.9617 - val_loss: 1.6886 - val_acc: 0.8790\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6585 - acc: 0.9640 - val_loss: 1.6837 - val_acc: 0.8815\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6550 - acc: 0.9658 - val_loss: 1.6768 - val_acc: 0.8859\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6520 - acc: 0.9674 - val_loss: 1.6704 - val_acc: 0.8914\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6494 - acc: 0.9685 - val_loss: 1.6702 - val_acc: 0.8911\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6472 - acc: 0.9693 - val_loss: 1.6622 - val_acc: 0.8959\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6452 - acc: 0.9702 - val_loss: 1.6643 - val_acc: 0.8959\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6437 - acc: 0.9714 - val_loss: 1.6519 - val_acc: 0.8992\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6421 - acc: 0.9722 - val_loss: 1.6475 - val_acc: 0.9019\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6406 - acc: 0.9725 - val_loss: 1.6404 - val_acc: 0.9053\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6394 - acc: 0.9732 - val_loss: 1.6502 - val_acc: 0.9065\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6385 - acc: 0.9738 - val_loss: 1.6557 - val_acc: 0.9026\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6374 - acc: 0.9744 - val_loss: 1.6493 - val_acc: 0.9068\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6364 - acc: 0.9749 - val_loss: 1.6492 - val_acc: 0.9042\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6356 - acc: 0.9751 - val_loss: 1.6427 - val_acc: 0.9058\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6350 - acc: 0.9752 - val_loss: 1.6436 - val_acc: 0.9111\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6344 - acc: 0.9754 - val_loss: 1.6318 - val_acc: 0.9129\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6337 - acc: 0.9759 - val_loss: 1.6332 - val_acc: 0.9091\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6333 - acc: 0.9760 - val_loss: 1.6398 - val_acc: 0.9104\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6328 - acc: 0.9769 - val_loss: 1.6377 - val_acc: 0.9098\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6322 - acc: 0.9773 - val_loss: 1.6377 - val_acc: 0.9081\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6319 - acc: 0.9772 - val_loss: 1.6340 - val_acc: 0.9119\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6316 - acc: 0.9773 - val_loss: 1.6395 - val_acc: 0.9145\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6311 - acc: 0.9775 - val_loss: 1.6440 - val_acc: 0.9094\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6308 - acc: 0.9777 - val_loss: 1.6340 - val_acc: 0.9124\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6304 - acc: 0.9782 - val_loss: 1.6435 - val_acc: 0.9133\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6302 - acc: 0.9785 - val_loss: 1.6213 - val_acc: 0.9146\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6299 - acc: 0.9788 - val_loss: 1.6412 - val_acc: 0.9125\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6297 - acc: 0.9790 - val_loss: 1.6372 - val_acc: 0.9138\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6292 - acc: 0.9790 - val_loss: 1.6357 - val_acc: 0.9134\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6291 - acc: 0.9794 - val_loss: 1.6270 - val_acc: 0.9179\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6288 - acc: 0.9795 - val_loss: 1.6284 - val_acc: 0.9152\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6286 - acc: 0.9799 - val_loss: 1.6206 - val_acc: 0.9209\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6282 - acc: 0.9797 - val_loss: 1.6288 - val_acc: 0.9181\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6280 - acc: 0.9801 - val_loss: 1.6243 - val_acc: 0.9159\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6279 - acc: 0.9800 - val_loss: 1.6307 - val_acc: 0.9166\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6276 - acc: 0.9804 - val_loss: 1.6276 - val_acc: 0.9208\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6274 - acc: 0.9799 - val_loss: 1.6269 - val_acc: 0.9196\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6271 - acc: 0.9804 - val_loss: 1.6314 - val_acc: 0.9160\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6270 - acc: 0.9804 - val_loss: 1.6295 - val_acc: 0.9166\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6268 - acc: 0.9808 - val_loss: 1.6288 - val_acc: 0.9212\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6267 - acc: 0.9809 - val_loss: 1.6163 - val_acc: 0.9243\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6265 - acc: 0.9810 - val_loss: 1.6204 - val_acc: 0.9251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15535fdd8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "T = 5\n",
    "w = 0.4 / (T**2) \n",
    "\n",
    "# apply both hard & soft targets to learn\n",
    "y_hard_soft_train_omit3 = np.concatenate((y_train_omit3, y_train_soft_omit3), axis=1)\n",
    "\n",
    "# student net - WITH distillation & weighted hard soft loss - digit 3 omitted in the transfer set\n",
    "mix_omit3 = Sequential()\n",
    "mix_omit3.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "mix_omit3.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "mix_omit3.add(Dense(10, name='logit'))\n",
    "\n",
    "# y_pred at the end of 10-node dense layer is the logit_pred\n",
    "mix_omit3.compile(loss=lambda y_true, y_pred: avg_mix_loss(y_true, y_pred, w, T), \n",
    "                  optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "mix_omit3.fit(x_train_omit3, y_hard_soft_train_omit3, \n",
    "              batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_hard_soft_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluations of these student nets on the Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Accuracy on Test set === \n",
      "\n",
      "NO DISTILLATION\n",
      "loss = 1.7654574642470455, accuracy = 0.8672, #errors = 1328\n",
      "\n",
      "WITH DISTILLATION\n",
      "loss = 0.3250568591117859, accuracy = 0.9209, #errors = 790\n",
      "\n",
      "WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET\n",
      "loss = 1.62039946975708, accuracy = 0.9251, #errors = 748\n"
     ]
    }
   ],
   "source": [
    "print('=== Overall Accuracy on Test set === \\n')\n",
    "\n",
    "loss, accuracy = hard_omit3.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('NO DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION')\n",
    "loss, accuracy = soft_omit3.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET')\n",
    "loss, accuracy = mix_omit3.evaluate(x_test, y_hard_soft_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Digit-3 Only Accuracy on Test set === \n",
      "\n",
      "NO DISTILLATION\n",
      "loss = 16.11809539794922, accuracy = 0.0, #errors = 1010\n",
      "\n",
      "WITH DISTILLATION\n",
      "loss = 1.238349964595077, accuracy = 0.4811881188708957, #errors = 523\n",
      "\n",
      "WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET\n",
      "loss = 1.2482987630485307, accuracy = 0.5217821782473291, #errors = 482\n"
     ]
    }
   ],
   "source": [
    "print('=== Digit-3 Only Accuracy on Test set === \\n')\n",
    "\n",
    "idx2 = y_test[:,3] == 1\n",
    "x_test_3 = x_test[idx2, :]\n",
    "y_test_3 = y_test[idx2, :]\n",
    "y_test_soft_3 = y_test[idx2,:]\n",
    "y_hard_soft_test_3 = np.concatenate((y_test_3, y_test_soft_3), axis=1)\n",
    "\n",
    "loss, accuracy = hard_omit3.evaluate(x_test_3, y_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test_3))\n",
    "print('NO DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "loss, accuracy = soft_omit3.evaluate(x_test_3, y_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test_3))\n",
    "print('WITH DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "loss, accuracy = mix_omit3.evaluate(x_test_3, y_hard_soft_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test_3))\n",
    "print('WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Bias\n",
    "The paper also claimed that better performance can be acheived by tuning the bias term during distillation, so here we show the implementation of tuning the bias to optimize test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 2s 37us/step - loss: 1.0913 - acc: 0.7864 - val_loss: 0.6302 - val_acc: 0.8288\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.7331 - acc: 0.9324 - val_loss: 0.5332 - val_acc: 0.8457\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6994 - acc: 0.9460 - val_loss: 0.4937 - val_acc: 0.8550\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6826 - acc: 0.9532 - val_loss: 0.4508 - val_acc: 0.8640\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6719 - acc: 0.9577 - val_loss: 0.4296 - val_acc: 0.8719\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6637 - acc: 0.9625 - val_loss: 0.4201 - val_acc: 0.8765\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6577 - acc: 0.9650 - val_loss: 0.4011 - val_acc: 0.8844\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6529 - acc: 0.9670 - val_loss: 0.3869 - val_acc: 0.8913\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6495 - acc: 0.9695 - val_loss: 0.3834 - val_acc: 0.8915\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6468 - acc: 0.9704 - val_loss: 0.3866 - val_acc: 0.8907\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6442 - acc: 0.9716 - val_loss: 0.3738 - val_acc: 0.8960\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6422 - acc: 0.9729 - val_loss: 0.3658 - val_acc: 0.9011\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6403 - acc: 0.9740 - val_loss: 0.3579 - val_acc: 0.9044\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6387 - acc: 0.9742 - val_loss: 0.3533 - val_acc: 0.9056\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6374 - acc: 0.9757 - val_loss: 0.3516 - val_acc: 0.9055\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6360 - acc: 0.9762 - val_loss: 0.3447 - val_acc: 0.9064\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 1s 23us/step - loss: 0.6350 - acc: 0.9767 - val_loss: 0.3408 - val_acc: 0.9086\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6342 - acc: 0.9770 - val_loss: 0.3387 - val_acc: 0.9128\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6332 - acc: 0.9779 - val_loss: 0.3382 - val_acc: 0.9123\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6326 - acc: 0.9781 - val_loss: 0.3418 - val_acc: 0.9077\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6321 - acc: 0.9782 - val_loss: 0.3351 - val_acc: 0.9116\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6314 - acc: 0.9786 - val_loss: 0.3235 - val_acc: 0.9191\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6308 - acc: 0.9794 - val_loss: 0.3309 - val_acc: 0.9188\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6303 - acc: 0.9798 - val_loss: 0.3320 - val_acc: 0.9161\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6299 - acc: 0.9795 - val_loss: 0.3194 - val_acc: 0.9185\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6295 - acc: 0.9802 - val_loss: 0.3204 - val_acc: 0.9229\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6290 - acc: 0.9807 - val_loss: 0.3235 - val_acc: 0.9187\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6287 - acc: 0.9805 - val_loss: 0.3236 - val_acc: 0.9182\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6284 - acc: 0.9811 - val_loss: 0.3200 - val_acc: 0.9191\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6279 - acc: 0.9813 - val_loss: 0.3181 - val_acc: 0.9221\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6277 - acc: 0.9813 - val_loss: 0.3238 - val_acc: 0.9214\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6275 - acc: 0.9807 - val_loss: 0.3182 - val_acc: 0.9252\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.6271 - acc: 0.9815 - val_loss: 0.3213 - val_acc: 0.9221\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6269 - acc: 0.9816 - val_loss: 0.3217 - val_acc: 0.9222\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6267 - acc: 0.9817 - val_loss: 0.3196 - val_acc: 0.9223\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6264 - acc: 0.9818 - val_loss: 0.3135 - val_acc: 0.9250\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6262 - acc: 0.9822 - val_loss: 0.3155 - val_acc: 0.9202\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6262 - acc: 0.9822 - val_loss: 0.3167 - val_acc: 0.9218\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6258 - acc: 0.9824 - val_loss: 0.3174 - val_acc: 0.9242\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6258 - acc: 0.9824 - val_loss: 0.3134 - val_acc: 0.9241\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6255 - acc: 0.9824 - val_loss: 0.3152 - val_acc: 0.9237\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6254 - acc: 0.9823 - val_loss: 0.3203 - val_acc: 0.9203\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6251 - acc: 0.9827 - val_loss: 0.3136 - val_acc: 0.9231\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6252 - acc: 0.9827 - val_loss: 0.3107 - val_acc: 0.9272\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6249 - acc: 0.9825 - val_loss: 0.3131 - val_acc: 0.9262\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6247 - acc: 0.9829 - val_loss: 0.3165 - val_acc: 0.9262\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6248 - acc: 0.9831 - val_loss: 0.3137 - val_acc: 0.9291\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6245 - acc: 0.9830 - val_loss: 0.3144 - val_acc: 0.9255\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6244 - acc: 0.9831 - val_loss: 0.3188 - val_acc: 0.9200\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6243 - acc: 0.9833 - val_loss: 0.3122 - val_acc: 0.9293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154a78c88>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune bias\n",
    "soft_omit3_bias = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "bias = soft_omit3_bias.layers[2].get_weights()[1]\n",
    "bias[3] = 3.5\n",
    "K.set_value(soft_omit3_bias.layers[2].bias, bias)\n",
    "\n",
    "soft_omit3_bias.fit(x_train_omit3, y_train_soft_omit3, \n",
    "                    batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BIAS TUNED - Overall Accuracy on Test set === \n",
      "\n",
      "loss = 0.312171789765358, accuracy = 0.9293, #errors = 706\n",
      "\n",
      "=== BIAS TUNED - Digit-3 Only Accuracy on Test set === \n",
      "\n",
      "loss = 1.102321712805493, accuracy = 0.5613861386728759, #errors = 442\n"
     ]
    }
   ],
   "source": [
    "print('=== BIAS TUNED - Overall Accuracy on Test set === \\n')\n",
    "loss, accuracy = soft_omit3_bias.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('=== BIAS TUNED - Digit-3 Only Accuracy on Test set === \\n')\n",
    "loss, accuracy = soft_omit3_bias.evaluate(x_test_3, y_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test_3))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Omit3 Distillation Performance on Full Test Set ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.8672</td>\n",
       "      <td>1.765457</td>\n",
       "      <td>1328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_tuned</th>\n",
       "      <td>0.9293</td>\n",
       "      <td>0.312172</td>\n",
       "      <td>706.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.9209</td>\n",
       "      <td>0.325057</td>\n",
       "      <td>790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.9251</td>\n",
       "      <td>1.620399</td>\n",
       "      <td>748.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy      loss  num_error\n",
       "baseline                    0.8672  1.765457     1328.0\n",
       "bias_tuned                  0.9293  0.312172      706.0\n",
       "dense_distilled             0.9209  0.325057      790.0\n",
       "dense_distilled_mix_loss    0.9251  1.620399      748.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Omit3 Distillation Performance on Digit 3 Test Samples ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.118095</td>\n",
       "      <td>1010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_tuned</th>\n",
       "      <td>0.561386</td>\n",
       "      <td>1.102322</td>\n",
       "      <td>442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.481188</td>\n",
       "      <td>1.238350</td>\n",
       "      <td>523.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.521782</td>\n",
       "      <td>1.248299</td>\n",
       "      <td>482.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy       loss  num_error\n",
       "baseline                  0.000000  16.118095     1010.0\n",
       "bias_tuned                0.561386   1.102322      442.0\n",
       "dense_distilled           0.481188   1.238350      523.0\n",
       "dense_distilled_mix_loss  0.521782   1.248299      482.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "omit3_summary_overall = {\n",
    "    'baseline': {'loss': 1.7654574642470455, 'accuracy': 0.8672, 'num_error': 1328},\n",
    "    'dense_distilled': {'loss': 0.3250568591117859, 'accuracy': 0.9209, 'num_error': 790},\n",
    "    'dense_distilled_mix_loss': {'loss': 1.62039946975708, 'accuracy': 0.9251, 'num_error': 748},\n",
    "    'bias_tuned': {'loss': 0.312171789765358, 'accuracy': 0.9293, 'num_error': 706}\n",
    "}\n",
    "omit3_summary_3only = {\n",
    "    'baseline': {'loss': 16.11809539794922, 'accuracy': 0.0, 'num_error': 1010},\n",
    "    'dense_distilled': {'loss': 1.238349964595077, 'accuracy': 0.4811881188708957, 'num_error': 523},\n",
    "    'dense_distilled_mix_loss': {'loss': 1.2482987630485307, 'accuracy': 0.5217821782473291, 'num_error': 482},\n",
    "    'bias_tuned': {'loss': 1.102321712805493, 'accuracy': 0.5613861386728759, 'num_error': 442}\n",
    "}\n",
    "\n",
    "df_omit3_summary_overall = pd.DataFrame().from_dict(omit3_summary_overall).T\n",
    "df_omit3_summary_3only = pd.DataFrame().from_dict(omit3_summary_3only).T\n",
    "\n",
    "print('=== Omit3 Distillation Performance on Full Test Set ===')\n",
    "display(df_omit3_summary_overall)\n",
    "print()\n",
    "\n",
    "print('=== Omit3 Distillation Performance on Digit 3 Test Samples ===')\n",
    "display(df_omit3_summary_3only)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions - distillation performance of student nets trained on data sets without digit 3\n",
    "\n",
    "Comparing the accuracy on the full test data and on the digit 3 only subset, we observed the worst performance from the non-distilled student baseline test accuracy = 86.8% overall and 0% on 3-only set. With distillation, the student net showed an improved accuracy = 92.1% on full test set and 48.1% on 3-only set. According to the paper, most of the errors are caused by an inappropriate bias for class 3. After adjusting this bias to 3.5 (which optimizes overall accuracy), the distilled model further increased its accuracy to 92.9% and 56.1% on 3-only set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Part II-b on Partial MNIST Data -- keeping only digit 7 and 8 in the transfer set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the misclassification distributions of all digits HW7, 7 and 8 with the digits that they are mostly likely being misclassified as cover all class labels. In the following, we kept only digit 7 & 8 in the transfer set and showed results of distillation on 3 student nets: 1) a non-distilled baseline, 2) a distilled student with tuned bias and 3) a distilled student with a mixture of hard and soft losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide code to keep only the digits 7 and 8 from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep 7 and 8 in transfer set\n",
    "idx78 = [True if y[7]==1 or y[8] == 1 else False for y in y_train]\n",
    "\n",
    "x_train_78 = x_train[idx78,:]\n",
    "y_train_soft_78 = y_train_soft[idx78,:]\n",
    "y_train_78 = y_train[idx78,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Student Net 1 - Baseline, No Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12116/12116 [==============================] - 2s 161us/step - loss: 0.3170 - acc: 0.9272 - val_loss: 8.2521 - val_acc: 0.1965\n",
      "Epoch 2/50\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.0342 - acc: 0.9889 - val_loss: 9.1068 - val_acc: 0.1973\n",
      "Epoch 3/50\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.0259 - acc: 0.9919 - val_loss: 9.5551 - val_acc: 0.1976\n",
      "Epoch 4/50\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.0220 - acc: 0.9931 - val_loss: 9.8298 - val_acc: 0.1974\n",
      "Epoch 5/50\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.0201 - acc: 0.9931 - val_loss: 10.0648 - val_acc: 0.1980\n",
      "Epoch 6/50\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.0183 - acc: 0.9941 - val_loss: 10.2347 - val_acc: 0.1979\n",
      "Epoch 7/50\n",
      "12116/12116 [==============================] - 1s 99us/step - loss: 0.0177 - acc: 0.9938 - val_loss: 10.3479 - val_acc: 0.1977\n",
      "Epoch 8/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0162 - acc: 0.9948 - val_loss: 10.4804 - val_acc: 0.1980\n",
      "Epoch 9/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0146 - acc: 0.9953 - val_loss: 10.5505 - val_acc: 0.1983\n",
      "Epoch 10/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0140 - acc: 0.9955 - val_loss: 10.6187 - val_acc: 0.1985\n",
      "Epoch 11/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.0148 - acc: 0.9954 - val_loss: 10.6355 - val_acc: 0.1977\n",
      "Epoch 12/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.0123 - acc: 0.9962 - val_loss: 10.7293 - val_acc: 0.1985\n",
      "Epoch 13/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0119 - acc: 0.9960 - val_loss: 10.7953 - val_acc: 0.1981\n",
      "Epoch 14/50\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.0110 - acc: 0.9960 - val_loss: 10.8560 - val_acc: 0.1989\n",
      "Epoch 15/50\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.0102 - acc: 0.9968 - val_loss: 10.8845 - val_acc: 0.1985\n",
      "Epoch 16/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.0084 - acc: 0.9971 - val_loss: 10.9419 - val_acc: 0.1988\n",
      "Epoch 17/50\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 11.0060 - val_acc: 0.1983\n",
      "Epoch 18/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.0073 - acc: 0.9974 - val_loss: 11.0178 - val_acc: 0.1986\n",
      "Epoch 19/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.0067 - acc: 0.9977 - val_loss: 11.0525 - val_acc: 0.1984\n",
      "Epoch 20/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 11.0905 - val_acc: 0.1975\n",
      "Epoch 21/50\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.0059 - acc: 0.9979 - val_loss: 11.1244 - val_acc: 0.1980\n",
      "Epoch 22/50\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 11.1774 - val_acc: 0.1984\n",
      "Epoch 23/50\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.0046 - acc: 0.9982 - val_loss: 11.2052 - val_acc: 0.1985\n",
      "Epoch 24/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 11.2654 - val_acc: 0.1986\n",
      "Epoch 25/50\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 11.2863 - val_acc: 0.1986\n",
      "Epoch 26/50\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.0034 - acc: 0.9988 - val_loss: 11.3389 - val_acc: 0.1983\n",
      "Epoch 27/50\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 11.3731 - val_acc: 0.1986\n",
      "Epoch 28/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 11.3431 - val_acc: 0.1988\n",
      "Epoch 29/50\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.0018 - acc: 0.9998 - val_loss: 11.4389 - val_acc: 0.1984\n",
      "Epoch 30/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 11.4664 - val_acc: 0.1986\n",
      "Epoch 31/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 11.4807 - val_acc: 0.1984\n",
      "Epoch 32/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 11.5301 - val_acc: 0.1985\n",
      "Epoch 33/50\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 9.7775e-04 - acc: 1.0000 - val_loss: 11.5446 - val_acc: 0.1987\n",
      "Epoch 34/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 11.5673 - val_acc: 0.1986\n",
      "Epoch 35/50\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 8.9365e-04 - acc: 1.0000 - val_loss: 11.6144 - val_acc: 0.1987\n",
      "Epoch 36/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 7.0834e-04 - acc: 1.0000 - val_loss: 11.6400 - val_acc: 0.1987\n",
      "Epoch 37/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 5.6447e-04 - acc: 1.0000 - val_loss: 11.6790 - val_acc: 0.1985\n",
      "Epoch 38/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 5.2345e-04 - acc: 1.0000 - val_loss: 11.6894 - val_acc: 0.1987\n",
      "Epoch 39/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 4.6989e-04 - acc: 1.0000 - val_loss: 11.7391 - val_acc: 0.1986\n",
      "Epoch 40/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 5.1136e-04 - acc: 1.0000 - val_loss: 11.7582 - val_acc: 0.1986\n",
      "Epoch 41/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 3.9014e-04 - acc: 1.0000 - val_loss: 11.7727 - val_acc: 0.1987\n",
      "Epoch 42/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 3.4619e-04 - acc: 1.0000 - val_loss: 11.7876 - val_acc: 0.1987\n",
      "Epoch 43/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 3.3569e-04 - acc: 1.0000 - val_loss: 11.7975 - val_acc: 0.1985\n",
      "Epoch 44/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 2.9386e-04 - acc: 1.0000 - val_loss: 11.8095 - val_acc: 0.1985\n",
      "Epoch 45/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 2.8390e-04 - acc: 1.0000 - val_loss: 11.8394 - val_acc: 0.1987\n",
      "Epoch 46/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 2.5653e-04 - acc: 1.0000 - val_loss: 11.8632 - val_acc: 0.1987\n",
      "Epoch 47/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 2.1209e-04 - acc: 1.0000 - val_loss: 11.8771 - val_acc: 0.1987\n",
      "Epoch 48/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 1.8624e-04 - acc: 1.0000 - val_loss: 11.8905 - val_acc: 0.1987\n",
      "Epoch 49/50\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 1.7570e-04 - acc: 1.0000 - val_loss: 11.9116 - val_acc: 0.1987\n",
      "Epoch 50/50\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 1.5753e-04 - acc: 1.0000 - val_loss: 11.9318 - val_acc: 0.1986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153fbe400>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train hard target model with training set omitting digit 3\n",
    "hard_78 = MNIST_StudentNet(n_hidden=20, T=1)\n",
    "hard_78.fit(x_train_78, y_train_78, batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Student Net 2 - distilling from Dense teacher at T = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12116/12116 [==============================] - 2s 166us/step - loss: 1.3137 - acc: 0.8350 - val_loss: 2.7221 - val_acc: 0.1943\n",
      "Epoch 2/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.8349 - acc: 0.9833 - val_loss: 2.3942 - val_acc: 0.1964\n",
      "Epoch 3/100\n",
      "12116/12116 [==============================] - 1s 92us/step - loss: 0.7921 - acc: 0.9874 - val_loss: 2.2301 - val_acc: 0.1971\n",
      "Epoch 4/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7810 - acc: 0.9888 - val_loss: 2.1232 - val_acc: 0.1986\n",
      "Epoch 5/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7738 - acc: 0.9893 - val_loss: 2.0344 - val_acc: 0.2018\n",
      "Epoch 6/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7676 - acc: 0.9901 - val_loss: 1.9403 - val_acc: 0.2127\n",
      "Epoch 7/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7628 - acc: 0.9902 - val_loss: 1.8744 - val_acc: 0.2199\n",
      "Epoch 8/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7592 - acc: 0.9904 - val_loss: 1.7867 - val_acc: 0.2264\n",
      "Epoch 9/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7562 - acc: 0.9904 - val_loss: 1.7531 - val_acc: 0.2319\n",
      "Epoch 10/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7533 - acc: 0.9907 - val_loss: 1.7217 - val_acc: 0.2363\n",
      "Epoch 11/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7511 - acc: 0.9912 - val_loss: 1.6749 - val_acc: 0.2551\n",
      "Epoch 12/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7493 - acc: 0.9914 - val_loss: 1.6703 - val_acc: 0.2611\n",
      "Epoch 13/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7476 - acc: 0.9917 - val_loss: 1.6474 - val_acc: 0.2658\n",
      "Epoch 14/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7461 - acc: 0.9917 - val_loss: 1.6107 - val_acc: 0.2834\n",
      "Epoch 15/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7449 - acc: 0.9915 - val_loss: 1.6027 - val_acc: 0.2828\n",
      "Epoch 16/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7438 - acc: 0.9919 - val_loss: 1.5888 - val_acc: 0.2916\n",
      "Epoch 17/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7426 - acc: 0.9922 - val_loss: 1.5615 - val_acc: 0.3042\n",
      "Epoch 18/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7416 - acc: 0.9925 - val_loss: 1.5384 - val_acc: 0.3189\n",
      "Epoch 19/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7408 - acc: 0.9923 - val_loss: 1.5228 - val_acc: 0.3277\n",
      "Epoch 20/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7399 - acc: 0.9923 - val_loss: 1.5058 - val_acc: 0.3397\n",
      "Epoch 21/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7391 - acc: 0.9922 - val_loss: 1.4942 - val_acc: 0.3494\n",
      "Epoch 22/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7383 - acc: 0.9927 - val_loss: 1.4793 - val_acc: 0.3577\n",
      "Epoch 23/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7376 - acc: 0.9929 - val_loss: 1.4694 - val_acc: 0.3685\n",
      "Epoch 24/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7369 - acc: 0.9932 - val_loss: 1.4461 - val_acc: 0.3853\n",
      "Epoch 25/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7361 - acc: 0.9935 - val_loss: 1.4303 - val_acc: 0.3880\n",
      "Epoch 26/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7354 - acc: 0.9938 - val_loss: 1.4123 - val_acc: 0.4031\n",
      "Epoch 27/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7350 - acc: 0.9941 - val_loss: 1.3995 - val_acc: 0.4143\n",
      "Epoch 28/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7343 - acc: 0.9941 - val_loss: 1.3867 - val_acc: 0.4208\n",
      "Epoch 29/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7339 - acc: 0.9940 - val_loss: 1.3710 - val_acc: 0.4240\n",
      "Epoch 30/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7334 - acc: 0.9940 - val_loss: 1.3750 - val_acc: 0.4209\n",
      "Epoch 31/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7331 - acc: 0.9942 - val_loss: 1.3493 - val_acc: 0.4410\n",
      "Epoch 32/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7326 - acc: 0.9945 - val_loss: 1.3333 - val_acc: 0.4527\n",
      "Epoch 33/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7322 - acc: 0.9946 - val_loss: 1.3469 - val_acc: 0.4409\n",
      "Epoch 34/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7320 - acc: 0.9943 - val_loss: 1.3001 - val_acc: 0.4823\n",
      "Epoch 35/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7316 - acc: 0.9943 - val_loss: 1.3247 - val_acc: 0.4549\n",
      "Epoch 36/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7313 - acc: 0.9944 - val_loss: 1.2791 - val_acc: 0.4989\n",
      "Epoch 37/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7310 - acc: 0.9946 - val_loss: 1.2862 - val_acc: 0.4877\n",
      "Epoch 38/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7308 - acc: 0.9944 - val_loss: 1.2711 - val_acc: 0.5010\n",
      "Epoch 39/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7306 - acc: 0.9948 - val_loss: 1.2662 - val_acc: 0.5079\n",
      "Epoch 40/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7304 - acc: 0.9946 - val_loss: 1.2673 - val_acc: 0.5080\n",
      "Epoch 41/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7301 - acc: 0.9948 - val_loss: 1.2551 - val_acc: 0.5102\n",
      "Epoch 42/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7298 - acc: 0.9951 - val_loss: 1.2458 - val_acc: 0.5226\n",
      "Epoch 43/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7297 - acc: 0.9949 - val_loss: 1.2335 - val_acc: 0.5369\n",
      "Epoch 44/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7296 - acc: 0.9950 - val_loss: 1.2453 - val_acc: 0.5225\n",
      "Epoch 45/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7294 - acc: 0.9950 - val_loss: 1.2305 - val_acc: 0.5339\n",
      "Epoch 46/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7292 - acc: 0.9951 - val_loss: 1.2232 - val_acc: 0.5364\n",
      "Epoch 47/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7291 - acc: 0.9952 - val_loss: 1.2312 - val_acc: 0.5298\n",
      "Epoch 48/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7289 - acc: 0.9952 - val_loss: 1.2136 - val_acc: 0.5407\n",
      "Epoch 49/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7289 - acc: 0.9950 - val_loss: 1.2184 - val_acc: 0.5407\n",
      "Epoch 50/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7286 - acc: 0.9951 - val_loss: 1.2098 - val_acc: 0.5449\n",
      "Epoch 51/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7285 - acc: 0.9952 - val_loss: 1.2072 - val_acc: 0.5404\n",
      "Epoch 52/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7284 - acc: 0.9954 - val_loss: 1.1965 - val_acc: 0.5573\n",
      "Epoch 53/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7284 - acc: 0.9955 - val_loss: 1.1991 - val_acc: 0.5518\n",
      "Epoch 54/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7282 - acc: 0.9956 - val_loss: 1.1877 - val_acc: 0.5629\n",
      "Epoch 55/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7281 - acc: 0.9953 - val_loss: 1.1825 - val_acc: 0.5637\n",
      "Epoch 56/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7279 - acc: 0.9958 - val_loss: 1.1849 - val_acc: 0.5545\n",
      "Epoch 57/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7279 - acc: 0.9960 - val_loss: 1.1736 - val_acc: 0.5672\n",
      "Epoch 58/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7277 - acc: 0.9957 - val_loss: 1.1702 - val_acc: 0.5678\n",
      "Epoch 59/100\n",
      "12116/12116 [==============================] - 1s 72us/step - loss: 0.7278 - acc: 0.9957 - val_loss: 1.1507 - val_acc: 0.5857\n",
      "Epoch 60/100\n",
      "12116/12116 [==============================] - 1s 106us/step - loss: 0.7276 - acc: 0.9960 - val_loss: 1.1569 - val_acc: 0.5796\n",
      "Epoch 61/100\n",
      "12116/12116 [==============================] - 1s 94us/step - loss: 0.7274 - acc: 0.9956 - val_loss: 1.1543 - val_acc: 0.5824\n",
      "Epoch 62/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7274 - acc: 0.9960 - val_loss: 1.1489 - val_acc: 0.5867\n",
      "Epoch 63/100\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.7271 - acc: 0.9959 - val_loss: 1.1546 - val_acc: 0.5783\n",
      "Epoch 64/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7272 - acc: 0.9960 - val_loss: 1.1522 - val_acc: 0.5810\n",
      "Epoch 65/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7270 - acc: 0.9960 - val_loss: 1.1494 - val_acc: 0.5796\n",
      "Epoch 66/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7269 - acc: 0.9960 - val_loss: 1.1401 - val_acc: 0.5877\n",
      "Epoch 67/100\n",
      "12116/12116 [==============================] - 1s 60us/step - loss: 0.7268 - acc: 0.9958 - val_loss: 1.1410 - val_acc: 0.5846\n",
      "Epoch 68/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7267 - acc: 0.9960 - val_loss: 1.1471 - val_acc: 0.5758\n",
      "Epoch 69/100\n",
      "12116/12116 [==============================] - 1s 55us/step - loss: 0.7266 - acc: 0.9959 - val_loss: 1.1320 - val_acc: 0.5927\n",
      "Epoch 70/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7266 - acc: 0.9964 - val_loss: 1.1292 - val_acc: 0.5935\n",
      "Epoch 71/100\n",
      "12116/12116 [==============================] - 1s 82us/step - loss: 0.7265 - acc: 0.9964 - val_loss: 1.1204 - val_acc: 0.5972\n",
      "Epoch 72/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7265 - acc: 0.9961 - val_loss: 1.1223 - val_acc: 0.5940\n",
      "Epoch 73/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7263 - acc: 0.9962 - val_loss: 1.1193 - val_acc: 0.5944\n",
      "Epoch 74/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7262 - acc: 0.9965 - val_loss: 1.1082 - val_acc: 0.6142\n",
      "Epoch 75/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7261 - acc: 0.9962 - val_loss: 1.1069 - val_acc: 0.6071\n",
      "Epoch 76/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7261 - acc: 0.9963 - val_loss: 1.1091 - val_acc: 0.5999\n",
      "Epoch 77/100\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.7261 - acc: 0.9967 - val_loss: 1.1006 - val_acc: 0.6103\n",
      "Epoch 78/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7259 - acc: 0.9961 - val_loss: 1.0991 - val_acc: 0.6141\n",
      "Epoch 79/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7259 - acc: 0.9964 - val_loss: 1.1042 - val_acc: 0.6061\n",
      "Epoch 80/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7258 - acc: 0.9960 - val_loss: 1.1070 - val_acc: 0.6044\n",
      "Epoch 81/100\n",
      "12116/12116 [==============================] - 1s 73us/step - loss: 0.7259 - acc: 0.9960 - val_loss: 1.1087 - val_acc: 0.6010\n",
      "Epoch 82/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7256 - acc: 0.9963 - val_loss: 1.0914 - val_acc: 0.6188\n",
      "Epoch 83/100\n",
      "12116/12116 [==============================] - 1s 86us/step - loss: 0.7256 - acc: 0.9965 - val_loss: 1.1047 - val_acc: 0.6018\n",
      "Epoch 84/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7254 - acc: 0.9962 - val_loss: 1.0916 - val_acc: 0.6130\n",
      "Epoch 85/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7254 - acc: 0.9965 - val_loss: 1.1055 - val_acc: 0.5986\n",
      "Epoch 86/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7255 - acc: 0.9965 - val_loss: 1.0815 - val_acc: 0.6292\n",
      "Epoch 87/100\n",
      "12116/12116 [==============================] - 1s 62us/step - loss: 0.7254 - acc: 0.9965 - val_loss: 1.1044 - val_acc: 0.6028\n",
      "Epoch 88/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7253 - acc: 0.9964 - val_loss: 1.0909 - val_acc: 0.6130\n",
      "Epoch 89/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7251 - acc: 0.9967 - val_loss: 1.0916 - val_acc: 0.6120\n",
      "Epoch 90/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7250 - acc: 0.9965 - val_loss: 1.0944 - val_acc: 0.6038\n",
      "Epoch 91/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7251 - acc: 0.9960 - val_loss: 1.0875 - val_acc: 0.6145\n",
      "Epoch 92/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7251 - acc: 0.9965 - val_loss: 1.0903 - val_acc: 0.6117\n",
      "Epoch 93/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7250 - acc: 0.9965 - val_loss: 1.0882 - val_acc: 0.6089\n",
      "Epoch 94/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7250 - acc: 0.9965 - val_loss: 1.0912 - val_acc: 0.6106\n",
      "Epoch 95/100\n",
      "12116/12116 [==============================] - 1s 52us/step - loss: 0.7249 - acc: 0.9965 - val_loss: 1.0715 - val_acc: 0.6286\n",
      "Epoch 96/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7249 - acc: 0.9962 - val_loss: 1.0791 - val_acc: 0.6202\n",
      "Epoch 97/100\n",
      "12116/12116 [==============================] - 1s 62us/step - loss: 0.7248 - acc: 0.9966 - val_loss: 1.0793 - val_acc: 0.6188\n",
      "Epoch 98/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7247 - acc: 0.9962 - val_loss: 1.0711 - val_acc: 0.6213\n",
      "Epoch 99/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7248 - acc: 0.9964 - val_loss: 1.0795 - val_acc: 0.6143\n",
      "Epoch 100/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7246 - acc: 0.9964 - val_loss: 1.0752 - val_acc: 0.6182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15681ada0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train soft target model with training set omitting digit 3\n",
    "soft_78 = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "soft_78.fit(x_train_78, y_train_soft_78, batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Student Net 3 - Distilling from weighted loss of both the soft target and the true labels' cross-entropy at T = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12116/12116 [==============================] - 4s 314us/step - loss: 1.4717 - acc: 0.8966 - val_loss: 3.1774 - val_acc: 0.1925\n",
      "Epoch 2/100\n",
      "12116/12116 [==============================] - 1s 110us/step - loss: 0.8514 - acc: 0.9808 - val_loss: 2.7144 - val_acc: 0.1955\n",
      "Epoch 3/100\n",
      "12116/12116 [==============================] - 1s 82us/step - loss: 0.8034 - acc: 0.9891 - val_loss: 2.5803 - val_acc: 0.1967\n",
      "Epoch 4/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7909 - acc: 0.9908 - val_loss: 2.5080 - val_acc: 0.1974\n",
      "Epoch 5/100\n",
      "12116/12116 [==============================] - 1s 75us/step - loss: 0.7839 - acc: 0.9916 - val_loss: 2.4242 - val_acc: 0.1986\n",
      "Epoch 6/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7786 - acc: 0.9922 - val_loss: 2.4065 - val_acc: 0.2003\n",
      "Epoch 7/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7741 - acc: 0.9929 - val_loss: 2.3520 - val_acc: 0.2033\n",
      "Epoch 8/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7705 - acc: 0.9931 - val_loss: 2.3089 - val_acc: 0.2072\n",
      "Epoch 9/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7675 - acc: 0.9936 - val_loss: 2.2714 - val_acc: 0.2103\n",
      "Epoch 10/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7648 - acc: 0.9936 - val_loss: 2.2362 - val_acc: 0.2143\n",
      "Epoch 11/100\n",
      "12116/12116 [==============================] - 1s 70us/step - loss: 0.7627 - acc: 0.9939 - val_loss: 2.2003 - val_acc: 0.2175\n",
      "Epoch 12/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7606 - acc: 0.9941 - val_loss: 2.1660 - val_acc: 0.2246\n",
      "Epoch 13/100\n",
      "12116/12116 [==============================] - 1s 72us/step - loss: 0.7583 - acc: 0.9944 - val_loss: 2.1081 - val_acc: 0.2313\n",
      "Epoch 14/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7564 - acc: 0.9943 - val_loss: 2.0878 - val_acc: 0.2324\n",
      "Epoch 15/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7546 - acc: 0.9946 - val_loss: 2.0283 - val_acc: 0.2476\n",
      "Epoch 16/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7530 - acc: 0.9951 - val_loss: 2.0129 - val_acc: 0.2533\n",
      "Epoch 17/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7512 - acc: 0.9953 - val_loss: 1.9817 - val_acc: 0.2617\n",
      "Epoch 18/100\n",
      "12116/12116 [==============================] - 1s 52us/step - loss: 0.7497 - acc: 0.9954 - val_loss: 1.9586 - val_acc: 0.2733\n",
      "Epoch 19/100\n",
      "12116/12116 [==============================] - 1s 83us/step - loss: 0.7484 - acc: 0.9950 - val_loss: 1.9090 - val_acc: 0.2905\n",
      "Epoch 20/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7470 - acc: 0.9954 - val_loss: 1.8885 - val_acc: 0.3021\n",
      "Epoch 21/100\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.7461 - acc: 0.9960 - val_loss: 1.8663 - val_acc: 0.3118\n",
      "Epoch 22/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7448 - acc: 0.9956 - val_loss: 1.8397 - val_acc: 0.3300\n",
      "Epoch 23/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7438 - acc: 0.9959 - val_loss: 1.8317 - val_acc: 0.3287\n",
      "Epoch 24/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7426 - acc: 0.9957 - val_loss: 1.7921 - val_acc: 0.3515\n",
      "Epoch 25/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7414 - acc: 0.9959 - val_loss: 1.7836 - val_acc: 0.3571\n",
      "Epoch 26/100\n",
      "12116/12116 [==============================] - 1s 52us/step - loss: 0.7402 - acc: 0.9960 - val_loss: 1.7695 - val_acc: 0.3587\n",
      "Epoch 27/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7392 - acc: 0.9957 - val_loss: 1.7414 - val_acc: 0.3790\n",
      "Epoch 28/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7384 - acc: 0.9957 - val_loss: 1.7280 - val_acc: 0.3808\n",
      "Epoch 29/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7377 - acc: 0.9961 - val_loss: 1.7074 - val_acc: 0.3899\n",
      "Epoch 30/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7371 - acc: 0.9963 - val_loss: 1.6971 - val_acc: 0.3960\n",
      "Epoch 31/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7364 - acc: 0.9960 - val_loss: 1.6845 - val_acc: 0.4029\n",
      "Epoch 32/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7361 - acc: 0.9957 - val_loss: 1.6755 - val_acc: 0.4029\n",
      "Epoch 33/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7355 - acc: 0.9964 - val_loss: 1.6645 - val_acc: 0.4110\n",
      "Epoch 34/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7352 - acc: 0.9961 - val_loss: 1.6446 - val_acc: 0.4230\n",
      "Epoch 35/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7347 - acc: 0.9961 - val_loss: 1.6405 - val_acc: 0.4182\n",
      "Epoch 36/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7345 - acc: 0.9965 - val_loss: 1.6334 - val_acc: 0.4176\n",
      "Epoch 37/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7342 - acc: 0.9963 - val_loss: 1.6181 - val_acc: 0.4303\n",
      "Epoch 38/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7339 - acc: 0.9964 - val_loss: 1.6123 - val_acc: 0.4320\n",
      "Epoch 39/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7335 - acc: 0.9964 - val_loss: 1.5968 - val_acc: 0.4409\n",
      "Epoch 40/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7333 - acc: 0.9966 - val_loss: 1.5890 - val_acc: 0.4447\n",
      "Epoch 41/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7330 - acc: 0.9965 - val_loss: 1.5703 - val_acc: 0.4587\n",
      "Epoch 42/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7328 - acc: 0.9961 - val_loss: 1.5716 - val_acc: 0.4483\n",
      "Epoch 43/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7325 - acc: 0.9967 - val_loss: 1.5599 - val_acc: 0.4587\n",
      "Epoch 44/100\n",
      "12116/12116 [==============================] - 2s 137us/step - loss: 0.7323 - acc: 0.9965 - val_loss: 1.5419 - val_acc: 0.4716\n",
      "Epoch 45/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7319 - acc: 0.9967 - val_loss: 1.5337 - val_acc: 0.4799\n",
      "Epoch 46/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7317 - acc: 0.9965 - val_loss: 1.5360 - val_acc: 0.4721\n",
      "Epoch 47/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7314 - acc: 0.9965 - val_loss: 1.5236 - val_acc: 0.4796\n",
      "Epoch 48/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7313 - acc: 0.9966 - val_loss: 1.5133 - val_acc: 0.4893\n",
      "Epoch 49/100\n",
      "12116/12116 [==============================] - 1s 93us/step - loss: 0.7312 - acc: 0.9966 - val_loss: 1.5166 - val_acc: 0.4867\n",
      "Epoch 50/100\n",
      "12116/12116 [==============================] - 1s 64us/step - loss: 0.7310 - acc: 0.9966 - val_loss: 1.5087 - val_acc: 0.4923\n",
      "Epoch 51/100\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.7308 - acc: 0.9968 - val_loss: 1.5069 - val_acc: 0.4873\n",
      "Epoch 52/100\n",
      "12116/12116 [==============================] - 1s 92us/step - loss: 0.7307 - acc: 0.9971 - val_loss: 1.5043 - val_acc: 0.4873\n",
      "Epoch 53/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7306 - acc: 0.9969 - val_loss: 1.4903 - val_acc: 0.5030\n",
      "Epoch 54/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7304 - acc: 0.9967 - val_loss: 1.4883 - val_acc: 0.5033\n",
      "Epoch 55/100\n",
      "12116/12116 [==============================] - 1s 60us/step - loss: 0.7302 - acc: 0.9966 - val_loss: 1.4820 - val_acc: 0.5019\n",
      "Epoch 56/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7302 - acc: 0.9965 - val_loss: 1.4820 - val_acc: 0.5042\n",
      "Epoch 57/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7299 - acc: 0.9967 - val_loss: 1.4840 - val_acc: 0.5008\n",
      "Epoch 58/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7298 - acc: 0.9969 - val_loss: 1.4815 - val_acc: 0.5040\n",
      "Epoch 59/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7298 - acc: 0.9969 - val_loss: 1.4737 - val_acc: 0.5095\n",
      "Epoch 60/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7296 - acc: 0.9967 - val_loss: 1.4726 - val_acc: 0.5099\n",
      "Epoch 61/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7295 - acc: 0.9968 - val_loss: 1.4675 - val_acc: 0.5117\n",
      "Epoch 62/100\n",
      "12116/12116 [==============================] - 1s 98us/step - loss: 0.7294 - acc: 0.9966 - val_loss: 1.4583 - val_acc: 0.5168\n",
      "Epoch 63/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7292 - acc: 0.9969 - val_loss: 1.4433 - val_acc: 0.5330\n",
      "Epoch 64/100\n",
      "12116/12116 [==============================] - 1s 96us/step - loss: 0.7290 - acc: 0.9970 - val_loss: 1.4454 - val_acc: 0.5273\n",
      "Epoch 65/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7289 - acc: 0.9970 - val_loss: 1.4350 - val_acc: 0.5345\n",
      "Epoch 66/100\n",
      "12116/12116 [==============================] - 1s 78us/step - loss: 0.7287 - acc: 0.9969 - val_loss: 1.4380 - val_acc: 0.5307\n",
      "Epoch 67/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7287 - acc: 0.9967 - val_loss: 1.4179 - val_acc: 0.5501\n",
      "Epoch 68/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7283 - acc: 0.9967 - val_loss: 1.4233 - val_acc: 0.5421\n",
      "Epoch 69/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7283 - acc: 0.9969 - val_loss: 1.4262 - val_acc: 0.5348\n",
      "Epoch 70/100\n",
      "12116/12116 [==============================] - 1s 73us/step - loss: 0.7281 - acc: 0.9970 - val_loss: 1.4106 - val_acc: 0.5548\n",
      "Epoch 71/100\n",
      "12116/12116 [==============================] - 1s 84us/step - loss: 0.7278 - acc: 0.9971 - val_loss: 1.3971 - val_acc: 0.5590\n",
      "Epoch 72/100\n",
      "12116/12116 [==============================] - 1s 75us/step - loss: 0.7277 - acc: 0.9970 - val_loss: 1.3877 - val_acc: 0.5645\n",
      "Epoch 73/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7276 - acc: 0.9969 - val_loss: 1.3855 - val_acc: 0.5631\n",
      "Epoch 74/100\n",
      "12116/12116 [==============================] - 1s 88us/step - loss: 0.7275 - acc: 0.9970 - val_loss: 1.3854 - val_acc: 0.5640\n",
      "Epoch 75/100\n",
      "12116/12116 [==============================] - 1s 91us/step - loss: 0.7273 - acc: 0.9971 - val_loss: 1.3775 - val_acc: 0.5673\n",
      "Epoch 76/100\n",
      "12116/12116 [==============================] - 1s 80us/step - loss: 0.7272 - acc: 0.9970 - val_loss: 1.3728 - val_acc: 0.5705\n",
      "Epoch 77/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7271 - acc: 0.9969 - val_loss: 1.3608 - val_acc: 0.5816\n",
      "Epoch 78/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7270 - acc: 0.9969 - val_loss: 1.3653 - val_acc: 0.5753\n",
      "Epoch 79/100\n",
      "12116/12116 [==============================] - 1s 84us/step - loss: 0.7269 - acc: 0.9971 - val_loss: 1.3564 - val_acc: 0.5770\n",
      "Epoch 80/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7267 - acc: 0.9973 - val_loss: 1.3536 - val_acc: 0.5835\n",
      "Epoch 81/100\n",
      "12116/12116 [==============================] - 1s 95us/step - loss: 0.7267 - acc: 0.9971 - val_loss: 1.3424 - val_acc: 0.5913\n",
      "Epoch 82/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7265 - acc: 0.9971 - val_loss: 1.3438 - val_acc: 0.5857\n",
      "Epoch 83/100\n",
      "12116/12116 [==============================] - 1s 94us/step - loss: 0.7265 - acc: 0.9973 - val_loss: 1.3459 - val_acc: 0.5789\n",
      "Epoch 84/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7265 - acc: 0.9974 - val_loss: 1.3412 - val_acc: 0.5856\n",
      "Epoch 85/100\n",
      "12116/12116 [==============================] - 1s 75us/step - loss: 0.7263 - acc: 0.9974 - val_loss: 1.3393 - val_acc: 0.5878\n",
      "Epoch 86/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7264 - acc: 0.9970 - val_loss: 1.3284 - val_acc: 0.5929\n",
      "Epoch 87/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7262 - acc: 0.9972 - val_loss: 1.3272 - val_acc: 0.5973\n",
      "Epoch 88/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7262 - acc: 0.9973 - val_loss: 1.3272 - val_acc: 0.5950\n",
      "Epoch 89/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7262 - acc: 0.9974 - val_loss: 1.3172 - val_acc: 0.6009\n",
      "Epoch 90/100\n",
      "12116/12116 [==============================] - 1s 71us/step - loss: 0.7260 - acc: 0.9972 - val_loss: 1.3208 - val_acc: 0.5956\n",
      "Epoch 91/100\n",
      "12116/12116 [==============================] - 1s 64us/step - loss: 0.7259 - acc: 0.9974 - val_loss: 1.3108 - val_acc: 0.6095\n",
      "Epoch 92/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7259 - acc: 0.9973 - val_loss: 1.3163 - val_acc: 0.5970\n",
      "Epoch 93/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7259 - acc: 0.9973 - val_loss: 1.3147 - val_acc: 0.6037\n",
      "Epoch 94/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7258 - acc: 0.9970 - val_loss: 1.3103 - val_acc: 0.6069\n",
      "Epoch 95/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7257 - acc: 0.9975 - val_loss: 1.3087 - val_acc: 0.6069\n",
      "Epoch 96/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7256 - acc: 0.9972 - val_loss: 1.2975 - val_acc: 0.6186\n",
      "Epoch 97/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7257 - acc: 0.9971 - val_loss: 1.3142 - val_acc: 0.6010\n",
      "Epoch 98/100\n",
      "12116/12116 [==============================] - 1s 62us/step - loss: 0.7256 - acc: 0.9970 - val_loss: 1.3085 - val_acc: 0.6089\n",
      "Epoch 99/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7255 - acc: 0.9974 - val_loss: 1.3045 - val_acc: 0.6123\n",
      "Epoch 100/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7254 - acc: 0.9974 - val_loss: 1.2919 - val_acc: 0.6241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15519a668>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "T = 5\n",
    "w = 0.5 / (T**2) \n",
    "\n",
    "# apply both hard & soft targets to learn\n",
    "y_hard_soft_train_78 = np.concatenate((y_train_78, y_train_soft_78), axis=1)\n",
    "\n",
    "# student net - WITH distillation & weighted hard soft loss - digit 3 omitted in the transfer set\n",
    "mix_78 = Sequential()\n",
    "mix_78.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "mix_78.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "mix_78.add(Dense(10, name='logit'))\n",
    "\n",
    "# y_pred at the end of 10-node dense layer is the logit_pred\n",
    "mix_78.compile(loss=lambda y_true, y_pred: avg_mix_loss(y_true, y_pred, w, T), \n",
    "               optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "mix_78.fit(x_train_78, y_hard_soft_train_78, \n",
    "           batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_hard_soft_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Bias\n",
    "Similar to experiments on omitting 3's, we can also tune the bias in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12116/12116 [==============================] - 2s 158us/step - loss: 1.1857 - acc: 0.9439 - val_loss: 2.7235 - val_acc: 0.1951\n",
      "Epoch 2/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.8168 - acc: 0.9835 - val_loss: 2.4055 - val_acc: 0.1966\n",
      "Epoch 3/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7902 - acc: 0.9879 - val_loss: 2.2500 - val_acc: 0.1972\n",
      "Epoch 4/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7781 - acc: 0.9889 - val_loss: 2.1407 - val_acc: 0.1979\n",
      "Epoch 5/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7688 - acc: 0.9896 - val_loss: 1.9956 - val_acc: 0.1990\n",
      "Epoch 6/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7621 - acc: 0.9907 - val_loss: 1.9205 - val_acc: 0.2064\n",
      "Epoch 7/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7574 - acc: 0.9910 - val_loss: 1.8677 - val_acc: 0.2155\n",
      "Epoch 8/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7539 - acc: 0.9914 - val_loss: 1.8046 - val_acc: 0.2334\n",
      "Epoch 9/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7506 - acc: 0.9917 - val_loss: 1.7750 - val_acc: 0.2409\n",
      "Epoch 10/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7482 - acc: 0.9920 - val_loss: 1.7308 - val_acc: 0.2534\n",
      "Epoch 11/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7460 - acc: 0.9920 - val_loss: 1.6992 - val_acc: 0.2588\n",
      "Epoch 12/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7443 - acc: 0.9926 - val_loss: 1.6775 - val_acc: 0.2617\n",
      "Epoch 13/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7426 - acc: 0.9925 - val_loss: 1.6517 - val_acc: 0.2693\n",
      "Epoch 14/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7417 - acc: 0.9931 - val_loss: 1.6111 - val_acc: 0.2863\n",
      "Epoch 15/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7401 - acc: 0.9931 - val_loss: 1.5931 - val_acc: 0.2908\n",
      "Epoch 16/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7391 - acc: 0.9933 - val_loss: 1.5818 - val_acc: 0.2927\n",
      "Epoch 17/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7383 - acc: 0.9932 - val_loss: 1.5364 - val_acc: 0.3159\n",
      "Epoch 18/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7373 - acc: 0.9936 - val_loss: 1.5101 - val_acc: 0.3225\n",
      "Epoch 19/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7366 - acc: 0.9937 - val_loss: 1.4798 - val_acc: 0.3437\n",
      "Epoch 20/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7358 - acc: 0.9941 - val_loss: 1.4584 - val_acc: 0.3529\n",
      "Epoch 21/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7352 - acc: 0.9940 - val_loss: 1.4392 - val_acc: 0.3604\n",
      "Epoch 22/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7344 - acc: 0.9941 - val_loss: 1.4249 - val_acc: 0.3673\n",
      "Epoch 23/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7338 - acc: 0.9944 - val_loss: 1.4007 - val_acc: 0.3840\n",
      "Epoch 24/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7333 - acc: 0.9944 - val_loss: 1.3750 - val_acc: 0.4071\n",
      "Epoch 25/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7328 - acc: 0.9943 - val_loss: 1.3739 - val_acc: 0.3989\n",
      "Epoch 26/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7324 - acc: 0.9947 - val_loss: 1.3493 - val_acc: 0.4254\n",
      "Epoch 27/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7319 - acc: 0.9946 - val_loss: 1.3388 - val_acc: 0.4246\n",
      "Epoch 28/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7314 - acc: 0.9944 - val_loss: 1.3160 - val_acc: 0.4396\n",
      "Epoch 29/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7311 - acc: 0.9946 - val_loss: 1.3113 - val_acc: 0.4490\n",
      "Epoch 30/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7307 - acc: 0.9944 - val_loss: 1.3019 - val_acc: 0.4496\n",
      "Epoch 31/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7303 - acc: 0.9945 - val_loss: 1.2916 - val_acc: 0.4579\n",
      "Epoch 32/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7300 - acc: 0.9947 - val_loss: 1.2824 - val_acc: 0.4624\n",
      "Epoch 33/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7298 - acc: 0.9945 - val_loss: 1.2597 - val_acc: 0.4820\n",
      "Epoch 34/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7296 - acc: 0.9950 - val_loss: 1.2578 - val_acc: 0.4777\n",
      "Epoch 35/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7292 - acc: 0.9953 - val_loss: 1.2347 - val_acc: 0.5060\n",
      "Epoch 36/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7291 - acc: 0.9951 - val_loss: 1.2388 - val_acc: 0.4992\n",
      "Epoch 37/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7287 - acc: 0.9951 - val_loss: 1.2439 - val_acc: 0.4876\n",
      "Epoch 38/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7286 - acc: 0.9951 - val_loss: 1.2358 - val_acc: 0.4965\n",
      "Epoch 39/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7284 - acc: 0.9947 - val_loss: 1.2146 - val_acc: 0.5168\n",
      "Epoch 40/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7282 - acc: 0.9953 - val_loss: 1.2164 - val_acc: 0.5105\n",
      "Epoch 41/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7281 - acc: 0.9954 - val_loss: 1.2115 - val_acc: 0.5148\n",
      "Epoch 42/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7279 - acc: 0.9957 - val_loss: 1.1935 - val_acc: 0.5381\n",
      "Epoch 43/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7279 - acc: 0.9953 - val_loss: 1.1997 - val_acc: 0.5235\n",
      "Epoch 44/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7276 - acc: 0.9950 - val_loss: 1.1819 - val_acc: 0.5383\n",
      "Epoch 45/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7274 - acc: 0.9953 - val_loss: 1.1809 - val_acc: 0.5380\n",
      "Epoch 46/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7273 - acc: 0.9955 - val_loss: 1.1855 - val_acc: 0.5288\n",
      "Epoch 47/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7272 - acc: 0.9954 - val_loss: 1.1608 - val_acc: 0.5568\n",
      "Epoch 48/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7271 - acc: 0.9951 - val_loss: 1.1711 - val_acc: 0.5418\n",
      "Epoch 49/100\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.7270 - acc: 0.9954 - val_loss: 1.1627 - val_acc: 0.5519\n",
      "Epoch 50/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7268 - acc: 0.9954 - val_loss: 1.1500 - val_acc: 0.5686\n",
      "Epoch 51/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7267 - acc: 0.9957 - val_loss: 1.1531 - val_acc: 0.5630\n",
      "Epoch 52/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7267 - acc: 0.9954 - val_loss: 1.1511 - val_acc: 0.5603\n",
      "Epoch 53/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7265 - acc: 0.9951 - val_loss: 1.1462 - val_acc: 0.5631\n",
      "Epoch 54/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7264 - acc: 0.9955 - val_loss: 1.1280 - val_acc: 0.5838\n",
      "Epoch 55/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7263 - acc: 0.9955 - val_loss: 1.1308 - val_acc: 0.5820\n",
      "Epoch 56/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7262 - acc: 0.9957 - val_loss: 1.1296 - val_acc: 0.5783\n",
      "Epoch 57/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7261 - acc: 0.9958 - val_loss: 1.1392 - val_acc: 0.5654\n",
      "Epoch 58/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7259 - acc: 0.9959 - val_loss: 1.1230 - val_acc: 0.5842\n",
      "Epoch 59/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7258 - acc: 0.9957 - val_loss: 1.1268 - val_acc: 0.5737\n",
      "Epoch 60/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7258 - acc: 0.9956 - val_loss: 1.1162 - val_acc: 0.5952\n",
      "Epoch 61/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7257 - acc: 0.9960 - val_loss: 1.1146 - val_acc: 0.5917\n",
      "Epoch 62/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7256 - acc: 0.9958 - val_loss: 1.1011 - val_acc: 0.6008\n",
      "Epoch 63/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7255 - acc: 0.9960 - val_loss: 1.1063 - val_acc: 0.5917\n",
      "Epoch 64/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7255 - acc: 0.9960 - val_loss: 1.1095 - val_acc: 0.5871\n",
      "Epoch 65/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7254 - acc: 0.9961 - val_loss: 1.0997 - val_acc: 0.6020\n",
      "Epoch 66/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7253 - acc: 0.9962 - val_loss: 1.0974 - val_acc: 0.6036\n",
      "Epoch 67/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7252 - acc: 0.9961 - val_loss: 1.1014 - val_acc: 0.5965\n",
      "Epoch 68/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7251 - acc: 0.9965 - val_loss: 1.0910 - val_acc: 0.6101\n",
      "Epoch 69/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7251 - acc: 0.9960 - val_loss: 1.0862 - val_acc: 0.6142\n",
      "Epoch 70/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7250 - acc: 0.9963 - val_loss: 1.0899 - val_acc: 0.6071\n",
      "Epoch 71/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7249 - acc: 0.9963 - val_loss: 1.0895 - val_acc: 0.6066\n",
      "Epoch 72/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7249 - acc: 0.9966 - val_loss: 1.0904 - val_acc: 0.6071\n",
      "Epoch 73/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7248 - acc: 0.9965 - val_loss: 1.0833 - val_acc: 0.6128\n",
      "Epoch 74/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7248 - acc: 0.9965 - val_loss: 1.0821 - val_acc: 0.6143\n",
      "Epoch 75/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7247 - acc: 0.9966 - val_loss: 1.0708 - val_acc: 0.6232\n",
      "Epoch 76/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7246 - acc: 0.9967 - val_loss: 1.0802 - val_acc: 0.6112\n",
      "Epoch 77/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7245 - acc: 0.9963 - val_loss: 1.0816 - val_acc: 0.6074\n",
      "Epoch 78/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7246 - acc: 0.9966 - val_loss: 1.0781 - val_acc: 0.6122\n",
      "Epoch 79/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7244 - acc: 0.9965 - val_loss: 1.0716 - val_acc: 0.6245\n",
      "Epoch 80/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7246 - acc: 0.9965 - val_loss: 1.0750 - val_acc: 0.6102\n",
      "Epoch 81/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7244 - acc: 0.9965 - val_loss: 1.0701 - val_acc: 0.6215\n",
      "Epoch 82/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7243 - acc: 0.9965 - val_loss: 1.0655 - val_acc: 0.6270\n",
      "Epoch 83/100\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.7242 - acc: 0.9966 - val_loss: 1.0671 - val_acc: 0.6170\n",
      "Epoch 84/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7242 - acc: 0.9968 - val_loss: 1.0557 - val_acc: 0.6298\n",
      "Epoch 85/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7242 - acc: 0.9965 - val_loss: 1.0649 - val_acc: 0.6263\n",
      "Epoch 86/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7243 - acc: 0.9965 - val_loss: 1.0625 - val_acc: 0.6266\n",
      "Epoch 87/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7242 - acc: 0.9962 - val_loss: 1.0639 - val_acc: 0.6226\n",
      "Epoch 88/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7241 - acc: 0.9968 - val_loss: 1.0573 - val_acc: 0.6279\n",
      "Epoch 89/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7241 - acc: 0.9967 - val_loss: 1.0604 - val_acc: 0.6205\n",
      "Epoch 90/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7241 - acc: 0.9968 - val_loss: 1.0581 - val_acc: 0.6277\n",
      "Epoch 91/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7240 - acc: 0.9965 - val_loss: 1.0475 - val_acc: 0.6412\n",
      "Epoch 92/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7239 - acc: 0.9964 - val_loss: 1.0507 - val_acc: 0.6291\n",
      "Epoch 93/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7239 - acc: 0.9969 - val_loss: 1.0493 - val_acc: 0.6455\n",
      "Epoch 94/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7239 - acc: 0.9969 - val_loss: 1.0559 - val_acc: 0.6333\n",
      "Epoch 95/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7238 - acc: 0.9967 - val_loss: 1.0523 - val_acc: 0.6356\n",
      "Epoch 96/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7238 - acc: 0.9965 - val_loss: 1.0415 - val_acc: 0.6445\n",
      "Epoch 97/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7238 - acc: 0.9967 - val_loss: 1.0436 - val_acc: 0.6428\n",
      "Epoch 98/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7237 - acc: 0.9969 - val_loss: 1.0450 - val_acc: 0.6358\n",
      "Epoch 99/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7237 - acc: 0.9969 - val_loss: 1.0400 - val_acc: 0.6423\n",
      "Epoch 100/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7237 - acc: 0.9969 - val_loss: 1.0510 - val_acc: 0.6302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1591e5f60>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_78_bias = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "bias = soft_78_bias.layers[2].get_weights()[1]\n",
    "bias[7] = 1\n",
    "bias[8] = 1\n",
    "K.set_value(soft_78_bias.layers[2].bias, bias)\n",
    "\n",
    "soft_78_bias.fit(x_train_78, y_train_soft_78, \n",
    "                 batch_size=128, epochs=100,verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Accuracy on Test set === \n",
      "\n",
      "NO DISTILLATION\n",
      "loss = 11.931765022277832, accuracy = 0.1986, #errors = 8014\n",
      "\n",
      "WITH DISTILLATION\n",
      "loss = 1.07515452003479, accuracy = 0.6182, #errors = 3818\n",
      "\n",
      "WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET\n",
      "loss = 1.2918906003952026, accuracy = 0.6241, #errors = 3759\n",
      "\n",
      "WITH DISTILLATION & BIAS TUNED\n",
      "loss = 1.050957187271118, accuracy = 0.6302, #errors = 3698\n"
     ]
    }
   ],
   "source": [
    "print('=== Overall Accuracy on Test set === \\n')\n",
    "\n",
    "loss, accuracy = hard_78.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('NO DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION')\n",
    "loss, accuracy = soft_78.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET')\n",
    "loss, accuracy = mix_78.evaluate(x_test, y_hard_soft_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION & BIAS TUNED')\n",
    "loss, accuracy = soft_78_bias.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Only 7 & 8 Distillation Performance on Full Test Set ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.1986</td>\n",
       "      <td>11.931765</td>\n",
       "      <td>8014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_tuned</th>\n",
       "      <td>0.6302</td>\n",
       "      <td>1.050957</td>\n",
       "      <td>3698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.6182</td>\n",
       "      <td>1.075155</td>\n",
       "      <td>3818.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.6241</td>\n",
       "      <td>1.291891</td>\n",
       "      <td>3759.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy       loss  num_error\n",
       "baseline                    0.1986  11.931765     8014.0\n",
       "bias_tuned                  0.6302   1.050957     3698.0\n",
       "dense_distilled             0.6182   1.075155     3818.0\n",
       "dense_distilled_mix_loss    0.6241   1.291891     3759.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep78_summary = {\n",
    "    'baseline': {'loss': 11.931765022277832, 'accuracy': 0.1986, 'num_error': 8014},\n",
    "    'dense_distilled': {'loss': 1.07515452003479, 'accuracy': 0.6182, 'num_error': 3818},\n",
    "    'dense_distilled_mix_loss': {'loss': 1.2918906003952026, 'accuracy': 0.6241, 'num_error': 3759},\n",
    "    'bias_tuned': {'loss': 1.050957187271118, 'accuracy': 0.6302, 'num_error': 3698}\n",
    "}\n",
    "\n",
    "print('=== Only 7 & 8 Distillation Performance on Full Test Set ===')\n",
    "df_keep78_summary = pd.DataFrame().from_dict(keep78_summary).T\n",
    "display(df_keep78_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion - distillation performances of student nets on transfer sets of digit 7 & 8 only\n",
    "\n",
    "The non-distilled student baseline achieved the worst accuracy = 19.9%. All other distilled models performed stunningly better, with accuracies over 61%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Experiments Part III - Distillation Applied to Fake Datasets\n",
    "\n",
    "As shown in the first figure of this tutorial, increasing temperature leads to increasing target \"softness\". Therefore, we hypothesized that datasets with different data quality (i.e. degree of class separability and the number of informative features) may require different temperature for optimal distillation outcome. In the following experiment, we used the $\\texttt{sklearn}$ library to synthesize 9 datasets with varying degrees of data quality. Specifically, the datasets were synthesized using the follwoing parameters: $n\\_samples$ = 50000, $n\\_features$ = 100, $n\\_classes$ = 10, $n\\_clusters\\_per\\_class$ = 1. In addition, data quailty was synthesized using $class\\_sep \\in (0.2, 0.6, 1)$ and $p\\_informative \\in (0.5, 0.8, 1)$ where $n\\_informative$ = $p\\_informative$ x $n\\_features$. The teacher net has 3 hidden layers, each with 100 nodes; the student net has 2 hidden layers, each with 50 nodes. Distillation was done at $T \\in (2, 2.5, 3, 4, 5, 10, 15, 20)$. Both teacher and students were trained for 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the FakeDataGenerator class to generate fake data with the option to vary data quality (as defined by the number of informative features and class separation) and the structures of the teacher/student net. The FakeTeacherNet has 3 Dense layers, each with 100 nodes and interspersed with a Dropout of 0.2. The FakeStudentNet has 2 Dense layers, each with 50 nodes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fake dataset class\n",
    "class FakeDataGenerator():\n",
    "    def __init__(self, n_samples, n_features, n_classes, n_clusters_per_class):\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.n_clusters_per_class = n_clusters_per_class\n",
    "        \n",
    "    def generate_data(self, p_informative, class_sep):\n",
    "        self.n_informative = round(p_informative*self.n_features)\n",
    "        self.n_redundant = self.n_features - self.n_informative\n",
    "        self.class_sep = class_sep\n",
    "        self.X, self.Y = make_classification(n_samples=self.n_samples, \n",
    "                                             n_features=self.n_features,\n",
    "                                             n_redundant=self.n_redundant,\n",
    "                                             n_informative=self.n_informative,\n",
    "                                             n_clusters_per_class=self.n_clusters_per_class, \n",
    "                                             n_classes=self.n_classes,\n",
    "                                             class_sep=self.class_sep)\n",
    "        return self.X, self.Y\n",
    "\n",
    "# Teacher net structure\n",
    "def FakeTeacherNet(input_shape, n_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, name='hidden_1', input_shape=input_shape, activation='relu'))\n",
    "    model.add(Dropout(0.2, name='dropout_1'))\n",
    "    model.add(Dense(100, name='hidden_2', activation='relu'))\n",
    "    model.add(Dropout(0.2, name='dropout_2'))\n",
    "    model.add(Dense(100, name='hidden_3', activation='relu'))\n",
    "    model.add(Dropout(0.2, name='dropout_3'))\n",
    "    model.add(Dense(n_classes, name='logit'))\n",
    "    model.add(Activation('softmax', name='softmax'))\n",
    "    return model\n",
    "\n",
    "# Student net structure\n",
    "def FakeStudentNet(input_shape, n_classes, n_hidden, T=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_hidden, name='hidden_1', input_shape=input_shape, activation='relu'))\n",
    "    model.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "    model.add(Dense(10, name='logit'))\n",
    "    model.add(Lambda(lambda x: x / T, name='logit_soft'))\n",
    "    model.add(Activation('softmax', name='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide code to define functions that trains the teacher net (train_teacher), trains the student net without distillation (train_student) and distills the student at specified temperature (distill_student)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x_train, x_test):\n",
    "    x_train_mean = x_train.mean(axis=0)\n",
    "    x_train_std = x_train.std(axis=0)\n",
    "    x_train_norm = (x_train - x_train_mean) / x_train_std\n",
    "    x_test_norm = (x_test - x_train_mean) / x_train_std\n",
    "    return x_train_norm, x_test_norm\n",
    "\n",
    "def train_teacher(x_train, y_train, x_test, y_test, n_classes, \n",
    "                  batch_size=100, epochs=50, save_model=False, model_name=None):\n",
    "    '''\n",
    "    Function to build a cumbersome teacher net using hard cross-entropy loss\n",
    "    ''' \n",
    "    model = FakeTeacherNet(input_shape=(x_train.shape[1],), n_classes=n_classes)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    # train teacher net\n",
    "    model.fit(x_train, y_train, \n",
    "              batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    \n",
    "    # teacher net model evaluation\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    loss, accuracy = score[0], score[1]\n",
    "    num_errors = int((1-score[1])*len(x_test))\n",
    "    print('On test set:')\n",
    "    print('loss = {}, accuracy = {}'.format(loss, accuracy))\n",
    "    print('#errors = {}'.format(num_errors))\n",
    "    \n",
    "    # save the fitted model\n",
    "    if save_model:\n",
    "        directory = './models/fakeset'\n",
    "        filename = os.path.join(directory, 'fake_teacher.h5')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        if model_name:\n",
    "            filename = os.path.join(directory, model_name)\n",
    "        model.save(filename)\n",
    "        print(\"model saved at {}\".format(filename))\n",
    "    \n",
    "    return model, loss, accuracy, num_errors\n",
    "\n",
    "def train_student(x_train, y_train, x_test, y_test, n_classes, n_hidden, \n",
    "                  batch_size=100, epochs=50, save_model=False, model_name=None):\n",
    "    '''\n",
    "    Function to build a smaller student net with T = 1 as baseline for distillation performance\n",
    "    ''' \n",
    "    model = FakeStudentNet(input_shape=(x_train.shape[1],), n_classes=n_classes, n_hidden=n_hidden, T=1)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    # train student\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    \n",
    "    # student net model evaluation\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    loss, accuracy = score[0], score[1]\n",
    "    num_errors = int((1-score[1])*len(x_test))\n",
    "    print('On test set:')\n",
    "    print('loss = {}, accuracy = {}'.format(loss, accuracy))\n",
    "    print('#errors = {}'.format(num_errors))\n",
    "    \n",
    "    # save the fitted model\n",
    "    if save_model:\n",
    "        directory = './models/fakeset'\n",
    "        filename = os.path.join(directory, 'fake_student.h5')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        if model_name:\n",
    "            filename = os.path.join(directory, model_name)\n",
    "        model.save(filename)\n",
    "        print(\"model saved at {}\".format(filename))\n",
    "    \n",
    "    return model, loss, accuracy, num_errors\n",
    "            \n",
    "def distill_student(x_train, y_train, x_test, y_test, n_classes, n_hidden, T, teacher_model, \n",
    "                    batch_size=100, epochs=50, save_model=False, model_name=None):\n",
    "    '''\n",
    "    Function to distill knowledge from the fitted 'teacher_model' to a student net under some T\n",
    "    ''' \n",
    "    # make the 'soft target' to train the student net\n",
    "    teacher_logit = Model(inputs=teacher_model.input, outputs=teacher_model.get_layer('logit').output)\n",
    "    logit_train = teacher_logit.predict(x_train)\n",
    "    y_train_soft = K.softmax(logit_train / T).eval(session=K.get_session())\n",
    "    \n",
    "    # build a baseline student net with T specified\n",
    "    model = FakeStudentNet(input_shape=(x_train.shape[1],), n_classes=n_classes, n_hidden=n_hidden, T=T)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    # train the student net with the 'soft target'\n",
    "    model.fit(x_train, y_train_soft, \n",
    "              batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    \n",
    "    # distilled student net model evaluation\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    loss, accuracy = score[0], score[1]\n",
    "    num_errors = int((1-score[1])*len(x_test))\n",
    "    print('On test set:')\n",
    "    print('loss = {}, accuracy = {}'.format(loss, accuracy))\n",
    "    print('#errors = {}'.format(num_errors))\n",
    "    \n",
    "    # save the fitted model\n",
    "    if save_model:\n",
    "        directory = './models/fakeset'\n",
    "        filename = os.path.join(directory, 'fake_distill_student.h5')\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        if model_name:\n",
    "            filename = os.path.join(directory, model_name)\n",
    "        model.save(filename)\n",
    "        print(\"model saved at {}\".format(filename))\n",
    "    \n",
    "    return model, loss, accuracy, num_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the function to perform 1 experiment at 1 data quality. For each experiment, a fake dataset is generated given the $\\textit{class_sep}$ and $\\textit{p_informative}$ parameters. The teacher net and the student with $\\textit{n_hidden}$ nodes are trained for $\\textit{epochs}$ number of epochs. Subsequently, student nets are distilled at the temperatures provided in the $\\textit{Ts}$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function to perform 1 experiment at 1 data quality and varying temperatures\n",
    "def experiment(class_sep, p_informative, epochs, n_hidden, Ts):\n",
    "    '''\n",
    "    Function to run experiments on fake datasets generated from different hyper parameter sets\n",
    "    '''\n",
    "    # constant params for the fake dataset\n",
    "    n_samples = 50000\n",
    "    n_features = 100\n",
    "    n_classes = 10\n",
    "    n_clusters_per_class = 1\n",
    "    \n",
    "    # constant params for training\n",
    "    test_size = 0.2\n",
    "    random_state = 12345\n",
    "    batch_size=100\n",
    "    \n",
    "    # dictionaries for results\n",
    "    results = {\n",
    "        'experiment_meta': {},\n",
    "        'teacher': {},\n",
    "        'student': {},\n",
    "        'distilled_student': {}  \n",
    "    }\n",
    "    \n",
    "    # record results\n",
    "    results['experiment_meta']['n_samples'] = n_samples\n",
    "    results['experiment_meta']['n_features'] = n_features\n",
    "    results['experiment_meta']['n_classes'] = n_classes\n",
    "    results['experiment_meta']['n_clusters_per_class'] = n_clusters_per_class\n",
    "    results['experiment_meta']['test_size'] = test_size\n",
    "    results['experiment_meta']['random_state'] = random_state\n",
    "    results['experiment_meta']['batch_size'] = batch_size\n",
    "    \n",
    "    results['experiment_meta']['class_sep'] = class_sep\n",
    "    results['experiment_meta']['p_informative'] = p_informative\n",
    "    results['experiment_meta']['epochs'] = epochs\n",
    "    results['experiment_meta']['n_hidden'] = n_hidden\n",
    "    results['experiment_meta']['Ts'] = Ts\n",
    "    \n",
    "    # generate dataset\n",
    "    FakeDataset = FakeDataGenerator(n_samples, n_features, n_classes, n_clusters_per_class)\n",
    "    X, y = FakeDataset.generate_data(p_informative=p_informative, class_sep=class_sep)\n",
    "\n",
    "    # test train split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # standardize features\n",
    "    x_train, x_test = standardize(x_train, x_test)\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "    \n",
    "    # train teacher\n",
    "    print(\"training teacher...\")\n",
    "    teacher_model_filename = 'teacher_classsep={}_pinform={}.h5'.format(class_sep, p_informative)\n",
    "    teacher_model, teacher_loss, teacher_acc, teacher_nerr = train_teacher(x_train, y_train, x_test, y_test, \n",
    "                                                                           n_classes, batch_size=batch_size, \n",
    "                                                                           epochs=epochs, save_model=True, \n",
    "                                                                           model_name=teacher_model_filename)\n",
    "    \n",
    "    results['teacher']['loss'] = teacher_loss\n",
    "    results['teacher']['accuracy'] = teacher_acc\n",
    "    results['teacher']['num_err'] = teacher_nerr\n",
    "    \n",
    "    \n",
    "    # train basline student -- NO distillation\n",
    "    print(\"training student...\")\n",
    "    student_model_filename = 'student_classsep={}_pinform={}_nhidden={}.h5'.format(class_sep, p_informative, n_hidden)\n",
    "    student_model, student_loss, student_acc, student_nerr = train_student(x_train, y_train, x_test, y_test, \n",
    "                                                                           n_classes, n_hidden=n_hidden, \n",
    "                                                                           batch_size=batch_size, \n",
    "                                                                           epochs=epochs, save_model=True, \n",
    "                                                                           model_name=student_model_filename)\n",
    "    \n",
    "    results['student']['loss'] = student_loss\n",
    "    results['student']['accuracy'] = student_acc\n",
    "    results['student']['num_err'] = student_nerr\n",
    "    \n",
    "    # distill student -- WITH distillation\n",
    "    print(\"distilling student...\")\n",
    "    for T in Ts:\n",
    "        print(\"At T={}\".format(T))\n",
    "        distill_student_model_filename = 'distill_student_classsep={}_pinform={}_nhidden={}_T={}.h5'.format(class_sep, p_informative, n_hidden, T)\n",
    "        distill_student_model, d_stu_loss, d_stu_acc, d_stu_nerr = distill_student(x_train, y_train, x_test, y_test, \n",
    "                                                                                   n_classes, n_hidden=n_hidden, T=T, \n",
    "                                                                                   teacher_model=teacher_model, \n",
    "                                                                                   batch_size=batch_size, \n",
    "                                                                                   epochs=epochs, save_model=True, \n",
    "                                                                                   model_name=distill_student_model_filename)\n",
    "        results['distilled_student'][T] = {}\n",
    "        results['distilled_student'][T]['loss'] = d_stu_loss\n",
    "        results['distilled_student'][T]['accuracy'] = d_stu_acc\n",
    "        results['distilled_student'][T]['num_err'] = d_stu_nerr\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then performed 9 experiments with the parameters specified below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params for training\n",
    "epochs = 50\n",
    "n_hidden = 50 # num hidden nodes for student network\n",
    "\n",
    "# T to experiment for distillation\n",
    "Ts = [2, 2.5, 3, 4, 5, 10, 15, 20]\n",
    "class_sep_list = [0.2, 0.6, 1]\n",
    "p_informative_list = [0.5, 0.8, 1]\n",
    "var_combo = list(itertools.product(class_sep_list, p_informative_list))\n",
    "\n",
    "results_list = []\n",
    "for cs, pi in var_combo:\n",
    "    result = experiment(cs, pi, epochs, n_hidden, Ts)\n",
    "    results_list.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the results of these 9 experiments as a json files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving results\n",
    "def dump_pkl(output_path, data):\n",
    "    bytes_out = cPickle.dumps(data)\n",
    "    with open(output_path, 'wb') as f_out:\n",
    "        for idx in range(0, len(bytes_out), MAX_BYTES):\n",
    "            f_out.write(bytes_out[idx:idx+MAX_BYTES])\n",
    "    print('data dumped to %s' % output_path)\n",
    "            \n",
    "for result in results_list:\n",
    "    class_sep = result['experiment_meta']['class_sep']\n",
    "    p_informative = result['experiment_meta']['p_informative']\n",
    "    \n",
    "    # filepath\n",
    "    directory = './results/fakeset'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filepath = os.path.join(directory, 'class_sep={}_p_info={}_results.json'.format(class_sep, p_informative))\n",
    "    dump_pkl(filepath, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary results for the Fake Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading results\n",
    "def load_pkl(file_path):\n",
    "    bytes_in = bytearray(0)\n",
    "    input_size = os.path.getsize(file_path)\n",
    "    with open(file_path, 'rb') as f_in:\n",
    "        for _ in range(0, input_size, MAX_BYTES):\n",
    "            bytes_in += f_in.read(MAX_BYTES)\n",
    "    data = cPickle.loads(bytes_in)\n",
    "\n",
    "    return data\n",
    "\n",
    "directory = './results/fakeset'\n",
    "results_df = pd.DataFrame(columns=['class_sep', 'p_informative', 'model', \n",
    "                                  'temp', 'num_error', 'loss', 'accuracy'])\n",
    "\n",
    "# extract teacher information\n",
    "counter = 0\n",
    "for idx, (class_sep, p_informative) in enumerate(var_combo):\n",
    "    filepath = os.path.join(directory, 'class_sep={}_p_info={}_results.json'.format(class_sep, p_informative))\n",
    "    result = load_pkl(filepath)\n",
    "    teacher_entry = pd.Series({'class_sep': class_sep, 'p_informative': p_informative, 'model': 'teacher', \n",
    "             'temp': 1, 'num_error': result['teacher']['num_err'], 'loss': result['teacher']['loss'], \n",
    "             'accuracy': result['teacher']['accuracy']}, name= counter)\n",
    "    results_df = results_df.append(teacher_entry)\n",
    "    counter += 1\n",
    "    \n",
    "    student_entry = pd.Series({'class_sep': class_sep, 'p_informative': p_informative, 'model': 'student', \n",
    "             'temp': 1, 'num_error': result['student']['num_err'], 'loss': result['student']['loss'], \n",
    "             'accuracy': result['student']['accuracy']}, name= counter)\n",
    "    results_df = results_df.append(student_entry)\n",
    "    counter += 1\n",
    "    \n",
    "    for temp, data in result['distilled_student'].items():\n",
    "        entry = pd.Series({'class_sep': class_sep, 'p_informative': p_informative, 'model': 'distilled_student', \n",
    "             'temp': temp, 'num_error': data['num_err'], 'loss': data['loss'], \n",
    "             'accuracy': data['accuracy']}, name= counter)\n",
    "        results_df = results_df.append(entry)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>num_error</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_sep</th>\n",
       "      <th>p_informative</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.2</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.5</th>\n",
       "      <th>teacher</th>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>0.100470</td>\n",
       "      <td>0.9882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>0.256389</td>\n",
       "      <td>0.9412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilled_student</th>\n",
       "      <td>2</td>\n",
       "      <td>199</td>\n",
       "      <td>0.230405</td>\n",
       "      <td>0.9801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilled_student</th>\n",
       "      <td>2.5</td>\n",
       "      <td>213</td>\n",
       "      <td>0.362845</td>\n",
       "      <td>0.9786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilled_student</th>\n",
       "      <td>3</td>\n",
       "      <td>231</td>\n",
       "      <td>0.511031</td>\n",
       "      <td>0.9769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          temp num_error      loss  accuracy\n",
       "class_sep p_informative model                                               \n",
       "0.2       0.5           teacher              1       118  0.100470    0.9882\n",
       "                        student              1       587  0.256389    0.9412\n",
       "                        distilled_student    2       199  0.230405    0.9801\n",
       "                        distilled_student  2.5       213  0.362845    0.9786\n",
       "                        distilled_student    3       231  0.511031    0.9769"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.set_index(['class_sep', 'p_informative', 'model'], inplace=True)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABQcAAADlCAYAAADuiUEYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEuIfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15665b080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABQoAAADlCAYAAAAbd8qoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xt8zvX/x/HHtSPbmDlEQg61KZlzQ4jIKVIKbUxIoqwv\nJaacWgclZUWoiK9hDhHVN79ySuUwTCGFb87nTRg7Xjt8fn/suys7aZtd23W5nvfb7Xv72uf6XNfn\ndX1ez12fq/fen8/HZBiGgYiIiIiIiIiIiDg0p9IuQEREREREREREREqfBgpFREREREREREREA4Ui\nIiIiIiIiIiKigUIRERERERERERFBA4UiIiIiIiIiIiKCBgpFRESsJiMj45bensitRL8/IiIiIhoo\nFBERB/PQQw/h5+dn+d8999xDixYtGDhwID/99FO2dVevXo2fnx8PPfRQobZx9epVwsLC+OqrryzL\nQkND8fPzIzQ0FIDTp09bajh9+nSe6xTU0aNHGTx4MGfPns31PlevXl2o15JbX87fgbz+FxUVVdpl\nlpgjR47w9NNPExMTU9qliIiIiJQ6l9IuQEREpDR4e3vj7u6O2WwmLi6OqKgodu7cydSpU3n88ccB\nKFu2LFWrVqVKlSqFeu3+/ftz+PBh7rvvvmzbq1q1Kt7e3sX6PmJiYnj00UdJTU3NtrxKlSqkpaVR\ntmzZYt2e2L+sbADEx8eTkJCAq6srFStWtKzj5uZWWuWVqPPnz9OrV69cvz8iIiIijkoDhSIi4pBC\nQ0Pp3bs3AJcvX2bSpEl8//33hIWF0aFDBypUqEC3bt3o1q1boV87ISEh17Lx48czfvz4m647J7PZ\nnOcgx/Lly4t9W3JruD4bM2fOZNasWTRp0oSIiIhSrKp0pKSkaJBQRERE5Do69VhERByej48PU6dO\nxcPDg8TERL799lsg71OP9+zZw8CBAwkICKBx48b07NmTFStWWB5/6KGHOHPmDJA5OJj13KKeVvzf\n//6XZ599loCAABo2bMjDDz/M7NmzMQyD06dP07FjR8u6HTt2tLx+XqceX7t2jbfeeov27dtz3333\n0a1bN/79739jGIZlnaw6P/30Uz7//HMefPBBGjVqxPDhw7lw4UK+dUZFReHn50fXrl3Zt28f/fr1\no2HDhnTt2pUNGzZY1svvdO6cp7xeX8eMGTNo2bIlLVq04IMPPiA5OZmwsDCaNWtGy5YtCQ8Pz/Ye\nCuLEiRMMHz6cJk2a0Lx5c0JCQjh16pTl8ZkzZ+Ln58f48eMJCQmhUaNGPP/889ne55w5c2jSpAkd\nO3YkPj4egC1bttC/f3+aNGlCixYtCAkJ4dixY3nup7yen+XXX3/Fz8+PBg0acPny5VzLGzVqRHx8\nPHFxcUyZMoUOHTrQsGFD2rVrx8SJE4mLiyvU/sjPsWPHGDZsGI0bN6Z58+a8+OKLllPlAWbMmIGf\nnx9vvfUWCxYsoG3btjRp0oRJkyaRkpJCeHg4LVu2pHnz5rz++uuYzWbL/vfz86NJkyb8/vvvPPXU\nU/j7+/Poo4+yffv2ItXw2muv8fzzz9OoUSNefPFFAA4dOsTQoUOz/f7MnTsXwzA4ceIEnTt3trzO\ngw8+yGuvvQZAu3bt8PPzY+3atbm2M2jQoFzvYeXKlbRo0YJ27dpx6tQpDMPg888/p2PHjpbftaVL\nlxZLT0RERESsSTMKRUREAC8vLxo2bEhUVBT79u0jKCgo1zoXLlzgmWeeITExEQ8PD1xdXTl8+DAT\nJ07Ey8uL7t27U6VKFc6fP096ejre3t6FPm35esnJyQwZMoSYmBg8PDxwd3fn5MmTfPjhh9SoUYP7\n77+fKlWqEBsbC2SeUprfqc3JyckEBQVx+PBhADw9PTl69Chvv/02x44dY8qUKdnWX7ZsGWfPnqVs\n2bIkJyezefNm3Nzc+Oijj25Y8+XLlxk8eDCGYWA2mzl27BgvvfQSP/zwQ7ZTWwtqwYIFxMXF4e7u\nTmJiIp988gnfffcdJ0+epEyZMly+fJk5c+bg6+tL9+7dC/SaFy9eJCgoiIsXL1K2bFnS09P5/vvv\n+eWXX/j666/x8fGxrPv111+TkZGBu7s7vr6+luWnT58mPDyc8uXLU7VqVby8vFizZg2hoaEYhkGZ\nMmVISEjg+++/Z8eOHSxfvpy6deve8PnXa9y4MbVr1+b48eN8//339OvXD4D//Oc/QOagsJeXFy+8\n8AIbNmzA2dkZb29vYmNjWbFiBefOnWPevHmF3t/Xi4mJISgoiEuXLln203fffcevv/7K119/nS1r\n33zzDZcvX6Zs2bIkJiayfPlyoqKiOHHiBB4eHiQkJLB06VJq1arF4MGDLc9LTU1l8ODBJCUlkZGR\nwaFDh3j22WdZvnw5DRo0KFQNa9euzdarxMREnnnmGWJjY/Hw8MDNzY2TJ08yY8YMatWqhb+/f4F/\nf24kOTmZyZMnWz4TatasSXh4OHPmzMFkMuHt7c2xY8d4/fXXSUhI4Nlnn72JroiIiIhYl2YUioiI\n/E/lypUB+Ouvv/J8fO/evSQmJuLv78/u3bvZtWsXI0eOpH379iQnJwOZp3VWq1YNyJwVdzOnAJ86\ndQpfX18eeOABduzYwa5duyyDYfv27aNatWosW7bMsv6yZcvyPb05IiKCw4cP4+3tzZo1a9izZw/v\nvvsuAJGRkezbty/b+hcvXmTFihVER0fTt29fALZu3fqPNV+5coV+/fqxe/dulixZAmSe3hkdHV34\nHUDmIMzXX39NVFQUderUAeDcuXOsWbOGqKgoy+Dbnj17CvyaCxcu5OLFi3Tp0oWdO3eya9cuHn30\nUWJjY3PN+kpNTWXhwoXs2rUr1wBXSEgIu3btYtasWZjNZt5++20Mw6Bv375ER0ezdetWGjZsyNWr\nV5k6dWqu173++XnJulZm1gzXjIwM/u///g+AXr16AVhm33355Zds376dFStW0KxZM2rUqEFKSkqB\n90leFixYwKVLl3jkkUfYtWsXO3fupEePHly4cCFb7iCz70uWLCE6Opr7778fyJxxt3DhQnbv3m1Z\nljMHqampNG7cmF27dvHTTz/h6+tLamoqn3zySaFrSE1NJSIigp07dzJw4EBOnjyJr68vbdq0ISoq\nit27d9OlSxcg83e5Ro0alowCfPHFF4wdO7bQ+ykjI4MnnniC3bt3s3LlSi5fvsz8+fNxcXFh9erV\nREVF8eWXX+Ls7Mwnn3xy030RERERsSbNKBQREfkfk8kEZP6Hf17uueceXF1d2bdvHwMGDOCBBx6g\nVatWPP/88zg7Oxd7PXfffTfz588nJSWFffv2sWfPHn7//Xcg7+sg3sjmzZsB6NOnD/fccw8Ajz32\nGEuWLGHfvn1s2rQJf39/y/otWrSw/NyxY0dWrFhR4G0OHjwYJycnmjdvjo+PD5cvXy50vVmaN29O\nvXr1AGjQoAHHjh2jRYsW+Pn5AZk9OXr0aKFef+fOnUDmIFunTp0ALAO9UVFRvPDCC5Z1q1WrZhnk\nyjnb7NFHHwWgYsWKbN++nbi4OJydnQkNDcXFxQUfHx9CQkIYNmwY27ZtyzVAdP3z89KrVy8+/PBD\ndu3axcWLFzly5AgxMTFUqVKFNm3aAODv78/27dsZNmwYHTp04P7772f27NlUqFChwPsjP1mngf/0\n00+WU9yTkpIsjz333HOWde+66y6aNWsGQMOGDdm5cyd33XUXLVu2BOC+++5j586defbpueeew93d\nHXd3d4KDg5k4caJl4LcwNdxxxx2WGlxdXSlfvjyff/45KSkp/Prrr+zZs4dDhw4BkJiYeNP753o9\ne/YEMnu5efNmzGYzTk5ODB8+3LKOYRhcu3aNAwcO0LRp02LdvoiIiEhx0UChiIjI/1y6dAkg26mn\n16tZsyYzZ85k+vTp7Nmzhz179jBz5kyqVavG1KlTad26dbHWk56eztSpU1m5ciXJycnUrl0bF5fM\nQ3dhr8mXNUuyRo0a2ZbXqFGDffv25ZpFef0+KFOmTKG2ef1zy5Yty+XLl/MdfAUsd+DNy/UDXq6u\nrgB53p23MPsj6/p9V69e5erVq9kei4mJyfbzjU4dv/6xrP3n4+ODp6enZXnW/k5LS+PKlSsFfm2A\n22+/nYCAALZv3853331nOW28R48eloHp9957jylTprBlyxYiIyOJjIzE1dWV/v373/TNcwqzn/Lq\n0/U5uFGfsmbyAtx2222WbRa2hqznZklLS+Ptt99m1apVJCcnU6dOHcvvz43ymJ/09PR8H7u+l1k1\nZ2Rk5Hldz5x1i4iIiNgSDRSKiIiQeffg3377DSDbzLqc2rdvT0BAALGxsezcuZN169axdetWxo4d\ny88//1ysNS1btoyIiAjuuusu5s+fT7Vq1ZgxYwZ//vmnZZ2sWZD/pHLlyhw/ftxyo5UsWTeFuH6w\nBrAMqBRmGwV5rpNT5lVPsm5qAZk3WclPXjM1b3b2ZqVKlTh+/DihoaGW04mTkpIoU6ZMrnqzBknz\ncv1jlSpVArDMnswaLMzav66urrlm+d3otbM89thjbN++nU2bNlkGCrNOO4bMAaqsU8h37tzJzp07\nWbx4MQsXLuSBBx6gXbt2/7iN/FSuXJnTp0/z2muvMXDgQCD//XQzfTpz5gy1atUCsFwvMGtfFaYG\nd3f3bD8vXbqUJUuW4Ovry7x586hatSrvvfdegX5/CpvTvLJQoUIFy4xIwHJtUxERERFbpmsUioiI\nw4uPj+fNN9/k6tWrlC1b1nIaYU4LFy6kadOmBAYG4uPjQ58+fSw3PYmLi7PMjMsaKIuPj7/hbLl/\n8t///hfIHISoWLEisbGxrF+/Hvh7RtT1g3I32l7WqaorV67k4MGDAHz11VeWaxNef/dkaypfvjyQ\nOXvzxIkTAKxZs6ZEtp0l6/TUL774gr/++guz2cyzzz5Ls2bNct0A5EaDpNc/1qRJEzw9PUlPT2fa\ntGmWGYRZ1x9s06ZNroGsggzAdu7cGQ8PD7Zu3UpMTAy+vr6WU8dPnz5N27Ztad68Ofv37+ehhx5i\n5MiRlpl1198tuSiyTo9duXIlly5dwmw2M2TIEJo1a8aCBQtu6rWvN3v2bBISErh27RqRkZHA3z0q\nTA0592fW70/ZsmXx8fHhwoULbNy4Efj79+f6wczrf3+ycvrLL78AmXn98ccf830P12+7QYMGuLu7\nc+XKFct1FH/44QeaNm1Kt27dct3hWkRERMSWaEahiIg4pHfeeYfw8HAyMjK4cuUKqampAEyYMCHf\nU487duzInDlzOHjwIK1bt8bLy8syGNOjRw/LoN0dd9zBiRMnmDZtGp9++mmRZxo2btyYyMhIfvvt\nNwICAjCbzZaBjKzBBh8fHzw8PEhMTCQwMJC2bdvmeWfi4OBg1q5dy7Fjx+jVqxeenp6W68UNGDCA\n++67r0g1FuU9lS1blqSkJHr16kW1atW4cOEC5cuXz3VqqbUEBwezYsUK/vzzT9q1a4e7uzsJCQl4\neXlZrllYWGXKlGHcuHFMmjSJZcuWsXbtWlJTU0lLS6NChQqEhoYW6XU9PDzo2rUrq1evBjJnGGap\nUaMG/v7+bNiwgUGDBuHj40N8fDypqalUrlzZMjhcVE8//TRffPEFhw8fpm3btri5uZGYmEi5cuWK\ndWD5t99+o2XLlhiGQWpqKm5ubpZrD95MDY0bN2bFihXs3buXli1bkpKSYvn9ycp+5cqVKVOmDMnJ\nyfTp04cHH3yQ8PBwWrVqxaFDh1i1ahW7du3i0qVLue5MnZ+KFSvSv39/Pv/8cyZPnsz06dOJj4/H\nMAzL54aIiIiIrdKMQhERcUhxcXFcuHCBixcvUrZsWVq1asW8efN48skn831OzZo1iYiIoHPnzlSo\nUIGkpCRq165NSEgIU6ZMsaw3fPhw6tati8lkwsfHp8izCnv16sXw4cOpUqUKTk5ONGrUiNdffx3I\nvHts1qDKv/71L6pUqYJhGPkOQnh5ebF8+XKCg4O5/fbbMZvN1KlThwkTJjBhwoQi1VcUFStW5MMP\nP6RevXpkZGRQoUIFFi5caDldsyTcdtttLFmyhA4dOlhOGW3VqhULFy6kdu3aRX7dfv36MXfuXJo3\nb47JZKJs2bJ06dKF5cuX39TrZg0OOjs755rt+v777zNixAjuvPNOEhMT8fHxoUuXLixatOim92m1\natVYsmQJ7du3t5zq27p1a/79739bThUuDrNmzaJBgwaYTCbq16/PZ599Rv369W+6ht69ezNs2DAq\nV66MyWSiSZMmTJ48Gcg8TTstLQ13d3fL7w/8PZMwJCSEXr16WQbhBwwYwIsvvljg9zRmzBhefvll\n7rzzTpKTk6levTohISG8+uqrRdlFIiIiIiXGZBT2augiIiIiIjfhxIkTdO7cGYAtW7ZQrVq1Uq5I\nRERERECnHouIiMgtYsGCBf947bxu3brd9N2ARURERERuVRooFBERkVtCfHw8Fy5cuOE6cXFxJVSN\niIiIiIj90anHIiIiIiIiIiIiopuZiIiIiIiIiIiIiAYKRUREREREREREBA0UioiIiIiIiIiICBoo\nFBERERERERERETRQKCIiIiIiIiIiImigUERERERERERERNBAoYiIiIiIiIiIiKCBQhERERERERER\nEUEDhSIiIiIiIiIiIoIGCkVERERERERERAQNFIqIiIiIiIiIiAgaKBQRERERERERERE0UCgiIiIi\nIiIiIiJooFBERERERERERETQQKGIiIiIiIiIiIiggUIRERERERERERFBA4UiIiIiIiIiIiKCBgpF\nREREREREREQEDRSKiIiIiIiIiIgIt/BAYVRUFD169CjtMkrcDz/8QM+ePenSpQsvvvgi8fHxea63\ndu1aHn30UXr16sVTTz3F/v37S7hS61MGlAFlQBlQBm6cgUOHDhEcHMxjjz1G7969+e2330q4UutS\n/x27/6AMKAN5c9RcABiGQWhoKPPnzy/tUkqVMqAMKAPKgOTvlh0odESXLl1i/PjxzJw5k++++46a\nNWsyffr0XOsdPXqU9957j3nz5rF27VpGjBhBSEhIKVQsxU0ZEGVACpqBpKQknnnmGYYOHcqaNWt4\n/vnnGTNmTClULMVJ/RdlQPJz5MgRnn76adatW1fapUgpUQZEGZCCcCntAorDF198wYIFC3BycsLH\nx4d333032+PHjh0jLCyMxMREYmJiqF+/PuHh4bi7u/PRRx+xfv16XF1d8fHxYerUqdx22235Ls9P\nbGws48aN4/LlywA8+OCDjBo1CoCVK1cSGRlJRkYGFSpUYOLEidSrV4/Q0FBMJhNHjhzh0qVLPPDA\nA0yYMAFXV9dsr/3mm2+ya9eubMvc3NxYuXJltmU///wzDRs2pHbt2gAEBgbSq1cvJk+ejMlkyvbc\nN9980/J+7rvvPi5evIjZbMbNza0Qe952KAOZlAFlQBlQBgqaga1bt1KzZk0efPBBADp27EiNGjUK\nscdti/qfyVH7D8pAFkfOQF6Ui78tWbKE3r17U7169SLtS3ulDPxNGVAGHDUDUkiGnfvjjz+MgIAA\n4+zZs4ZhGMaCBQuMiRMnGjt27DAeeeQRwzAM45133jHWrFljGIZhmM1mo0ePHsb//d//GWfPnjWa\nNm1qpKSkGIZhGPPnzzfWr1+f7/IbmTVrljFx4kTDMAwjISHBGDVqlHH16lUjKirKCAoKMhITEw3D\nMIyffvrJ6Natm2EYhjFu3DjjscceM+Lj442UlBSjf//+RkRERJH3xSeffGKpwTAMIzU11fD19TWu\nXbuW73MyMjKMl19+2QgJCSnydkubMvA3ZUAZUAaUgYJm4NNPPzVCQkKM8ePHG48//rjx9NNPG7/9\n9luRt1ua1P+/OWL/DUMZuJ6jZiAvykXexo0bZ8ybN69YXsvWKQN5UwaUAUfKgBSe3c8o3L59O23a\ntOH2228HYNCgQUDmNQeyvPLKK2zdupXPPvuM48ePExMTQ2JiIlWrVqV+/fo8/vjjtGvXjnbt2tGq\nVSsyMjLyXH4jbdu2ZdiwYZw7d47WrVvz8ssvU65cOX744QdOnDjBU089ZVk3Li6OK1euAPD444/j\n6ekJQK9evdi4cSMDBgzI9toF/QtBRkZGnrU5OeV9hnliYiKhoaGcP3+eefPm3fD92TJl4G/KgDKg\nDCgDBc1AWloaW7ZsYdGiRTRq1IgNGzYwbNgwNm/ebHezStX/vzli/0EZuJ6jZiAvyoUoA6IMiBSe\n3Q8UOjs7ZzuNIjk5mTNnzmRb56WXXiI9PZ1u3brRvn17zp07h2EYODk5sXjxYvbv38/27dt5++23\nCQgIYMKECfkuz4+/vz8bN25k+/bt7Nixgz59+vDxxx+TkZFBr169eOWVV4DML28xMTF4e3tb6s+S\nVVNON9ru9W6//Xb27t1r+fnChQt4e3vj4eGRa92zZ88yfPhw6tWrx6JFiyhTpkyBtmGLlIG/KQOZ\nlAFlQBn45wzcdttt1K1bl0aNGgHQqVMnJkyYwKlTp6hXr16BtmUr1P+/OWL/QRm4nqNmIC/KhSgD\nogyIFJ7d38wkICCA7du3ExMTA8CyZct47733sq3z888/88ILL9C9e3dMJhN79+4lPT2dgwcP0qNH\nD+rVq8dzzz3HoEGDOHToUL7Lb2T69OnMnj2bTp068dprr3HXXXdx/PhxHnjgAf7zn/9Y6ouMjOTp\np5+2PG/dunWYzWZSUlL48ssv6dChQ5H3RZs2bdi7dy/Hjx+37IuOHTvmWu/KlSsMGDCAzp07M2PG\nDLseHABl4HrKgDKgDCgDBc1Au3btOHPmjOUup7t27cJkMtnlNcrU/785Yv9BGbieo2YgL8qFKAOi\nDIgUnt3PKPTz8+OVV15h6NChAFSpUoW3337b8uUIYPTo0bzwwgt4e3tTtmxZWrRowcmTJ+nTpw/d\nunXjiSeewMPDgzJlyjBhwgTq16+f5/IbefrppwkNDaVHjx64ubnh5+dn+fezzz7LkCFDMJlMeHl5\nMWvWLMtfNcqUKUNQUBBXr16lS5cuPPHEE0XeF5UqVWLq1Km8+OKLpKamUqtWLcuFWvfv38+ECRNY\nu3YtkZGRnDt3jvXr17N+/XrL8xcuXMjp06ct69kLZeBvyoAyoAwoAwXNQJUqVfj44495/fXXSUpK\nws3NjZkzZ+Lu7p5tPXug/v/NEfsPysD1HDUDeVEuCuZW6XdelIGCUQaUgVs5A1J4JsMwjNIuwlGF\nhoZy991388wzz5R2KdkMGzaMTz/9tLTLcAjKgCgDogw4NvVflAHJS0nnQv22PcqAKANSWux+RmFJ\nCgoKIiEhIc/HlixZgpeXVwlXVPwuXLhAUFBQaZdhs5QBUQZEGXBs6r8oA5IXe86F+l08lAFRBuRW\noRmFIiIiIiIiIiIiYv83MxEREREREREREZGbd8sPFPbq1YurV6/ecJ0//viDTp068fjjj3P69OkS\nqizThAkTLHebe+2119i2bVuJbj89PZ0333yTrl278vDDDxMZGZnvui1btqRXr16W/3311VclWGnR\nKQM3VtAMpKenM3nyZLp370737t159913secJycpFwZw7d462bdty6dKlUtl+cVG/b0zHgkzKgGMd\nC5SJgtFxoGSUdr91HMikDDjWcQCUi4K6VY4FUkCGGDNnzjReffXVUtl2hw4djH379pXKtg3DMBYv\nXmwMHTrUSE1NNa5cuWJ06dLF2Lt3b671jhw5YnTu3LkUKiwZysA/Z2DVqlVGcHCwkZaWZpjNZqN3\n797Gt99+WwoVlxxHzoVhGMaXX35pdOjQwfD19TX++uuvUq2lJDhyv3UsyKQM6FiQkyNnwjB0HChJ\npd1vHQcyKQM6DuTFkXNhGI53LBDDsJubmURFRTFt2jSqVq3KqVOnKFOmDO+88w716tW74fP8/PzY\nvn07P/zwA+vXr8fJyYkTJ07g6urKu+++y8GDB4mMjCQ9PZ3k5GTef/99Pv74Y/7zn//g7OxMnTp1\nmDhxIlWqVCE4OBhvb2+OHj1KYGAg33//PQ0aNGDHjh389ddfDBw4kL/++oudO3eSlJREeHg4fn5+\n/Prrr7z33nuYzWZiY2Np3bo1b7/9NjNmzCAmJoYxY8Ywbdo0pk+fTv/+/fn999+Jj49n0qRJAPz4\n44/MnDmTlStXsmfPHqZPn05SUhImk4mQkBA6dOiQ7T1fvXqV4ODgXPuia9eujBgxItuyDRs20Ldv\nX1xcXPD29uaRRx7hq6++wt/fP9t6v/zyC05OTgQHB3PlyhW6dOnCiBEjcHZ2Lko7i0QZKN0MpKen\nk5SUhNlsJiMjg9TUVNzd3YvSymKlXFgnFxcuXGDDhg18+umnPPLIIzfZpeKjfutYoAzoWJCTMqHj\ngPqt44Ay4NjHAVAuHO1YIFZW2iOVBbVjxw6jfv36xq5duwzDMIylS5cajz/++D8+L2vUe9WqVUaz\nZs2Mc+fOGYZhGGFhYcbYsWMNwzCMjz76yHj99dcNwzCML774wujXr5+RkJBgeWzIkCGGYRjGgAED\njPHjx1tee8CAAcbIkSMNwzCMX3/91fD19TU2btxoGIZhvPXWW8aECRMMwzCM0aNHGzt27DAMwzDi\n4+ONgIAAY//+/YZhZP8LwYABA4x169YZJ0+eNAICAoyUlBTDMAzjX//6l7FixQrjypUrRufOnY1T\np04ZhmEY58+fN9q1a2ecOXOm8Dv0f7p06WL88ssvlp9XrFhhvPDCC7nWW758ufHGG28YKSkpRlxc\nnNGvXz9jwYIFRd5uUSgDpZuBtLQ0Y8iQIUbz5s2Nxo0bW953aVMurJOLvPaVLVC/dSxQBnQsyEmZ\n0HFA/dZxQBlw7OOAYSgXjnYsEOuymxmFAPXr16d58+YAPPHEE4SFhXH58mV8fHwK9PwGDRpQrVo1\nAO69917Wr1+fa50ff/yR3r174+HhAcDAgQOZO3cuZrMZwLL9LA8//DAANWvWBKBt27YA1KpVi507\ndwLwzjvv8OOPPzJ37lyOHj1KcnIyiYmJ+dZZs2ZN6tevz6ZNm2jVqhXbt2/nrbfeYvfu3cTGxvLC\nCy9Y1jX9IsOwAAAgAElEQVSZTBw6dIjq1atblhXmLwRGHteTcHLKfenKvn37Wv7t5ubG4MGDiYiI\nYNCgQfm+D2tQBkovA7NmzaJixYps3bqVlJQUnn/+eT7//HOGDBmS7/soKcpF8efClqnfOhYoAzoW\n5KRM6Digfus4oAw49nEAlAtHOxaI9djVQGHOKe2GYRRqmnuZMmUs/zaZTHl+GOZclpGRQVpamuXn\nrA+ELG5ubtl+dnV1zfWa/fv3p379+rRt25Zu3bqxd+/ef7zga58+fVizZg1//fUXDz/8MJ6enqSn\np1OvXj1WrlxpWe/ChQtUrFgx23PLly/P2rVrb/j6WW6//XZiY2OzvV7Wh+P11qxZQ/369alfvz6Q\nuZ9cXEo+PspA6WVg/fr1TJgwATc3N9zc3Hj88cf57rvvbOJLgXJR/LmwZeq3jgXKgI4FOSkTOg6o\n3zoOKAOOfRwA5cLRjgViPXZ11+ODBw9y8OBBAJYvX07Tpk0pX758sW6jTZs2rF692jKCHxERQYsW\nLXL9ghdUXFwcv/32G2PGjKFz585cuHCBkydPkpGRAWR+mF3/wZLl4Ycf5sCBA6xYscLyl7vGjRtz\n4sQJdu3aBWTefalLly7ExMQUqTaAjh07smrVKtLS0rh69Sr/+c9/6NSpU671/vvf//LRRx9Zrs2w\nZMkSunfvXuTtFpUyUHoZuPfee1m3bh0AqampbNq0iUaNGhV5u8VJuSj+XNgy9VvHAmVAx4KclAkd\nB9RvHQeUAcc+DoBy4WjHArEeu5pRWLlyZcLDwzlz5gwVK1Zk2rRpxb6NJ598knPnztGnTx8yMjK4\n8847mT59epFfz9vbm2HDhvH4449ToUIFfHx8aNq0KSdOnKBVq1Z06tSJ0aNH8+abb2Z7npubG927\nd2fbtm2WC8hWrFiRjz76iGnTppGSkoJhGEybNo077rijyPUFBgZy8uRJevXqRWpqKv369eP+++8H\n4MMPPwTgX//6FyNHjiQsLIyePXuSlpZG165d6dOnT5G3W1TKQOllYPz48bz55pt07doVZ2dnWrVq\nxbPPPlvk7RYn5aL4c2HL1G8dC5QBHQtyUiZ0HChujtZvHQdyUwbs6zgAyoWjHQvEekzGP81ptRFR\nUVG88cYbfPPNN6VdipQSZUDyolw4FvVblAHJSZlwLOq3KAOSF+VCpPjY1YzCvMybN4+vv/46z8ee\neeYZHn300RKuSEqaMiB5US4ci/otyoDkpEw4FvVblAHJi3IhUnh2M6NQRERERERERERErMeubmYi\nIiIiIiIiIiIi1qGBQhEREREREREREdFAoYiIiIiIiIiIiNwCNzO5XnR0dGmXcMtq1qxZaZfwj9R/\n67GH/oMyYE3KgCgDogw4NvVflAFRBsReMiA355YaKATbDG50dLRN1pWfnPXa0wetre5ne8qAPfcf\nlIHioAxYhzJQcmxxP9tT/0EZsAZ7yoD6bx3KQMmx1f2sDJQcW9zP9tR/sP8MSNHp1GMRERERERER\nERGx7kDh3r17CQ4OBuDEiRMEBgYSFBTE5MmTycjIAGDFihX07t2bvn37snnzZgCSk5MJCQkhKCiI\nZ599lkuXLlmzTLEiZUCUAVEGHJv6L8qAKAOiDDg29V/EvlhtoPCzzz5jwoQJpKSkADB16lRGjRrF\n0qVLMQyDjRs3EhsbS0REBMuWLWP+/Pl88MEHmM1mIiMj8fX1ZenSpTz22GPMnj3bWmWKFSkDogyI\nMuDY1H9RBkQZEGXAsan/Yi0pKSmsXLnypl9n9erVTJ8+vRgqunVYbaCwVq1azJw50/LzgQMHuP/+\n+wFo164d27ZtY9++fTRp0gQ3NzfKlStHrVq1OHjwINHR0bRt29ay7vbt261VpliRMiDKgCgDjk39\nF2VAlAFRBhyb+i/WEhsbWywDhZKb1W5m0qVLF06fPm352TAMTCYTAJ6enly7do34+HjKlStnWcfT\n05P4+Phsy7PWLShbvcCmrdaVn+KotzQyYMv72ZZry6m4ai2NDFSfVr1YareKjaVdQCFcV+vXHb8u\n8svocyA7W64tJ3s9DhRX7dZgq3XlRxkofrZaV17s+buALe9nW64tJ2XAOmy5tpx0HCh+tlpXfgpc\n7yuvQHEP2vXpA++9l+/Dc+fO5c8//2TWrFkcPnyYy5cvAzBhwgT8/PxYvHgx33//PUlJSfj4+DBr\n1iwyMjIYP348Z8+eJTU1lYkTJwKZp8cPGTKES5cuERgYSL9+/di5cyczZszA2dmZmjVrEhYWxtdf\nf82qVavIyMjgxRdfpFWrVsX7nm1Eid312Mnp78mLCQkJlC9fHi8vLxISErItL1euXLblWesWlC3e\nRUh3N8pUEhmw1f1sTxmw5t2tSiIDbm5uxVZvcTKbzTZbW07WrFWfA7ZZW072fBwA28yAPfUflAFr\nsKcM2Pt3AVvdz8pAJmXANmvLSceB4mdP/Qfbv+vx8OHDOXz4MElJSbRs2ZKgoCCOHz/O+PHjWbJk\nCVeuXGHhwoU4OTnxzDPPsH//fvbv388dd9zBjBkzOH78OD/88APly5fHxcWF+fPnc+bMGYYNG0bf\nvn2ZOHEiS5cupVKlSoSHh/Pll1/i4uJC+fLlmTNnTmm/fasqsYHCe++9l6ioKAICAvjxxx9p2bIl\n/v7+hIeHk5KSgtls5siRI/j6+tK0aVO2bNmCv78/P/74o139Mkn+lAEpiQwcH3Xcum+iiOzpi4E1\nvxToc8Cxqf+iDIgyIMqAY1P/b1HvvXfD2X/WdPjwYXbs2MG6desAiIuLw8nJCVdXV1566SU8PDw4\nf/48aWlpHD16lHbt2gFQu3ZtBg0axOrVq7n33nsxmUxUqVKF5ORkLl26RExMDKNGjQIyb6zTunVr\n7rzzTurUqVMq77MkldhA4bhx45g4cSIffPABdevWpUuXLjg7OxMcHExQUBCGYTB69Gjc3d0JDAxk\n3LhxBAYG4urqyvvvv19SZYoVKQOiDIgy4NjUfympDNQOr229N1FEZrMZt5/saGb5dbWuaruq2F5b\nnwOiDDg29V+Ki5OTExkZGdStW5dHH32Unj178tdff7Fy5UoOHjzIhg0bWLlyJUlJSfTu3RvDMKhX\nrx779++nU6dOnDp1ivDwcB544AHL6fBZfHx8qFatGrNnz6ZcuXJs3LgRDw8Pzp07l21W7K3KZBiG\nUdpFFBdbnbFjq3XlJ6/ZRPZQvy3Xacu15WSv/QfbrtWWa8tJGbAOW64tJ2Wg+NlqXfmx9ww88dMT\npV1GLvZ8CYpVbVfZVf9ttVZbri0ne/8MsNVabbm2nJSB4merdeXH1jOQkpJC3759adiwIZcuXbJc\n73LkyJG0bt2a5557DrPZDGRenurJJ5+kS5cuvPrqq1y4cIH09HReffVV/vvf/3L06FHGjBlDSkoK\n3bp1Y9OmTfz88898/PHHGIaBp6cn06ZNY8uWLZZ1b2UlNqNQRERERKQk2OJlKGztP7BuxNavSyUi\nIuLu7s7atWvzfXzRokV5Ls85M7Vhw4bZXnPTpk0AtGnThjZt2mRbt3fv3kUt167c+nMmRURERERE\nRERE5B9poFBEREREREREREQ0UCgiIiIiIiIiIiIaKBQRERERERERERF0MxMRERERERERuYXUDq9d\n2iXkYjabcfvJ7Z9XtBE5613VdlUpViMlSTMKRURERERERERERDMKRUREREREROTWcXzU8dIuIZfo\n6GiaNWtW2mUUWM56o6OjS7Gaglm8eDEDBgwo0LqRkZFcvHiRkJCQQm1j/fr1+Pv7U7Vq1Twfnzlz\nJpUrVyYwMLBQr/tPgoODmTJlCnv37sXb25uOHTsW6+tfTzMKRURERERERETErs2ZM8fq21i0aBHx\n8fFW305+evfubdVBQtCMQhERERERERERKaJXvn+Flb+vLNbX7HNvH97r/F6+jx87dozx48fj4uJC\nRkYGrVu3Ji4ujilTpuDv78/Ro0cZM2YMKSkpdOvWjU2bNrF7927efvttypcvj7OzM40bNwYgIiKC\nb775BpPJRPfu3Rk4cCChoaG4ublx5swZYmJieOedd4iNjeWPP/5g3LhxLF26FDe3vK85uWHDBtat\nW0dycjITJkzA39+fxYsX8/3335OUlISPjw+zZs3izJkz2d7D+++/z+23387777/P7t27ycjIYNCg\nQXTr1s3y2lkzFuvWrctnn32Gq6srp0+fpnv37owYMYJz584xceJEUlJScHd354033uD2228v1L7X\njEIREREREREREbEb27Ztw9/fnwULFhASEkLnzp3x9vZmypQp+T7n9ddf5/3332fhwoXUqFEDgD//\n/JNvv/2WpUuXsmTJEjZs2MDRo0cBqF69OvPnzyc4OJjly5fTvn177rnnHt599918BwkB7rjjDhYt\nWsRbb73F5MmTycjI4MqVKyxcuJCVK1eSnp7O/v37c72Ha9eusWXLFk6fPk1kZCSLFi1i7ty5XL16\nNc/tnD17lpkzZ7J8+XLmzZsHwLvvvktwcDARERE888wzTJ8+vdD7VjMKRURERERERESkSN7r/N4N\nZ/9Zw5NPPslnn33G0KFDKVeuHKNHj85zPcMwLP++ePEiderUAaBp06acPHmSw4cPc/bsWQYNGgRA\nXFwcJ06cAOCee+4BoFq1auzZs6fAtbVo0QKAu+++m9jYWJycnHB1deWll17Cw8OD8+fPk5aWlud7\nOHz4MAcOHCA4OBiAtLQ0zpw5k+d2fH19cXFxwcXFhTJlygBw+PBhPvnkE+bNm4dhGLi4FH7YTwOF\nIiIiIiIiIiJiNzZu3EizZs0YOXIk33zzjWVgDMDd3Z3Y2FgADhw4YHlO1apVOXLkCPXq1WP//v14\ne3tTt25d7rrrLubNm4fJZGLhwoX4+fnx3XffYTKZcm3XZDJlG3zMy759++jZsyeHDh2ievXqHDx4\nkA0bNrBy5UqSkpLo3bs3hmHk+R46depEQEAAb7zxBhkZGcyePZuaNWvmuZ286qtbty5DhgyhadOm\nHDlyhF27dhV4n2bRQKGIiIiIiIiIiNiN++67j3HjxjFnzhwyMjIYP348p0+fZsyYMUyaNInIyEgC\nAwNp0KABnp6eAISFhTF27Fi8vLzw9PTE29ub+vXr06pVKwIDAzGbzTe8ozFAkyZNGDt2LJ9//jkV\nKlTIc53Tp08zcOBAzGYzYWFh3HnnnZQtW5annnoKgCpVqhATE0Pjxo1zvYd7772XnTt3EhQURGJi\nIp06dcLLy6vA+2XcuHFMmTKFlJQUkpOTee211wqxVzNpoFBEREREREREROxGrVq1iIyMzLYsIiLC\n8u/Fixfneo6/vz+rVq3KtXzo0KEMHTo027J33nnH8u927drRrl07AEaPHp3vac4AISEheS5ftGhR\nnstzvgeA8ePH51qW9d6uf/2AgADLv7du3QpAzZo1mT9/fr71FYQGCkVERERERERERApo5MiRxMXF\nZVvm5eXFnDlzSqmi4qOBQrml1A6vXdol5MlsNuP2U/53RbIlOWtd1Tb3X1xEREREREREHNWsWbNK\nuwSrcSrtAkRERERERERERKT0leiMwtTUVEJDQzlz5gxOTk688cYbuLi4EBoaislk4u6772by5Mk4\nOTmxYsUKli1bhouLCyNGjKBDhw4lWapYibUzcHzUceu/iSKIjo6mWbNmpV1GgeSsNTo6ulhfX58D\nogw4NvVflAFRBkQZcGzqv4htK9GBwi1btpCWlsayZcvYunUr4eHhpKamMmrUKAICApg0aRIbN26k\ncePGREREsGrVKlJSUggKCuKBBx7Azc0+Tt2U/CkDogyIMuDY1H9RBkQZEGXAsan/IratRE89rlOn\nDunp6WRkZBAfH4+LiwsHDhzg/vvvBzLvJLNt2zb27dtHkyZNcHNzo1y5ctSqVYuDBw+WZKliJcqA\nKAOiDDg29V+UAVEGRBlwbOq/iG0r0RmFHh4enDlzhm7dunH58mXmzp3Lrl27MJlMAHh6enLt2jXi\n4+MpV66c5Xmenp7Ex8cXaBvFfZpkcbHVuvJjrXqtnQFb3s+2XFtO1qxVGbAPyoB12HJtOdnrccCa\ntd8sW60rP8pA8bPVuvKi44B12HJtOSkD1mHLteWk40Dxs9W68mNv9S5evJgBAwYUaN3IyEguXrxI\nSEhIobaxfv16/P39qVq16j+ue+TIEaZMmUJERASjR4/m3XffzXNG7JUrV/jpp5/o2bMnn376KS1b\ntuTPP//k6NGjPPXUU7z00kusWLHiH7eXkpJCt27d2LRpU6HeU04lOlC4cOFC2rRpw8svv8y5c+d4\n+umnSU1NtTyekJBA+fLl8fLyIiEhIdvy6z8gbsQWrwNnT9enA+teo87aGbDV/WxPGbD2NQqVAdun\nDFiHMpBJ3wXsgzJQ/OwpAzoOWIcy8DdlwPbpOFD87Kn/YP3PAWuYM2dOgQcKi2rRokVMmTKlQAOF\n15sxY0a+jx06dIhNmzbRs2dPhg0bBsCff/55U3XejBIdKCxfvjyurq4AeHt7k5aWxr333ktUVBQB\nAQH8+OOPtGzZEn9/f8LDw0lJScFsNnPkyBF8fX1LslSxEmVAlAFRBhyb+i/KgCgDogw4NvX/1vP5\n1wfYuvdMsb7mA43uYEjPBvk+fuzYMcaPH4+LiwsZGRm0bt2auLg4pkyZgr+/P0ePHmXMmDHZZtnt\n3r2bt99+m/Lly+Ps7Ezjxo0BiIiI4JtvvsFkMtG9e3cGDhxIaGgobm5unDlzhpiYGN555x1iY2P5\n448/GDduHEuXLs1zdmBMTAxjxozBMAyqVKliWf7QQw+xbt06tmzZwmeffYaLiwu33XYbM2bMYO7c\nuRw8eJDly5fzyy+/0L179zzf886dO5kxYwbOzs7UrFmTsLAwzGYzY8aM4erVq9SqVesm93qmEh0o\nHDRoEK+++ipBQUGkpqYyevRo7rvvPiZOnMgHH3xA3bp16dKlC87OzgQHBxMUFIRhGIwePRp3d/eS\nLFWsRBkQZUCUAcem/osyIMqAKAOOTf2X4rBt2zb8/f155ZVX2L17N5UqVSIyMpIpU6awevXqPJ/z\n+uuv89FHH1GnTh0mT54MZM7c+/bbb1m6dCkAgwcPpk2bNgBUr16dsLAwVqxYwfLlywkLC+Oee+5h\nypQp+d5UZ+7cufTo0YO+ffvy7bffEhkZme3xb775hmeeeYauXbuyZs0a4uPjGT58OMuWLaNfv378\n8ssveb6uYRhMnDiRpUuXUqlSJcLDw/nyyy+5du0avr6+jB49mr179xIVFVWk/Xm9Eh0o9PT05MMP\nP8y1fPHixbmW9e3bl759+5ZEWVKClAFRBkQZcGwl0f/a4bWLUppVmc1m3H6yn7s05qx3VdtVxfba\n+gwQZUCUAcem/t96hvRscMPZf9bw5JNP8tlnnzF06FDKlSvH6NGj81zPMAzLvy9evEidOnUAaNq0\nKSdPnuTw4cOcPXuWQYMGARAXF8eJEycAuOeeewCoVq0ae/bsKVBdx48ft2S2adOmuQYKx48fzyef\nfMLixYupW7cunTp1KtDrXrp0iZiYGEaNGgVAcnIyrVu35tKlSzz44IMANGrUCBeXmx/mK9Bdjy9d\nunTTGxL7pgw4NvVflAFRBkQZEGXAsan/ogyILdm4cSPNmjXj3//+N127dmXevHmWQUF3d3diY2MB\nOHDggOU5VatW5ciRIwDs378fgLp163LXXXexaNEiIiIi6N27N35+fgCWG+xcz2QyZRt8zKlevXqW\nWYFZ27je8uXLCQkJsQyMr1+/HicnJzIyMm74fn18fKhWrRqzZ88mIiKC4cOH07JlS+rVq8evv/4K\nwO+//05aWtoNX6cgCjTU2L9/f9atW3fTGxP7pQw4NvVflAGxpwwcH3W8tEvI5Va4gLk9ZUCsQxlw\nbOq/KANiS+677z7GjRvHnDlzyMjIYPz48Zw+fZoxY8YwadIkIiMjCQwMpEGDBnh6egIQFhbG2LFj\n8fLywtPTE29vb+rXr0+rVq0IDAzEbDb/4x2NmzRpwtixY/n888+pUKFCrsdHjBjBK6+8wrfffkuN\nGjVyPe7v789zzz2Hp6cnHh4etG/fHrPZzOHDh1m4cGG+23VycuK1115j2LBhGIaBp6cn06ZNo2nT\npowdO5bAwEDq1q1ruf7nzSjQQGH9+vVZs2YN/v7+lClTxrK8evXqN12A2AdlwLGp/6IMiDIgyoAo\nA45N/RdlQGxJrVq1cp3WGxERYfl3Xqey+/v7s2pV7supDB06lKFDh2Zb9s4771j+3a5dO9q1awfA\n6NGj8z3NGaBixYrMnz8/1/JNmzYBmTc1eeihh3I9fqNB+BUrVgDQpk0by/UTr5fXqfw3o0ADhXv3\n7mXv3r3ZlplMJjZu3FisxYjtUgYcm/ovyoAoA6IMiDLg2NR/UQZE/jZy5Eji4uKyLfPy8mLOnDml\nVFHxKdBAYdbIpzguZcCxqf+iDIgyIMqAKAOOTf0XZUDkb7NmzSrtEqymQAOFly5dIiwsjO3bt5Oe\nnk7Lli2ZMmUKlStXtnZ9YiOUAcem/osyIMqAKAOiDDg2e+p/7fDapV1CnnLeUd6W5ax1VdtVdpUB\nESm6At31eNKkSTRs2JCNGzeyadMmGjVqxGuvvWbt2sSGKAOOTf0XZUCUAVEGRBlwbOq/KAMijqFA\nMwpPnTqVbVrls88+y1dffWW1osT2KAOOTf0XZUCUAVEGRBlwbPbU/+Ojjpd2CXnKeUd5W5az1ujo\naLvKgIgUXYFmFJpMJs6dO2f5+ezZs7i4FGiMUW4RyoBjU/9FGRBlQJQBUQYcm/ovyoCIYyjQb/W/\n/vUv+vXrR6NGjTAMg7179/LGG29YuzaxIcqAY1P/RRkQZUCUAVEGHJv6L8qAiGMo0EDh7bffzpo1\na9i3bx8ZGRm8/vrrVKpUydq1iQ1RBhyb+i/KgCgDogyIMuDY1H9RBkQcQ4EGCkePHs26deto3769\nlcsRW6UMODb1X5QBUQZEGRBlwLGp/6IMiDiGAg0U3nXXXcyaNYtGjRpRpkwZy/IWLVpYrTCxLcqA\nY1P/RRkQZUCUAVEGHJv6L8qAiGMo0EDhlStXiIqKIioqyrLMZDKxaNEiqxUmtkUZcGzqvygDogyI\nMiDKgGNT/0UZEHEMBRoo7N69O4GBgdauRWyYMuDY1H9RBkQZEGVAlAHHpv6LMiDiGJwKstKSJUus\nXYfYOGXAsan/ogyIMiDKgCgDjk39F2VAxDEUaEZhtWrVGDhwII0aNcLd3d2yfOTIkVYrTGyLMuDY\n1H9RBkQZEGVAlAHHpv6LMiDiGAo0UNi4cWNr1yE2ThlwbOq/KAOiDIgyIMqAY1P/RRkQcQwFGigc\nOXIkiYmJnDx5El9fX5KTk/Hw8CjSBj/55BM2bdpEamoqgYGB3H///YSGhmIymbj77ruZPHkyTk5O\nrFixgmXLluHi4sKIESPo0KFDkbYnxUMZcGzF2X9QBuyRMiA6DogyIMqAY9N3AdFngIhjKNA1Crdv\n306vXr14/vnnuXjxIg899BA///xzoTcWFRXFL7/8QmRkJBEREZw/f56pU6cyatQoli5dimEYbNy4\nkdjYWCIiIli2bBnz58/ngw8+wGw2F3p7UnyUAcdWXP0HZcBeKQOi44AoA6IMODZ9FxB9Bog4hgIN\nFH7wwQcsXbqU8uXLc9ttt7F48WKmTZtW6I39/PPP+Pr68sILLzB8+HDat2/PgQMHuP/++wFo164d\n27ZtY9++fTRp0gQ3NzfKlStHrVq1OHjwYKG3J8VHGXBsxdV/UAbslTIgOg6IMiDKgGPTdwHRZ4CI\nYyjQqccZGRlUqVLF8vNdd91VpI1dvnyZs2fPMnfuXE6fPs2IESMwDAOTyQSAp6cn165dIz4+nnLl\nylme5+npSXx8fIG2ER0dXaTarM1W68pPznrtJQO2vJ9tubacrNV/UAbshTJgHbZcW072ehzIq3Zb\nYat15UcZKH62Wlde8qrVXjJgy/vZlmvLSd8FrMOWa8tJx4HiZ6t15cfe6pXiUeC7Hm/evBmTycTV\nq1dZsmQJ1atXL/TGKlSoQN26dXFzc6Nu3bq4u7tz/vx5y+MJCQmUL18eLy8vEhISsi2//gPiRp74\n6YlC12VtZrMZNze30i6jwHLWu6rtKrvJQLNmzQpdU0mIjo622dpyyllrdHR0sfUflAF7oAxYhzKQ\nqSS+C9jifran/oMyYA32lIG8+g/2898Etrqf7T0D+i5w85SBTDoO2If8jgVy6yvQQGFYWBhvvfUW\n586d4+GHHyYgIICwsLBCb6xZs2YsWrSIwYMHExMTQ1JSEq1atSIqKoqAgAB+/PFHWrZsib+/P+Hh\n4aSkpGA2mzly5Ai+vr6F3p4UH2XAsRVX/0EZsFf2lIHa4bWLVJe1mc1m3H6yjz8a5ax1VdtVOg6I\nMiDKgIOzp+8CYh36DBBxDAUaKKxUqRIffPBBno9NnDiRN954o0Ab69ChA7t27eLJJ5/EMAwmTZpE\njRo1mDhxIh988AF169alS5cuODs7ExwcTFBQEIZhMHr0aNzd3Qu0jeOjjhdovZJ0K/zlwJ4yIMWv\nuPoPyoC9UgZExwFRBkQZcGz6LiD6DBBxDAUaKLyR3377rVDrjx07NteyxYsX51rWt29f+vbtW+S6\npOQoA46tsP0HZeBWY2sZsMU/GIF9/dGosKea6DggyoAoA47N1r4LSMnTZ4DIreOmBwpFRERERERs\nhS5BcfPyugSFiIg4BqfSLkBERERERERERERKn2YUioiIiIjILUOXoLh5utupiIjjuukZhYZhFEcd\nYseUAcem/osyIMqAKAOiDDg29V+UAZFbx00PFLZu3bo46hA7pgw4NvVflAFRBkQZEGXAsan/ogyI\n3DoKdOrx7t27+fe//01cXFy25YsWLcrzbkVy61EGHJv6L8qAKAOiDIgy4NjUf1EGRBxDgQYKQ0ND\nGfc1OjAAACAASURBVDlyJNWrV7d2PWKjlAHHpv6LMiDKgCgDogw4NvVflAERx1CggcKqVavy2GOP\nWbsWsWHKgGNT/23Ppt2n+OqnIwx+pAGNfKtYfXvKgCgDogyIMuDY1H9RBkQcQ4EGCoODgxkzZgwt\nW7bExeXvp+hDwnEoA45N/bcdyeY0Plm9nw27TgLw1sKdvDuyDXWqe1t1u8qAKAOiDIgy4NjUf1EG\nRBxDgQYKly5dCkB0dHS25fpAcBzKgGNT/23DyfNXeTdiNyfPX6NeDW/aN63J/K9+Y8pnO3jvxbbc\n5uNhtW0rA6IMiDIgyoBjU/9FGRBxDAUaKIyNjWXdunXWrkVsmDLg2NT/0rdh50nmfrmPFHM6PR6o\nw5BHG+Dq4gwYzP/qAFM+28G0kW3w8nCzyvaVAVEGRBkQZcCxqf+iDIg4BqeCrNS8eXM2b95MWlqa\ntesRG6UMODb1v/Qkp6QxI3IPHy7/BWcnE6FPt+C53v7/GySEXu3q8Wjbupy6cI23Fu4kNS3dKnUo\nA6IMiDIgyoBjU/9FGRBxDAWaUbh582ZWrlyZbZnJZOKPP/6wSlFie5QBx6b+l44T56/y7qJdnLoQ\nz101KzAuuDnVKnlmW8dkMjHk0fu4GJfEtn3nCI/8hZf7N8PJyVSstSgDogyIMiDKgGNT/0UZEHEM\nBRoo/Pnnn61dh9g4ZcCxqf8lyzCM/51qvB9zajo929ZlcI97LbMIc3J2MvFSUDMuX93Gj7+eoXKF\nsgzu2aBYa1IGRBkQZUCUAcem/osyIOIYCjRQOGvWrDyXjxw5sliLEdulDDg29b/kJKWkMWfVXjZH\nn8azjAtj+regVcPq//g8d1dnJgwJYOzMn1j9w59U8SlLjzZ1i60uZUCUAVEGRBlwbOq/KAMijqFA\n1yi8XmpqKps2beKvv/6yRj1iB5QBx6b+W8/xc1d5KXwLm6NPc3fNCoS/1L5Ag4RZynu6MeXZllTw\ncufTNfvZvv+sVepUBkQZEGVAlAHHpv6LMlD6DMPg0IlL/PTLGY6djcOcap1rlYvjKdCMwpx/IXjh\nhRcYMmSIVQoS26QMODb137oMw+D7qJN8+uU+zGkZPNquLoMeaYCrS6H/lkO1Sp5MGhrAq7O3Mn1x\nNG+NKEP92hVvukZlQJQBsbcMXL6azDdbj/HkQ3dT1r1AX3nlH9hbBqR4qf+iDNiGK9dS2Bx9ivU7\nT3DqQrxluZMJqlb0pGbVctSs6vW//y9Hjdu88CjjWooVi70p0remhIQEzp61zkwVsQ/KgGO7Vftv\nTk0nISkVn/9v787jo67vfY+/Zkkmk8lM9n0jCXtCAEEWEazaCrZHxdqetvTC6bWtPbQ9Hnrvw6NX\nLXrac+q19np6a2vt5umt1no8padq61rUgoggAQQCAUJWsu97JsnM7/4xYSBsBsgyM3k/H488ZjLz\nm5lP5vvJb+b3+X0XV8SEvWZv/yBP/v4Af913Eoc9jHvWL2ZZQeoVPeeMzFju3XA13316F9/51S4e\nu3sl6YlRYxSxT6jmgIyeckACPQeKShp54S/HyEiK4vpFmZMdTkgK9ByQ8aX2F+XAxPF4vOw92sib\nu6vYXVyPx2tgtZhZuSCdmVmx1DZ1U9XQRXVDF7sP17P78MjHJ8TYyRouHJ5ZRHRGhk/OHyQBbVSF\nwhtuuAGTybeCpmEYdHZ28uUvf/myX7SlpYVPf/rTPP3001itVu677z5MJhMzZszgoYcewmw288IL\nL/D8889jtVrZuHEj119//WW/nlw55cDUNhXav889xAM/3cHx6nZS4x3Mm57AvLx45k1PID7aPi6v\nWV7bwaO/+YCaph5mZcXyT+sXkxQXOSbPvXhOMl+/Yz4//s/9PPyLnTz2D6uIcdou+/mmQg7IxSkH\nJNhyIDHGt++uaer+iC1ltIItB2Rsqf1FOTDxapu7+cvuKrZ+UE1rZz8A01JdfGJpFh+7KhOX49xC\nX0e3m+rhomF1YzfV9V1UNXSx92gje482jtg2xmk7XUBMiiIzxXc9Juryjxsk+I2qUPj000/z7rvv\n0t7eDoDL5cLlcl3WCw4ODrJ582YiInw9dh555BE2bdrE0qVL2bx5M1u3bmXBggU888wzbNmyBbfb\nzbp161ixYgXh4ap2TxblwNQW6u3v8Xh57Nk9HK9uJzvFSXN7H2/squSNXZUApCc6mDc9kcK8BAqm\nxxPrvLIeh4Zh8Pr7lfz8jwcZHPKy9ro8Nnxy7mUNNb6Y1cuyaWrv5T/ePMZ3fvU+39u4gojLHH4X\n6jkgH005IMGWA6mJDgDqmnouK0Y5V7DlgIwttb8oByZG/8AQ7x2o483dlRw64ZsDMjLCys3Lp/GJ\npVlMz4jxF2zPJzrKRnSUjYK8hBG3d/cNcrKxy184PNno64V4oLSZA6XNI7Z1RoZxY2EUixaN/d8n\ngW9UR4z/9m//Rm1tLXl5eZhMJmpqagBYu3btJb/go48+yuc//3l+/vOfA1BcXMySJUsAWLVqFTt2\n7MBsNrNw4ULCw8MJDw8nKyuLkpISCgsLL/n1ZGwoB6a2UG5/wzD42R8P8sHhBhbOTGTzV5ZhMpko\nq2nnYGkzB0+0UFzWzGs7K3htZwUAmclRzMtLoHB6IgV58URfwhm33v5BfvKfH7Jtfw1R9jDu23A1\nS/JTxvzvOuWLq2fT1NbHW3uq+f6ze3jgS0uwWC69IBnKOSCjoxyQYMuBhGg74VYztc2h16Owu2+Q\nN96v4K091aQnRfGlT+WTmuAY99cNthyQsaX2F+XA+DEMg9KT7by5q4q/7jtJb/8QAPPyEvjE0iyW\nz0slIvzK5tuNsocxOzuO2dkj5y/vdw9xsqn7dC/Ehi5qm3vwGlf0chLERpVpR48e5bXXXrviF/vD\nH/5AXFwcK1eu9O8QDMPwV8MdDgddXV10d3fjdDr9j3M4HHR3h96XvGCiHJjaQrn9/+udUl59r4Jp\nqS7u+7ursQ4X0WZkxjIjM5ZPXz8Dj8dL6cl2DpQ2c7C0mcMVrbzyXgWvvFcB+Lr/nxqqXJCXcMG5\nPspqfEONa5t7mJ0dyz3rF5MUOzZDjS/EZDLxD3+7gNbOfj443MBP/3CAb3xm/kXPQp5PKOeAjI5y\nQIItB8xmEykJDmqbe0Y8fzCrberm5e1l/OWDKvoHPJjNJirru9hd3MBtq3L524/PHNcJ64MtB2Rs\nqf1FOTD2OnsGeKeomjd3V1FR1wlAfHQEf3NtLh+/OmtCTgJF2KxMz4hhekbMiNuLiorG/bUlMI2q\nUJiXl0djYyNJSUlX9GJbtmzBZDKxc+dOjhw5wr333ktra6v//p6eHlwuF1FRUfT09Iy4/cwdxMUE\najIHalwXcna8wZIDgfw+B3JsZwvW9j9f7BdzqLKX3+9oxRVp4dNLHRwpPnDR7XNjIHexjU8tTKG2\ndYCKBjflDW6qGzupqOvk5e1lAKTEhjEtyUZOso2sJBsRYSb2lPbw2vPv4PHCijlR3DA/kuqyI1SP\nOtors2Z+GPVNYbz+fiWDfW2syr/4MJGpkgMTLZBjO5tyYOwFalwXEgo5EGkdpLd/iG3vfUBUhOWK\n4h4Ll5MDhmFQ0ehmZ0k3x2p881O5Ii2snBvNwjwHZfX9vLGvgy1vl/L6zjJuXBDN/JxIzFdYGD1f\nrMGSA4H8vxbIsZ0tFPYBgSiQYzulb8ALKAfGQ1FREV6vQVmDm30neig52YfH61uxeE6mnYW5kUxP\njcBs7qW2soTaysmPV6aeURUK+/v7WbNmDTNnzhwxH8BvfvObS3qx3/72t/7r69ev5+GHH+axxx5j\n165dLF26lG3btrFs2TIKCwv54Q9/iNvtZmBggBMnTjBz5sxRvcaiABxEX1RUFJBxXcjZ8RYVFQVN\nDgTq+xxMORDM7Q+jz4GSylZefGEHdpuVf9l4LTlp0Zf0t5xpcMjD0co2DpY2c+BEMyUVbdS3dfP+\n0W7MJkiOc1DX0oMzMoxvfeEqrp47fkONL2b2nD7ueWI7b33Yyfy507lh8flXAZ0qOTDRtB/wmao5\nEEztD6GTAwdqiyk5WUpiWh5zc+IvKc6xdqk5MDjk4a97a3hp+wnKa329TGZlxXLbqjyWF6b6e8Cv\nBD73N0P81zsn+P1bx3nx/TYO1xjcdfu8c4aXXW6spw4UgyUHAvV/LZj2A6GyDwg0gZ4DFXWdvLTt\nBO/srSUp2spT99/sv085cOX+8tddNPRFs3VPFU1tfQBkJju5aXhhkitZeHA8XOizQELfqAqFX/va\n18YtgHvvvZdvf/vbPP744+Tm5rJ69WosFgvr169n3bp1GIbBt771LWy2wPqnmWqUA1NbqLV/U1sf\n//rvu/F4vDz435deUZEQIMxqoSAvgYK8BL4AuAc9HK1s9Q9VPlbVTnZSOA/d9TESY8dnBeXRiI+2\n8/BXlvFPP36XH/3HPuJcNhbMHN0Z4VDLAbl0ygEJxhw4NWSrtqln0guFo9Xe5ebVnRW88l457V1u\nzGYT185P47br8i5Y+IsIt/KFm2Zx49WZ/L8/HWbb/hru+dF2PrYogy99ai7x0WPz2ROMOSBjR+0f\nmrxegz0lDby07QQfHvctaJGa4GDFnHMX7wvGHBgY9BBmNU/K9BMtHX0Ul7VwqKyF4rIWquq7gHrs\nNgs3Lc3mE0uzmJUVGxJTY0hoMRmGETJTVAbqGZpAjetCznfmIBjiD+Q4Azm2swVr+8P5Y21s7eVo\nZRsr5qdhNpvodw9x70/epaymg6+uLeDWlXnjHpfHa7B/396AeR8Pnmhm8892EmY18+g3z+1NGWo5\nECgCObazKQfGXqDGdSGhkgMHSpt44Kfv8dkbZ7Dhk3MDJq7zOd2T5ySDQ14cEVZWL5vGp67NueT5\nbIvLWvj5Hw9SVtNBRLiFz944k7XX5REeNrrh16HS/oEmkGM7m3JgfARSbP3uIbbuqebl7SeoGV4d\nvnB6AretymPxnGT2nfXdNZBi/yinYnUPevi7h19j1VUZfP2O+eP6moZhUNfSw+EzCoP1Lb3++23h\nFtLjrNxy3RxWzE/HbruyhUkmQjDvB+TKBH52iohcpj1HGvjBb4vo6Rvk9pPT+dKn5vJvz++lrKaD\n1cuyueXa3AmJw2IOrLOE8/IS+B9fuIrvP7uHh3/xPj+4e9Wk9nQUERkvaQlRANQ293zElpPD6zUo\nKmngxbN68ty2Mpcbrs667APJ/Nx4Ht90HVs/qOKZV47wzKtHeH1XJV++JZ/l81LVe0VkCmtq6+PP\nO8p47f1KevoGsVrM3Hh1JretyrviUTaBJtxqJjnewavvVXDdwgzyc8euZ7nXa1DV0EXxcFGwuKyZ\n1k63/36HPYwlc1PIz40jPzeevIwYPty/j0WLsscsBpHxokKhiISkN3ZV8uP/3I/VYiYx1s5/vVNK\naXU7B080U5AXz9duL5zSB0orF6bT3NHH0y8X8/Avd/LoN1cSZR+/lTJFRCZDnCuC8DALdU2BVSj8\nqJ485jE4wWQxm7hpaTYrCtN4/s2jvLy9jEf+3wcUTk/gK7cVhFxBQEQu7mhlKy9tK+PdA7V4vQbR\nUeF84aZZ3Lx8GrGuc4cZhwKTycTX7yjknie28+SWD/m//+Nj/vldL9WQx0tZTccZhcEWuvsG/ffH\nOm1cOz+Ngtx45ubGk53iGpN9uchkUKFQREJSU1sfmclOvvWFq4iyh3HPj7Zz8EQzyXGR3LfhasKs\nl/clIZSsvS6PpvY+Xt5exvf+fTf/fNcywqyTvyqoiMhYMZtNpCU4qG3uxjCMST9BdKonz+vvV9I9\nQT15HPYwvnxrAauXZfOrl4rZc6SBTY+/w5rl0/jimjm4HOEf/SQiEpQ8Hi87D9Xx4l9PUFLZBsC0\nVBe3rcpl1cKMUU9HEMxmZcexZtk0Xt1ZwbOvHmH9zXOwjKJY6B70cKyqzT+UuKSilf4Bj//+5LhI\nluSnUJAbT35uPKkJjkn/jBEZKyoUSkjq7R/k0IkWrp6brB32FPXFNbP54prZ/t8f+uoytrx1nHWr\nZxMdpYmwwXeW9cu3FtDc3sfOg3X88Pl9/M91mndEREJLaoKDirpO2rrcxE1grxnDMOjsGaChtZeG\n1l5e2dHC4effnLSePBlJTh76yjL2HGngly8e4pX3Kti2r4Z1q2dz8zXTLruXjYhcnHvQQ1V9J9UN\nXdTV9BGX2kFSbCSOcRzJ0d03yBvvV/KnHWX+1XUXz0lm7ao8CmckTLnjow2fnMOu4jq2vF3K7sP1\nfOGm2SwrSDnnBLnHa7C7uI4/vVvO4fJWhjxe/32ZyU5/UTA/N56EGE3bI6FLhUIJSW/squJXLx3i\nf3/j2jGdi0KC1/SMGO7dcPVkhxFwLGYT//OLi3jwpzvYtq+GxBg781InOyoRkbGT5l/5uHtMC4WG\nYdDdN0hDay+Nw8XAxtZe6lt7aWzzXT+z9wkERk+exXOSmT8jkT/vKOd3b5Tw8z8e5NWdFdy1toAF\nM5MmJSaRUNHe5aa8toPy2g7Kajopq+2gprEL7xnLhz6/7R0AouxhJMVFknzGT1JcJMmxvsvLmaO0\ntrmbl7eX8ZfdVfQPeLCFW/jkNdO4dVUe6YlRY/RXBp+oyHAe33Qdv3vjKG/uquT7z+whyh7GtQvS\nuX5RBrlp0bxVVM0f/3qCuuE5badnRJOfm0B+bjxzc+LU0UCmFBUKJSSdmmutuqFLhUKRj2ALs/Dg\nnUu598fb2fJ2KX2LY9CCZiISKtISTy9oUpCXcEmP7e0f9PcIPFUM9P/e1ktv/9B5H+eIsJKWEEVS\nnJ3kOAdJcXaGuhu4fc3ygOjJE2Y1s/a6PD52VQbPvnaEN3ZV8u2f7WRpfgp33po/2eGJBDyv17fC\nbVnNqaJgB+W1nbR29o/Yzm6zMCs7jtz0aLJTnBw/UUlYZKx/H3KysZuymo7zvobLEe4vHqYMXybF\nni4o2oZPNhiGwaETLby47QS7D9djGJAQHcHnPzGLm5Zl44zU9AIA8dF2vvnZBay9Lo/X369k276T\nvLazgtd2VmAxm/B4DawWMzctzWbtdXlkJjsnO2SRSaNCoYSk1OHeA/UtgTV5uUigio6y8fBXl3PP\nj7bzl/0dfO1zhiZgFpGQkJvum/vvjV2V3Hh11kVXoi8ua+Hld8uob+mhoaV3xET1Z7LbLL4CYGyk\nvxiY7C8KRp53caiiovaAKBKeKcZp45ufXcDNy6fxixcPsau4nqKSRpbOcrBggXdU83iJhLr+gSEq\n6zopq+2kfLgwWFHXeU6P4YToCK6em0xuWjQ56dHkpkWTHBc54vtUkq2VRYvm+383DIP2bveIExGN\nbX00tPTQ2NZLeW0nx6vbzxtXrNNGUlwk/e4hKuu7AJiZFcNtq/K4pjBN0wlcQEaSky/fWsCX/iaf\nA8ebeLuomtKT7VwzL41PrcgJ2YVdRC6FCoUSkk4VCutUKBQZtZR4B4/dvZK/7tyvIqGIhIzpGTFc\nOz+Ndz+s5dX3yvmba3PP2aats59//1MxbxedBCA8zEJynJ3Z0+JIirUPDwt0+IuCzsiwgCv6XYm8\njBge+foK3v2wlqdfLmbH4S5uKm3mqlkaiixTz4mT7ew92khFrW/ocG1T94ihw2aziaxkJzlpLnLT\no8lJjWZamuuyhqaaTCZinRHEOiOYlR13zv1er0FbVz+NrX00tPbQ0Nbrv97Y2kdpdTuGYbBifhpr\nV+Uxe9q5zyHnZzGbWDgriYXaz4mcQ4VCCUmxThu2cIt/jgkRGZ2UeAfTU3UmVURCy123z2P/sSZ+\n88phpmfGEBNlIzzMgtVi5q97T/Lsa0fo7R8iLyOav7+9kFnZsSFVCBwNk8nEygXpXD03mZff3MWC\nGYmTHZLImKtr7mHbvpMsK0glO9U14r6Wjj5+/efDvDN8wgAgMsLKnJx4X1FwuKdgVrJzwuYYNZtN\nxEfbiY+2Myfn3CKgx2swOOQhIlyH9SIydrRHkZBkMplIjXdQ39KDYRhT7su+iIiInBbrjODOW/L5\n0Qv7uedH28+532EP4+8/Xcia5dMuOjR5KogIt5KbEqGe5RIS2rr6MZtM/t5+T275kP3Hmnj2tRJm\nZcVy07Jsluan8OrOCn7/1nHcAx5y06P5zA0zmJEZQ3JcZEAfR1jMJiwqEorIGNNeRUJWSnwkFXWd\ndHQPEOMM7lWqDMOgo3uAk41dnGzsprqxi7rmHmKibOSkRZObHs20VBeO88yJJCIiIvDxJVkMeryU\n13YyMOhhcMjL4JCHxNhI/vbGmUH/XUFEzvWv/76blo5+fnLP9TS09rL/WBM5aS7io+3sLWngaFUb\nTwxvGxNl46618z5yLlMRkVCnQqGErNQE3yqHdc09QfPl3+Px0tDaS3WDryDo+/Fdv9CE6mdKiY8k\nJy3aVzxMc5GTHk1ijD2gz4SKiEyUZ187QnuXm6/dPo8w68QMGxsv/QNDHDjezMCQh4LchKD5nJtM\nJpOJT16TM9lhiEyqwSEvbV39tHe5ae3sp62znzb/dTdtXb7bzCYPTxZ6/CvrBqtFs5J47o2j/Pb1\nEnqGv0v/tzVzWJKfQlNbH1v3VLHncAMFefH87cdnEhmhk+4iIioUSshKjY8EfAuanG9Oj8nU2z94\nTiHwZGM3dc3dDHmMEdtazCZSExwU5MWTkeQkIymKjKQo0hKjaO3sp7ymw7cKW20HZTUd7DxYx86D\ndf7HR9nDfBMtp0X7J13OSHISZtVKaCISugaHvPzujRKuW5hBdqqL3v5Btrx1nCGPQUNLL/f/9yXY\nbcH1Nai9y80Hh+vZVVzPvmNNDAyeXnEzO8VJ4YxE5k9PID8v4byr7opIaDIMgz730IhiX2une7gI\n6Lutdfiyq3fgos8VZjUT67QR57CExKq5d9wwg7f3nuRP28swm02kJ0axeE4yAImxdj7/iVl8/hOz\nJjlKEZHAElzfkEUuQUq8b+Xj+kla+dgwDFo7+znZ0M2uo918UHnAXxRs6eg/Z/vICCt56TGkDxcC\nM5KcZCZHkRLvuOAXNWdkONkpLj62aORrltd2UlbTQVltB+U1HRw80cyB0mb/46wWE1nJLnLShydm\nHi4iRkWGj8t7ISIy0Tp73Pzn1uNU1Xfx4J1LOVjazJDHIMoexv7jTXz7Z+/x0FeW4Qzw/V51Qxe7\ni33FwZLKVozhc0mZyU6WFaRgt1k5WNpMcXkrlfVlvLy9DLMJpmfGUDg9kcLpCczJidNE9yIhwjAM\n6pp7OFTWQnFZC0crW2lq7x9x4uB8HPYwYp02ctJcvlV2XTZinRHEuWzEuiJ8xUFXBA67b0XvoqKi\nkBh+Gx5mYeOnC9n88514PQa3XZen+TdFRD6CvjVKyEpN8BUKx3vlY6/XoLm9j6qGLqrP+unpHzpj\ny3bAd/Zy4cxEMpKdZA4XBDOSoohx2q54iLDJdHpltFNnS8HXg7GyrovyOl+vw/LaDipqOymr7WAr\n1f7tkmLtJLtg9txBzXcoIkEtPtpOdoqTvUcb6e0fpOhoIwD3f2kJb+6u5O2ik9z/5A6+c9dyYpw2\nWjr6qWnqpr6ll1injawUJ0mxkRN+QOnxGhytbGXXoXp2FddR0+T7DDObYG5OPMsKUliSn0La8PQa\nAJ+9cSaDQx6OVrbx4fFmDpQ2cbSyjWNV7fz+reNYLWZmT4ulcHoi82ckMDMrNiR6ColMBV6vQWV9\nJ8VlLf7iYHuX23+/wx5GRlIUccPFvlhXBHGnLl0RxAxfD/YhxFdi4awkVi/L5nB5KzcszpzscERE\nAp4KhRKyEmPsWMwm6oZ7FNa39FzRF6VT8weeUxBs7MY9MPIsrsVsIi3RwfyZTjKTnAz2NrNq6TzS\nE6OImIShbpERYczJiRsxBNvj8VLb3OMvHPouOzlc7aajx61CoYgEvRXz03nu9RJ2H26gqKQRR4SV\nuTlx5OfG44gI4087ytn4/bfweLz0D5zbG8cWbiEz2UlWspPsFCdZKS6yUpxjPvdr/8AQ+481setQ\nPR8cqaej2zc0MCLcwvJ5qSzNT2HxnGT/qp3nE2a1UJCXQEFeAl9kNn3uIQ6Xt3DgeDMfljb5igwn\nWnjudd/zzs2NZ/70RApnJJCbFq0eNiIBYsjjpfRkO4eHC4OHy1v9c+sBxLlsrFyQTn5uPAW58WQm\nO/X/Owrf/OyCyQ5BRCRoTJlCYXffIFaLSUNvphCLxUxyXCT1LT0cPNHMgz/dQXyMna/eNo9lBSkX\nPMgbHPJS29w9XAjs9hcETzZ2M+Txjtg2zGomIymKzGSn/ycr2UlqwsjhwkVFReRlxIzr33upLBaz\nP+brrsrw3777gz0jeqqIiASrFYWpPPd6CVveOk5jay/XFKZiGd4333X7PKKdNv70bhlJsQ7SEqPI\nSIwiJT6Sti43lXVdVDV0UlnXSWl1+4jntdusZCU7yUo59eMiO8VJnCti1AXEtq5+PjjcwK5D9ew/\n1sjAkO/zJdZpY/WybJbmpzB/RiLhl3lyy26zsmh2Motm+3qXd/UOcLDUNw3FgdIm9pY0srfE18vS\nGRlGQV4C86cnUDgjkYwkfQaITJT+gSGOVbVRXNZKcVkzJZVtI05Ap8RHsqwghYLcePJzE0iJj9Qi\ndSIiMq6mRNWssr6T//WTd7FHhPHIxhUkxUVOdkgyQVISHOwtaeSJ/9iPAbR19vO9X+9m0ewk7rp9\nHmkJUXT1DvDh8Sb2HW3icHkLtc09eL0jFxSJCLeQk+YaUQzMTHaSFBcZEvO3nCnU/h4RmbqyUnz7\n7Yq6TgCumnV6SgaTyTSqSew9Hi/1rb1U1nVS1dBFVX0XVfWdnKhp52hV24htHRFWf6/DrBQn2cm+\n66dWJK5u6GJXcT27DtVxtKrtnPkGl+anMCMzdlx6Bzkjw7mmMI1rCtMAaOno42BpMx8O9zg8cyGs\nOFcEn7wqikWLxjwMkSmvp2+QIxWtHDrRzOHyVo5Xt41YyC4rxenvLZifG098tH0SoxURkalousIl\nnwAAEClJREFUQguFg4OD3H///dTU1DAwMMDGjRuZPn069913HyaTiRkzZvDQQw9hNpt54YUXeP75\n57FarWzcuJHrr7/+sl6zobWXzT/bSVfvIF29gzzw1A7+9zeuHZcPXa/XoKPbTVN7H03tfTS399HU\n1kdFdSvbju3FZAITJt+lyXdp9t2I2WTCBJjMw5em09udOl44deBgNpkwmUzYbZbheUgiiHH5JiCO\nGp6AOFBNdA6kDi9oUtfSw83XTOPWlbn87A8HKSpp5Bvff5usFCfltR3+g7XICCuzsmJHFAQzkqPG\nfJjZVDYZ+wEJHGp/megcuHZ+Gr974ygAV81KuuTHWyxm0hOjSE+M4pozbh/yeKlt6j6jeNhFZX0n\nR6vaOFLROuI5nJFhWExe2ntOAhefb3CixEfb+diiTD62KBPDMGho7eXD400cON7Mseo2et3ej36S\ny6T9gARyDtQ2dXOkopUhj4HXMPB6h3+Gr3vO+v2c+wwDr8fAc9b9Hq9BSVkDDb97xf+902w2kZse\n7S8Kzs2Jx+UI7AWWxkog54CMP7W/SGCb0ELhSy+9RExMDI899hjt7e2sXbuW2bNns2nTJpYuXcrm\nzZvZunUrCxYs4JlnnmHLli243W7WrVvHihUrCA+/tA/O+pYeNv98J62d/Xz51nx6+oZ4/s2jPPDT\nHTzy9WuJdUWM+rkMw6Cnf2i4+NfruzyrINjS0TfijOAI5b2XFPvlslrMxLpsxJ2xmplvMmObf4Wz\nOFcEMVE2//CriTTROXBqQZOYKBsbPjmXKHsY3/nacnYcqOWXLx6isq6T/Nx4FsxMZOHMJPIyYtSj\nbpxNdA5IYFH7y0TnwIrhQmFWipPE2LE7SWi1mId7D7pg/unbB4c81DT1UFXf6S8eVtV30drRO+r5\nBieayWQiJd5BSryD1cumAb4pM8aL9gMyUTmw92gjb++pZnpmDAtmJJKd6jrvdh6vQVFJA39+t5y9\nwwsfjQeL2XeSIH+4MDg7O5bIiKk5J7T2A1Ob2l8ksE1ooXDNmjWsXr0a8BXeLBYLxcXFLFmyBIBV\nq1axY8cOzGYzCxcuJDw8nPDwcLKysigpKaGwsHDUr1Vc1sL3fr2bzp4BPvfxmay9bjqGYTA45GHL\n26U88NR7fG/jCmKcNgzDoM89RFuXm+a24eJfh6/41+wvBvbS5z53onMAk8k3p1BuejQJMXYSYyKH\nL+0kxtqpKj/GvHnzMAwDwwCD4UvjrEtOX/cO3+AdPuXoNQwM7+nHeg2Dvv4hWjv7aevsp63L7bve\n1U9rp5sTNe0MVV2gaDkcs8sRTqzz9Ipoca4IEmLsxJjHrxfBROYAwLy8BBz2MP7+jkKihhfnMJlM\nXDs/neXz0hjyeKf0KnCTYaJzQALLVG3/s6czmMomOgeyU1x8dW0B2SnnLxCMtTCrhWmpLqadVZAo\nKipikcbyAlN3PyCnTVQOVNR28M7ek7yz19eb96u3FXDrqjz//c3tfbyz9ySv7aygodV3Uj8/N55V\nC9OJCLdiNpuwmE2YzSbMppHXzWbOuG4acd1y9vXh38tKD7NsyeIxfCeDl/YDU5vaXySwTWih0OHw\n9e7q7u7m7rvvZtOmTTz66KP+IZ0Oh4Ouri66u7txOp0jHtfd3T3q12lo7eXBp97Daxh8/TPzuXn5\nNMBXIPq7T81lyGPw4rYT/MP/eZtwq5n2Lrd/EvHzxm0PIznO4Sv+xfoKgKcKgQkxduKj7YRZL9w7\nr6vJSvIEz4toGAZdvYPDRURf8bCts5/Wrn7aO920dvkKjA2tvf65m0755OIYViwfn7gmKgdOyU2P\n5nffvfm8w4YtZhMWs4qEE22ic0ACy0S2/7OvHcEZGc5tZxwUjobXa9A/MESfe4j+AQ99/UP0DQzR\n7/bd1uf2nL7ffcZ27pG39Q14/NcHh7zEOCzMObibnLRoctNc5KRHT8lpDSZjH3DrykvLARlf+hyQ\nicqBT18/g2sK0ygua+E3rxzmFy8ewmtAlD2Md/ZWc6C0GcOA8DALq5dl86kVOeSkRY/tH3uGasvU\n2t9fjPYDU5vaXySwTfhiJnV1dXzjG99g3bp13HLLLTz22GP++3p6enC5XERFRdHT0zPi9jN3EBdT\nVFRE/4CX/KwI5udEkhTeQlFRy4htFqQbNM2JYs/xHrzhZuJdFqIiwnFEmImOtOCKtBIdaSHaYcEV\nacEWdmYRcAjoArpwt0NNO9SMMq7JFG2C6GiYFg1gAuzDPzAw5KW7z0tXnwf3oJdpybZxjXc8c2Cy\n3+eLCeTYzjbesSoHAl+w7gPgdOzbixqobR2ksqqaFXMu/tjOXg/7ynrYX9ZDW/f5e4+PltkE4WEm\nbFYztjATzggrFjM0dw6NWDACICLcREpMOCmxYf6fBFcY1gA4mAyFHAg0gRrXhSgHxl6gxnU+ofRd\nIMYMX1gZw6+3NvGrlw75b89MDGf+tEjysyOxh3torSulte7sZxtbyoHT9H0w8OlzYOwFalwXEmzx\nytiY0EJhc3Mzd955J5s3b2b5cl+Xtblz57Jr1y6WLl3Ktm3bWLZsGYWFhfzwhz/E7XYzMDDAiRMn\nmDlz5qhe49Swno/qEbd4Anv9B9two7PjHcudw3jnQKC+z8GUA+PZ/qAcCAbBvA+A0znw3Zxe/unH\n23lzXwdR0YnYbVYGhzw4IsKItIcRFRGGxzDYtu8kRUca8BpgC7eQnxtPZIQVe7iVCJsVu81KhM2C\nPdyKPcJKRLjvNrvN4r//zG0v1MN8z5495MzIp7y2k/LaDspqOiiv7aCyqYeKRrd/O6vFRFayi5x0\nF7lp0eSkR5OTFu2fPmEihEoOBJJg2geAcmA8BFMOhOp3gTlzOvn1nw8zMyuW6xdlkDK86N1EUQ6c\npu+DgU+fA2MvmNofxn8/IIFrQguFTz31FJ2dnTz55JM8+eSTADzwwAP8y7/8C48//ji5ubmsXr0a\ni8XC+vXrWbduHYZh8K1vfQubLXAm/ZbLpxwQ5cDUNpHtnxQXyXfuWs59P3mX/3qn9KLbzsiMYfWy\nbFYuSB+3ieVNJhPx0b7pKhbPSfbf3uceorKuk7IziocVdV2U1XawlerTf0+s3TdsebhwmJseTVJs\n8A1d1j5AlAMyWTmQnerioa8sG6s/Q66A9gNTm9pfJLCZDMMImRnWA7VCH6hxXcj5zhwEQ/yBHGcg\nx3a2YG1/COxYAzm2s4VaDrR09HGsqo1IWxhWq5ne/kF6+gbp6R9iYNDDgpmJ4zon1cViuxCP16C2\nqfuMnoedlNV00N7tHrFduNVMhM1KRLgFW/ipSwsR4dbhy+HrYZbT29gsw7+ftc0Zjz125ACLz+h6\nH+w5EAgCNa4LCbX9QCAI1LjOR+0/PgI5trMpB8ZHIMd2tqDPgTvumOwwzuEeGMAWRCs2nx1v0ZYt\nQZMDcmUmfI5CERGRiRQfbWf5PPtkh3FJLGYTmclOMpOdrFqY4b+9rbOfslpf4bC8poPa5m76Bzy4\nBz20d7lxDwxddHGu0SqcFjmhU3SIiIiIiEhgUKFQREQkSMS6IljkimDR7OQLbuPxGrgHhnAPenAP\neOgf8K3S7B449fvQeW47/Xv/wBCpTvcFn19EREQk4FVUTHYE5zgURL0y4Tzxao7CKUOFQhERkRBi\nMZuIjAi7orkWNVm1BL1p0yY7gnMUDAxAkAw5OyfWLVsmLxiRyxGA+wDQfkBEgoMKhSIiIhJaAvAA\nMZgODkEHiBLkAnAfAMG1H9A+QERk6lKhUERCiw4OrpgODkQk6GnI2RXRcDMJegG4DwDtB0QkOIRe\noTAAiwTBVCCAIC8SBGD7Q3DlQFC3v4gIBOQBYjAdHIIOECXIBeA+AIJrP6B9gIjI1BV6hUIRmdp0\ncHDFgv7gQCcMrphOGIiIiIiITE2hVygMwCJBMBUIIMiLBAHY/hBcORDU7S8iIiIiIiIily30CoUi\nIjK16YTBFdMJAxERERGRqclkGIYx2UGMlSIdyIybYDi4VfuPn2Bof1AOjCflgCgHRDkwtan9RTkg\nygEJlhyQKxNShUIRERERERERERG5PObJDkBEREREREREREQmnwqFIiIiIiIiIiIiokKhiIiIiIiI\niIiIqFAoIiIiIiIiIiIiqFAoIiIiIiIiIiIigHWyAwhVH374IT/4wQ945plnqKys5L777sNkMjFj\nxgweeughzObJr9EODg5y//33U1NTw8DAABs3bmT69OkBGWswUg5MbcHQ/qAcGE/KAQmGHFD7jy/l\nwNQWDO0PyoHxpByQYMgBtb+cTS09Dn7xi1/w4IMP4na7AXjkkUfYtGkTzz33HIZhsHXr1kmO0Oel\nl14iJiaG5557jl/+8pd897vfDdhYg41yYGoLlvYH5cB4UQ5IsOSA2n/8KAemtmBpf1AOjBflgARL\nDqj95WwqFI6DrKwsnnjiCf/vxcXFLFmyBIBVq1bx3nvvTVZoI6xZs4Z//Md/BMAwDCwWS8DGGmyU\nA1NbsLQ/KAfGi3JAgiUH1P7jRzkwtQVL+4NyYLwoByRYckDtL2dToXAcrF69Gqv19KhuwzAwmUwA\nOBwOurq6Jiu0ERwOB1FRUXR3d3P33XezadOmgI012CgHprZgaX9QDowX5YAESw6o/cePcmBqC5b2\nB+XAeFEOSLDkgNpfzqZC4QQ4cyx/T08PLpdrEqMZqa6ujg0bNnDbbbdxyy23BHSswSyQ31flwPgL\n9PdUOTD+Av09VQ6Mv0B+T9X+EyOQ31flwPgL9PdUOTD+Av09VQ6Mv0B+T9X+ciYVCifA3Llz2bVr\nFwDbtm1j8eLFkxyRT3NzM3feeSf33HMPn/nMZ4DAjTXYBer7qhyYGIH8nioHJkYgv6fKgYkRqO+p\n2n/iBOr7qhyYGIH8nioHJkYgv6fKgYkRqO+p2l/OpkLhBLj33nt54okn+NznPsfg4CCrV6+e7JAA\neOqpp+js7OTJJ59k/fr1rF+/nk2bNgVkrMFOOTC1BWr7g3JgoigHJFBzQO0/cZQDU1ugtj8oByaK\nckACNQfU/nI2k2EYxmQHISIiIiIiIiIiIpNLPQpFREREREREREREhUIRERERERERERFRoVBERERE\nRERERERQoVBERERERERERERQoVBEREREREREREQA62QHMJX88z//M3v37mVwcJCqqiry8vIA2LBh\nA3fcccckRyfjTe0vygFRDohyYGpT+4tyQJQDohyQQGcyDMOY7CCmmpMnT7JhwwbeeuutyQ5FJoHa\nX5QDohwQ5cDUpvYX5YAoB0Q5IIFKQ49FREREREREREREhUIRERERERERERFRoVBERERERERERERQ\noVBERERERERERERQoVBERERERERERERQoVBEREREREREREQAk2EYxmQHISIiIiIiIiIiIpNLPQpF\nREREREREREREhUIRERERERERERFRoVBERERERERERERQoVBERERERERERERQoVBERERERERERERQ\noVBERERERERERERQoVBERERERERERERQoVBERERERERERESA/w+CICcBniK2wgAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1565f2828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing results\n",
    "def viz_result(measure, v_min, v_max):\n",
    "    l = len(p_informative_list)\n",
    "    fig, ax = plt.subplots(1, 9, figsize=(20, 3))\n",
    "    plt.suptitle('Distillation {} vs Temperature'.format(measure), fontsize=16, weight='heavy')\n",
    "    plt.subplots_adjust(top=0.7, hspace=0.5, wspace=0.5)\n",
    "\n",
    "    for i, cs in enumerate(class_sep_list):\n",
    "        for j, pi in enumerate(p_informative_list):\n",
    "            ax[j+i*l].axhline(results_df.loc[(cs, pi, 'teacher'), measure].values[0], c='r', label='teacher')\n",
    "            ax[j+i*l].axhline(results_df.loc[(cs, pi, 'student'), measure].values[0], c='g', label='student_baseline')\n",
    "            ax[j+i*l].plot(results_df.loc[(cs, pi, 'distilled_student'), 'temp'].values, \n",
    "                           results_df.loc[(cs, pi, 'distilled_student'), measure].values, label='student_distilled')\n",
    "            ax[j+i*l].set_xlabel('T')\n",
    "            ax[j+i*l].set_ylabel(measure)\n",
    "            ax[j+i*l].set_ylim(v_min, v_max)\n",
    "            ax[j+i*l].set_title('class_sep = {}, \\np_informative = {}'.format(cs, pi))\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "viz_result('accuracy', 0.9, 1)\n",
    "viz_result('num_error', 100, 1050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion - Distillation Results on Fake Datasets\n",
    "\n",
    "We found that, while distilled student nets showed superior model performance over non-distilled student nets in all cases, data quality did not affect the optimal distillation temperature ($T_{opt} \\in (2.5, 5)$ in all cases). Based on the results reported in the paper: A $T_{opt}$ at 20, 8 and 2.5-4 was found for student nets with 800, 300 and 30 nodes per hidden layer, respectively, we suspect that optimal distillation temperature may be more correlated with student net's architecture complexity, where $T_{opt}$ is higher for more complex structures. This hypothesis remains to be tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
