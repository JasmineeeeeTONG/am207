{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Flatten, Dense, Dropout, Lambda, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# load mnist data & normalize to 0-1\n",
    "# 1D data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28).astype('float32')\n",
    "x_test = x_test.reshape(-1, 28*28).astype('float32')\n",
    "x_train /= 255.\n",
    "x_test /= 255. \n",
    "\n",
    "# 2D data\n",
    "(x_train_2D, _), (x_test_2D, _) = mnist.load_data()\n",
    "x_train_2D = x_train_2D.reshape(x_train_2D.shape[0], 1, 28, 28).astype('float32')\n",
    "x_test_2D = x_test_2D.reshape(x_test_2D.shape[0], 1, 28, 28).astype('float32')\n",
    "x_train_2D /= 255.\n",
    "x_test_2D /= 255.\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Net \n",
    "1. DENSE 1200-1200-10 with HEAVY REGULARIZATION\n",
    "2. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "k_constraint = keras.constraints.MaxNorm(max_value=15, axis=0)\n",
    "\n",
    "mnist_dense = Sequential()\n",
    "mnist_dense.add(Dense(1200, name='hidden_1', input_shape=(28*28, ), activation='relu', kernel_initializer=k_init, kernel_constraint=k_constraint))\n",
    "mnist_dense.add(Dropout(0.7, name='dropout_1'))\n",
    "mnist_dense.add(Dense(1200, name='hidden_2', activation='relu', kernel_initializer=k_init, kernel_constraint=k_constraint))\n",
    "mnist_dense.add(Dropout(0.7, name='dropout_2'))\n",
    "mnist_dense.add(Dense(10, name='logit'))\n",
    "mnist_dense.add(Activation('softmax', name='softmax'))\n",
    "\n",
    "mnist_dense.compile(loss=categorical_crossentropy, optimizer=Adam(0.0001), metrics=['accuracy'])\n",
    "\n",
    "mnist_dense.fit(x_train, y_train, batch_size=100, epochs=20, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "loss, accuracy = mnist_dense.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DENSE TECHERT NET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "\n",
    "mnist_dense.save('./models/mnist_teacher_dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENSE TECHERT NET - On test set:\n",
      "loss = 0.055868843965092674, accuracy = 0.9832, #errors = 168\n"
     ]
    }
   ],
   "source": [
    "mnist_dense = load_model('./models/mnist_teacher_dense.h5')\n",
    "loss, accuracy = mnist_dense.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DENSE TECHERT NET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a CNN teacher net\n",
    "mnist_cnn = Sequential()\n",
    "\n",
    "mnist_cnn.add(Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(1, 28, 28), name='conv_1'))\n",
    "mnist_cnn.add(Conv2D(128, (3, 3), activation='relu', padding='same', name='conv_2'))\n",
    "mnist_cnn.add(MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_1'))\n",
    "mnist_cnn.add(Dropout(0.25, name='dropout_1'))\n",
    "\n",
    "mnist_cnn.add(Flatten())\n",
    "mnist_cnn.add(Dense(10, name='logit'))\n",
    "mnist_cnn.add(Activation('softmax', name='softmax'))\n",
    "\n",
    "mnist_cnn.compile(loss=categorical_crossentropy, optimizer=Adam(lr=0.0005), metrics=['accuracy'])\n",
    "mnist_cnn.fit(x_train_2D, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test_2D, y_test))\n",
    "\n",
    "mnist_cnn.save('./models/mnist_teacher_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN TEACHER NET - On test set:\n",
      "loss = 0.030440742732951186, accuracy = 0.9903, #errors = 97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_cnn = load_model('./models/mnist_teacher_cnn.h5')\n",
    "loss, accuracy = mnist_cnn.evaluate(x_test_2D, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('CNN TEACHER NET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Net 1\n",
    "- NO DISTILLATION BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_StudentNet(n_hidden, T):\n",
    "    '''\n",
    "    Function to build a studnet net\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "    model.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "    model.add(Dense(10, name='logit'))\n",
    "    model.add(Lambda(lambda x: x / T, name='logit_soft'))\n",
    "    model.add(Activation('softmax', name='softmax'))\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.5879 - acc: 0.8240 - val_loss: 0.2767 - val_acc: 0.9236\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2539 - acc: 0.9275 - val_loss: 0.2207 - val_acc: 0.9364\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2081 - acc: 0.9404 - val_loss: 0.2061 - val_acc: 0.9383\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1822 - acc: 0.9475 - val_loss: 0.1834 - val_acc: 0.9473\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1647 - acc: 0.9527 - val_loss: 0.1707 - val_acc: 0.9512\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1513 - acc: 0.9568 - val_loss: 0.1581 - val_acc: 0.9537\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1416 - acc: 0.9590 - val_loss: 0.1510 - val_acc: 0.9555\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1334 - acc: 0.9610 - val_loss: 0.1466 - val_acc: 0.9585\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1252 - acc: 0.9639 - val_loss: 0.1440 - val_acc: 0.9600\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1189 - acc: 0.9655 - val_loss: 0.1362 - val_acc: 0.9612\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1133 - acc: 0.9667 - val_loss: 0.1392 - val_acc: 0.9615\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1083 - acc: 0.9678 - val_loss: 0.1390 - val_acc: 0.9617\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1036 - acc: 0.9699 - val_loss: 0.1450 - val_acc: 0.9586\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1020 - acc: 0.9694 - val_loss: 0.1338 - val_acc: 0.9632\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0958 - acc: 0.9713 - val_loss: 0.1309 - val_acc: 0.9635\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0941 - acc: 0.9716 - val_loss: 0.1286 - val_acc: 0.9641\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0900 - acc: 0.9736 - val_loss: 0.1339 - val_acc: 0.9625\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0871 - acc: 0.9738 - val_loss: 0.1335 - val_acc: 0.9632\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0853 - acc: 0.9740 - val_loss: 0.1326 - val_acc: 0.9642\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0819 - acc: 0.9753 - val_loss: 0.1453 - val_acc: 0.9601\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0811 - acc: 0.9754 - val_loss: 0.1321 - val_acc: 0.9634\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0784 - acc: 0.9764 - val_loss: 0.1355 - val_acc: 0.9640\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0752 - acc: 0.9770 - val_loss: 0.1341 - val_acc: 0.9651\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0740 - acc: 0.9781 - val_loss: 0.1342 - val_acc: 0.9643\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0721 - acc: 0.9781 - val_loss: 0.1372 - val_acc: 0.9609\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0709 - acc: 0.9786 - val_loss: 0.1365 - val_acc: 0.9641\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0682 - acc: 0.9793 - val_loss: 0.1293 - val_acc: 0.9660\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0675 - acc: 0.9792 - val_loss: 0.1418 - val_acc: 0.9628\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0662 - acc: 0.9801 - val_loss: 0.1349 - val_acc: 0.9649\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0642 - acc: 0.9804 - val_loss: 0.1387 - val_acc: 0.9641\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0638 - acc: 0.9803 - val_loss: 0.1356 - val_acc: 0.9649\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0616 - acc: 0.9810 - val_loss: 0.1305 - val_acc: 0.9645\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0603 - acc: 0.9811 - val_loss: 0.1395 - val_acc: 0.9627\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0588 - acc: 0.9823 - val_loss: 0.1358 - val_acc: 0.9643\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0581 - acc: 0.9817 - val_loss: 0.1384 - val_acc: 0.9634\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0572 - acc: 0.9827 - val_loss: 0.1403 - val_acc: 0.9619\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0558 - acc: 0.9824 - val_loss: 0.1473 - val_acc: 0.9649\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0548 - acc: 0.9831 - val_loss: 0.1422 - val_acc: 0.9655\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0540 - acc: 0.9832 - val_loss: 0.1416 - val_acc: 0.9635\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0533 - acc: 0.9833 - val_loss: 0.1490 - val_acc: 0.9626\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0510 - acc: 0.9844 - val_loss: 0.1540 - val_acc: 0.9628\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0511 - acc: 0.9842 - val_loss: 0.1468 - val_acc: 0.9646\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0507 - acc: 0.9839 - val_loss: 0.1540 - val_acc: 0.9617\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0500 - acc: 0.9845 - val_loss: 0.1505 - val_acc: 0.9632\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0477 - acc: 0.9849 - val_loss: 0.1481 - val_acc: 0.9637\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0471 - acc: 0.9851 - val_loss: 0.1564 - val_acc: 0.9617\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0475 - acc: 0.9858 - val_loss: 0.1571 - val_acc: 0.9627\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0451 - acc: 0.9857 - val_loss: 0.1531 - val_acc: 0.9640\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0452 - acc: 0.9859 - val_loss: 0.1527 - val_acc: 0.9642\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0445 - acc: 0.9862 - val_loss: 0.1535 - val_acc: 0.9632\n",
      "STUDENT BASELINE - On test set:\n",
      "loss = 0.15354005151256278, accuracy = 0.9632, #errors = 368\n"
     ]
    }
   ],
   "source": [
    "# train baseline student net - NO distillation\n",
    "mnist_student_basline = MNIST_StudentNet(n_hidden=20, T=1)\n",
    "mnist_student_basline.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "mnist_student_basline.fit(x_train, y_train, batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# baseline student net model evaluation\n",
    "loss, accuracy = mnist_student_basline.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('STUDENT BASELINE - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Net 2\n",
    "- DISTILLED --- DENSE TEACHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 1.0700 - acc: 0.7853 - val_loss: 0.4012 - val_acc: 0.9120\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.7297 - acc: 0.9250 - val_loss: 0.3376 - val_acc: 0.9288\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6959 - acc: 0.9388 - val_loss: 0.3081 - val_acc: 0.9382\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6797 - acc: 0.9446 - val_loss: 0.3004 - val_acc: 0.9417\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.6695 - acc: 0.9486 - val_loss: 0.2861 - val_acc: 0.9449\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6622 - acc: 0.9525 - val_loss: 0.2839 - val_acc: 0.9467\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.6564 - acc: 0.9557 - val_loss: 0.2758 - val_acc: 0.9501\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.6520 - acc: 0.9582 - val_loss: 0.2720 - val_acc: 0.9514\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.6486 - acc: 0.9598 - val_loss: 0.2664 - val_acc: 0.9525\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.6455 - acc: 0.9613 - val_loss: 0.2652 - val_acc: 0.9554\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.6428 - acc: 0.9628 - val_loss: 0.2621 - val_acc: 0.9555\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.6404 - acc: 0.9637 - val_loss: 0.2596 - val_acc: 0.9589\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.6382 - acc: 0.9652 - val_loss: 0.2557 - val_acc: 0.9590\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.6366 - acc: 0.9660 - val_loss: 0.2529 - val_acc: 0.9587\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.6348 - acc: 0.9673 - val_loss: 0.2565 - val_acc: 0.9574\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6334 - acc: 0.9680 - val_loss: 0.2501 - val_acc: 0.9591\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6325 - acc: 0.9684 - val_loss: 0.2515 - val_acc: 0.9603\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6312 - acc: 0.9690 - val_loss: 0.2459 - val_acc: 0.9600\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.6302 - acc: 0.9697 - val_loss: 0.2489 - val_acc: 0.9609\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.6293 - acc: 0.9712 - val_loss: 0.2466 - val_acc: 0.9603\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.6287 - acc: 0.9709 - val_loss: 0.2421 - val_acc: 0.9595\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6278 - acc: 0.9716 - val_loss: 0.2438 - val_acc: 0.9612\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.6272 - acc: 0.9722 - val_loss: 0.2439 - val_acc: 0.9626\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6265 - acc: 0.9724 - val_loss: 0.2435 - val_acc: 0.9612\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6260 - acc: 0.9730 - val_loss: 0.2435 - val_acc: 0.9613\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6254 - acc: 0.9732 - val_loss: 0.2446 - val_acc: 0.9613\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.6250 - acc: 0.9736 - val_loss: 0.2419 - val_acc: 0.9628\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.6244 - acc: 0.9740 - val_loss: 0.2410 - val_acc: 0.9630\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6239 - acc: 0.9745 - val_loss: 0.2385 - val_acc: 0.9625\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.6232 - acc: 0.9751 - val_loss: 0.2334 - val_acc: 0.9635\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6229 - acc: 0.9747 - val_loss: 0.2425 - val_acc: 0.9623\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.6223 - acc: 0.9755 - val_loss: 0.2364 - val_acc: 0.9649\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.6220 - acc: 0.9758 - val_loss: 0.2349 - val_acc: 0.9640\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6215 - acc: 0.9756 - val_loss: 0.2411 - val_acc: 0.9634\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.6209 - acc: 0.9757 - val_loss: 0.2323 - val_acc: 0.9641\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6205 - acc: 0.9759 - val_loss: 0.2375 - val_acc: 0.9630\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.6202 - acc: 0.9761 - val_loss: 0.2331 - val_acc: 0.9640\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6198 - acc: 0.9767 - val_loss: 0.2342 - val_acc: 0.9653\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.6194 - acc: 0.9765 - val_loss: 0.2303 - val_acc: 0.9656\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.6191 - acc: 0.9774 - val_loss: 0.2349 - val_acc: 0.9643\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.6187 - acc: 0.9772 - val_loss: 0.2314 - val_acc: 0.9638\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.6184 - acc: 0.9773 - val_loss: 0.2342 - val_acc: 0.9640\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.6182 - acc: 0.9781 - val_loss: 0.2349 - val_acc: 0.9653\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.6179 - acc: 0.9779 - val_loss: 0.2332 - val_acc: 0.9647\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6177 - acc: 0.9779 - val_loss: 0.2302 - val_acc: 0.9651\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.6173 - acc: 0.9783 - val_loss: 0.2322 - val_acc: 0.9661\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.6172 - acc: 0.9784 - val_loss: 0.2358 - val_acc: 0.9650\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6169 - acc: 0.9784 - val_loss: 0.2306 - val_acc: 0.9655\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.6168 - acc: 0.9784 - val_loss: 0.2291 - val_acc: 0.9652\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6165 - acc: 0.9790 - val_loss: 0.2331 - val_acc: 0.9654\n",
      "DISTILLED STUDENT - On test set:\n",
      "loss = 0.23310526382923127, accuracy = 0.9654, #errors = 345\n"
     ]
    }
   ],
   "source": [
    "def get_layer_output(model, layer_name):\n",
    "    output = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    return output\n",
    "\n",
    "# compute 'soft target'\n",
    "T = 3\n",
    "teacher_logit = get_layer_output(mnist_dense, 'logit')\n",
    "logit_train = teacher_logit.predict(x_train)\n",
    "y_train_soft = K.softmax(logit_train / T).eval(session=K.get_session())\n",
    "\n",
    "# train student net distilled from the dense teacher net\n",
    "mnist_student_distilled = MNIST_StudentNet(n_hidden=20, T=T)\n",
    "mnist_student_distilled.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "mnist_student_distilled.fit(x_train, y_train_soft, \n",
    "                            batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# distilled student net model evaluation\n",
    "loss, accuracy = mnist_student_distilled.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DISTILLED STUDENT - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Net 3\n",
    "- DISTILLED --- DENSE TEACHER + WEIGHTED HARD/SOFT TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_mix_loss(y_true, y_pred, w, T):    \n",
    "    # split hard & soft targets\n",
    "    y_hard, y_soft = y_true[:, :10], y_true[:, 10:]\n",
    "    \n",
    "    # convert logits to predicted values\n",
    "    y_hard_pred = K.softmax(y_pred) # hard target\n",
    "    y_soft_pred = K.softmax(y_pred / T) # soft target\n",
    "    \n",
    "    # compute weighted avg of the 2 parts of losses\n",
    "    avg_loss = w * categorical_crossentropy(y_hard, y_hard_pred) + categorical_crossentropy(y_soft, y_soft_pred)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 1.0612 - acc: 0.8294 - val_loss: 0.7709 - val_acc: 0.9172\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.7508 - acc: 0.9234 - val_loss: 0.7099 - val_acc: 0.9344\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.7103 - acc: 0.9362 - val_loss: 0.6848 - val_acc: 0.9429\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.6903 - acc: 0.9442 - val_loss: 0.6726 - val_acc: 0.9472\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6769 - acc: 0.9497 - val_loss: 0.6657 - val_acc: 0.9481\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6675 - acc: 0.9533 - val_loss: 0.6559 - val_acc: 0.9543\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6604 - acc: 0.9565 - val_loss: 0.6514 - val_acc: 0.9554\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6549 - acc: 0.9590 - val_loss: 0.6480 - val_acc: 0.9580\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6504 - acc: 0.9612 - val_loss: 0.6445 - val_acc: 0.9614\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.6469 - acc: 0.9632 - val_loss: 0.6416 - val_acc: 0.9627\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.6441 - acc: 0.9647 - val_loss: 0.6406 - val_acc: 0.9614\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6418 - acc: 0.9658 - val_loss: 0.6385 - val_acc: 0.9615\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.6399 - acc: 0.9669 - val_loss: 0.6365 - val_acc: 0.9646\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6383 - acc: 0.9686 - val_loss: 0.6354 - val_acc: 0.9636\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6367 - acc: 0.9690 - val_loss: 0.6353 - val_acc: 0.9644\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6355 - acc: 0.9696 - val_loss: 0.6350 - val_acc: 0.9637\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6342 - acc: 0.9704 - val_loss: 0.6352 - val_acc: 0.9631\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.6332 - acc: 0.9707 - val_loss: 0.6333 - val_acc: 0.9655\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6323 - acc: 0.9712 - val_loss: 0.6325 - val_acc: 0.9656\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6314 - acc: 0.9715 - val_loss: 0.6317 - val_acc: 0.9651\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6308 - acc: 0.9723 - val_loss: 0.6311 - val_acc: 0.9644\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.6300 - acc: 0.9724 - val_loss: 0.6305 - val_acc: 0.9662\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.6293 - acc: 0.9729 - val_loss: 0.6297 - val_acc: 0.9663\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6288 - acc: 0.9725 - val_loss: 0.6298 - val_acc: 0.9652\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6281 - acc: 0.9737 - val_loss: 0.6295 - val_acc: 0.9652\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.6277 - acc: 0.9737 - val_loss: 0.6286 - val_acc: 0.9662\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.6271 - acc: 0.9743 - val_loss: 0.6288 - val_acc: 0.9663\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.6267 - acc: 0.9744 - val_loss: 0.6287 - val_acc: 0.9655\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6265 - acc: 0.9747 - val_loss: 0.6286 - val_acc: 0.9673\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6259 - acc: 0.9748 - val_loss: 0.6272 - val_acc: 0.9674\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.6256 - acc: 0.9751 - val_loss: 0.6280 - val_acc: 0.9664\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.6250 - acc: 0.9756 - val_loss: 0.6275 - val_acc: 0.9667\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.6246 - acc: 0.9755 - val_loss: 0.6270 - val_acc: 0.9673\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.6243 - acc: 0.9757 - val_loss: 0.6258 - val_acc: 0.9676\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.6240 - acc: 0.9756 - val_loss: 0.6265 - val_acc: 0.9674\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.6236 - acc: 0.9762 - val_loss: 0.6257 - val_acc: 0.9682\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.6235 - acc: 0.9756 - val_loss: 0.6262 - val_acc: 0.9678\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.6229 - acc: 0.9767 - val_loss: 0.6287 - val_acc: 0.9661\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.6227 - acc: 0.9765 - val_loss: 0.6252 - val_acc: 0.9679\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.6223 - acc: 0.9769 - val_loss: 0.6250 - val_acc: 0.9673\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.6221 - acc: 0.9772 - val_loss: 0.6259 - val_acc: 0.9669\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6218 - acc: 0.9769 - val_loss: 0.6258 - val_acc: 0.9667\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6216 - acc: 0.9774 - val_loss: 0.6243 - val_acc: 0.9689\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6213 - acc: 0.9777 - val_loss: 0.6261 - val_acc: 0.9678\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6211 - acc: 0.9779 - val_loss: 0.6247 - val_acc: 0.9673\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.6208 - acc: 0.9773 - val_loss: 0.6243 - val_acc: 0.9673\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.6206 - acc: 0.9781 - val_loss: 0.6239 - val_acc: 0.9681\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6204 - acc: 0.9778 - val_loss: 0.6239 - val_acc: 0.9673\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.6202 - acc: 0.9782 - val_loss: 0.6249 - val_acc: 0.9670\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.6200 - acc: 0.9780 - val_loss: 0.6234 - val_acc: 0.9676\n",
      "DISTILLED STUDENT with WEIGHTED HARD/SOFT TARGET - On test set:\n",
      "loss = 0.6234446800231933, accuracy = 0.9676, #errors = 323\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "T = 3\n",
    "w = 0.7 / (T**2)\n",
    "\n",
    "# apply both hard & soft targets to learn\n",
    "logit_test = teacher_logit.predict(x_test)\n",
    "y_test_soft = K.softmax(logit_test / T).eval(session=K.get_session())\n",
    "y_hard_soft_train = np.concatenate((y_train, y_train_soft), axis=1)\n",
    "y_hard_soft_test = np.concatenate((y_test, y_test_soft), axis=1)\n",
    "\n",
    "# fit the student net distilled from the dense teacher net with the hard-soft weighted avg loss\n",
    "mnist_student_mix = Sequential()\n",
    "mnist_student_mix.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "mnist_student_mix.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "mnist_student_mix.add(Dense(10, name='logit'))\n",
    "\n",
    "# y_pred at the end of 10-node dense layer is the logit_pred\n",
    "mnist_student_mix.compile(loss=lambda y_true, y_pred: avg_mix_loss(y_true, y_pred, w, T), \n",
    "                          optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "mnist_student_mix.fit(x_train, y_hard_soft_train, \n",
    "                      batch_size=100, epochs=50, verbose=1, validation_data=(x_test, y_hard_soft_test))\n",
    "\n",
    "\n",
    "# distilled student net with mix-hard-soft loss model evaluation\n",
    "loss, accuracy = mnist_student_mix.evaluate(x_test, y_hard_soft_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DISTILLED STUDENT with WEIGHTED HARD/SOFT TARGET - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Net 4\n",
    "- DISTILLED --- CNN TEACHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.9567 - acc: 0.7943 - val_loss: 0.3894 - val_acc: 0.9022\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.6094 - acc: 0.9093 - val_loss: 0.3409 - val_acc: 0.9193\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.5729 - acc: 0.9221 - val_loss: 0.3056 - val_acc: 0.9297\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.5469 - acc: 0.9314 - val_loss: 0.2815 - val_acc: 0.9345\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.5277 - acc: 0.9388 - val_loss: 0.2654 - val_acc: 0.9405\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.5138 - acc: 0.9443 - val_loss: 0.2596 - val_acc: 0.9394\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.5043 - acc: 0.9469 - val_loss: 0.2498 - val_acc: 0.9442\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4966 - acc: 0.9500 - val_loss: 0.2448 - val_acc: 0.9450\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.4899 - acc: 0.9527 - val_loss: 0.2362 - val_acc: 0.9482\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.4847 - acc: 0.9547 - val_loss: 0.2314 - val_acc: 0.9488\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4802 - acc: 0.9568 - val_loss: 0.2281 - val_acc: 0.9496\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4763 - acc: 0.9582 - val_loss: 0.2175 - val_acc: 0.9525\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4728 - acc: 0.9597 - val_loss: 0.2181 - val_acc: 0.9538\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4700 - acc: 0.9607 - val_loss: 0.2139 - val_acc: 0.9541\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4671 - acc: 0.9619 - val_loss: 0.2099 - val_acc: 0.9572\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4647 - acc: 0.9629 - val_loss: 0.2153 - val_acc: 0.9560\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4626 - acc: 0.9633 - val_loss: 0.2038 - val_acc: 0.9585\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4600 - acc: 0.9653 - val_loss: 0.2099 - val_acc: 0.9578\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4589 - acc: 0.9658 - val_loss: 0.1965 - val_acc: 0.9605\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4565 - acc: 0.9666 - val_loss: 0.1949 - val_acc: 0.9591\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4550 - acc: 0.9675 - val_loss: 0.1955 - val_acc: 0.9605\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4533 - acc: 0.9680 - val_loss: 0.2008 - val_acc: 0.9591\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4520 - acc: 0.9688 - val_loss: 0.1958 - val_acc: 0.9623\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4510 - acc: 0.9693 - val_loss: 0.1948 - val_acc: 0.9615\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.4496 - acc: 0.9699 - val_loss: 0.1884 - val_acc: 0.9617\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.4485 - acc: 0.9703 - val_loss: 0.1899 - val_acc: 0.9618\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4473 - acc: 0.9711 - val_loss: 0.1880 - val_acc: 0.9616\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4462 - acc: 0.9716 - val_loss: 0.1957 - val_acc: 0.9606\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4456 - acc: 0.9716 - val_loss: 0.1828 - val_acc: 0.9638\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4448 - acc: 0.9725 - val_loss: 0.1899 - val_acc: 0.9607\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4443 - acc: 0.9728 - val_loss: 0.1916 - val_acc: 0.9617\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4432 - acc: 0.9729 - val_loss: 0.1911 - val_acc: 0.9619\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4423 - acc: 0.9729 - val_loss: 0.1863 - val_acc: 0.9610\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4417 - acc: 0.9737 - val_loss: 0.1847 - val_acc: 0.9640\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4413 - acc: 0.9737 - val_loss: 0.1861 - val_acc: 0.9636\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4403 - acc: 0.9746 - val_loss: 0.1855 - val_acc: 0.9633\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4396 - acc: 0.9743 - val_loss: 0.1871 - val_acc: 0.9641\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4391 - acc: 0.9742 - val_loss: 0.1858 - val_acc: 0.9645\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.4386 - acc: 0.9751 - val_loss: 0.1814 - val_acc: 0.9655\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4377 - acc: 0.9752 - val_loss: 0.1776 - val_acc: 0.9671\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4372 - acc: 0.9754 - val_loss: 0.1795 - val_acc: 0.9668\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4368 - acc: 0.9757 - val_loss: 0.1867 - val_acc: 0.9655\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4362 - acc: 0.9762 - val_loss: 0.1837 - val_acc: 0.9647\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4357 - acc: 0.9769 - val_loss: 0.1837 - val_acc: 0.9662\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4353 - acc: 0.9765 - val_loss: 0.1863 - val_acc: 0.9648\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4348 - acc: 0.9770 - val_loss: 0.1758 - val_acc: 0.9672\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4345 - acc: 0.9770 - val_loss: 0.1792 - val_acc: 0.9677\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4340 - acc: 0.9775 - val_loss: 0.1803 - val_acc: 0.9653\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.4336 - acc: 0.9773 - val_loss: 0.1770 - val_acc: 0.9668\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4335 - acc: 0.9776 - val_loss: 0.1806 - val_acc: 0.9660\n",
      "DISTILLED STUDENT from CNN TEACHER - On test set:\n",
      "loss = 0.1805808451652527, accuracy = 0.966, #errors = 340\n"
     ]
    }
   ],
   "source": [
    "# compute 'soft target' of the CNN teacher net\n",
    "T = 3\n",
    "teacher_logit_cnn = get_layer_output(mnist_cnn, 'logit')\n",
    "logit_train_cnn = teacher_logit_cnn.predict(x_train_2D)\n",
    "y_train_soft_cnn = K.softmax(logit_train_cnn / T).eval(session=K.get_session())\n",
    "\n",
    "# train student net distilled from the CNN teacher net\n",
    "mnist_student_distilled_cnn = MNIST_StudentNet(n_hidden=20, T=T)\n",
    "mnist_student_distilled_cnn.fit(x_train, y_train_soft_cnn, \n",
    "                                batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# student net model evalutation\n",
    "loss, accuracy = mnist_student_distilled_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('DISTILLED STUDENT from CNN TEACHER - On test set:')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary  performances of all distilled student nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Student Net Performance on Full Test Set ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.153540</td>\n",
       "      <td>368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_distilled</th>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.180581</td>\n",
       "      <td>340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.9654</td>\n",
       "      <td>0.233105</td>\n",
       "      <td>345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.9676</td>\n",
       "      <td>0.623445</td>\n",
       "      <td>323.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy      loss  num_error\n",
       "baseline                    0.9632  0.153540      368.0\n",
       "cnn_distilled               0.9660  0.180581      340.0\n",
       "dense_distilled             0.9654  0.233105      345.0\n",
       "dense_distilled_mix_loss    0.9676  0.623445      323.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "student_summary = {\n",
    "    'baseline': {'loss':0.15354005151256278, 'accuracy': 0.9632, 'num_error': 368},\n",
    "    'dense_distilled': {'loss': 0.23310526382923127, 'accuracy': 0.9654, 'num_error': 345},\n",
    "    'dense_distilled_mix_loss': {'loss': 0.6234446800231933, 'accuracy': 0.9676, 'num_error': 323},\n",
    "    'cnn_distilled': {'loss': 0.1805808451652527, 'accuracy': 0.966, 'num_error': 340}\n",
    "}\n",
    "\n",
    "print('=== Student Net Performance on Full Test Set ===')\n",
    "df_student_summary = pd.DataFrame().from_dict(student_summary).T\n",
    "display(df_student_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment with omitting digit 3 in the transfer set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (y_train[:,3] == 1)\n",
    "x_train_omit3 = x_train[~idx,:]\n",
    "y_train_omit3 = y_train[~idx,:]\n",
    "y_train_soft_omit3 = y_train_soft[~idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 3s 49us/step - loss: 0.6009 - acc: 0.8234 - val_loss: 1.2552 - val_acc: 0.8329\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.2178 - acc: 0.9372 - val_loss: 1.3667 - val_acc: 0.8483\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 2s 45us/step - loss: 0.1759 - acc: 0.9487 - val_loss: 1.4066 - val_acc: 0.8508\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.1539 - acc: 0.9545 - val_loss: 1.5036 - val_acc: 0.8594\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.1377 - acc: 0.9591 - val_loss: 1.5293 - val_acc: 0.8607\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.1264 - acc: 0.9620 - val_loss: 1.5749 - val_acc: 0.8619\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.1163 - acc: 0.9647 - val_loss: 1.5799 - val_acc: 0.8636\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.1093 - acc: 0.9673 - val_loss: 1.6266 - val_acc: 0.8657\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.1030 - acc: 0.9687 - val_loss: 1.6357 - val_acc: 0.8650\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.0972 - acc: 0.9700 - val_loss: 1.6565 - val_acc: 0.8629\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.0929 - acc: 0.9713 - val_loss: 1.6510 - val_acc: 0.8667\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.0868 - acc: 0.9735 - val_loss: 1.6737 - val_acc: 0.8664\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.0840 - acc: 0.9746 - val_loss: 1.6823 - val_acc: 0.8668\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.0805 - acc: 0.9758 - val_loss: 1.6897 - val_acc: 0.8633\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.0768 - acc: 0.9764 - val_loss: 1.6925 - val_acc: 0.8666\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.0747 - acc: 0.9774 - val_loss: 1.6955 - val_acc: 0.8671\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.0717 - acc: 0.9778 - val_loss: 1.7012 - val_acc: 0.8663\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.0678 - acc: 0.9798 - val_loss: 1.7133 - val_acc: 0.8677\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.0661 - acc: 0.9800 - val_loss: 1.7143 - val_acc: 0.8680\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.0638 - acc: 0.9805 - val_loss: 1.7195 - val_acc: 0.8675\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.0621 - acc: 0.9808 - val_loss: 1.7242 - val_acc: 0.8680\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0607 - acc: 0.9814 - val_loss: 1.7239 - val_acc: 0.8689\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0577 - acc: 0.9819 - val_loss: 1.7275 - val_acc: 0.8667\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0557 - acc: 0.9831 - val_loss: 1.7347 - val_acc: 0.8669\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.0546 - acc: 0.9833 - val_loss: 1.7271 - val_acc: 0.8694\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0530 - acc: 0.9843 - val_loss: 1.7280 - val_acc: 0.8689\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0514 - acc: 0.9840 - val_loss: 1.7351 - val_acc: 0.8682\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.0496 - acc: 0.9847 - val_loss: 1.7345 - val_acc: 0.8698\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.0488 - acc: 0.9848 - val_loss: 1.7387 - val_acc: 0.8682\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0480 - acc: 0.9853 - val_loss: 1.7374 - val_acc: 0.8682\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0466 - acc: 0.9857 - val_loss: 1.7433 - val_acc: 0.8679\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 1s 24us/step - loss: 0.0453 - acc: 0.9865 - val_loss: 1.7402 - val_acc: 0.8675\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0437 - acc: 0.9864 - val_loss: 1.7408 - val_acc: 0.8700\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.0419 - acc: 0.9868 - val_loss: 1.7455 - val_acc: 0.8679\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0416 - acc: 0.9872 - val_loss: 1.7434 - val_acc: 0.8694\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0407 - acc: 0.9876 - val_loss: 1.7470 - val_acc: 0.8680\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0403 - acc: 0.9876 - val_loss: 1.7500 - val_acc: 0.8679\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0384 - acc: 0.9882 - val_loss: 1.7441 - val_acc: 0.8688\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0369 - acc: 0.9888 - val_loss: 1.7459 - val_acc: 0.8684\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.0366 - acc: 0.9889 - val_loss: 1.7496 - val_acc: 0.8687\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0353 - acc: 0.9894 - val_loss: 1.7526 - val_acc: 0.8690\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0353 - acc: 0.9890 - val_loss: 1.7556 - val_acc: 0.8679\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.0331 - acc: 0.9897 - val_loss: 1.7718 - val_acc: 0.8647\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0339 - acc: 0.9896 - val_loss: 1.7713 - val_acc: 0.8649\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0331 - acc: 0.9895 - val_loss: 1.7564 - val_acc: 0.8677\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0317 - acc: 0.9901 - val_loss: 1.7582 - val_acc: 0.8680\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.0301 - acc: 0.9906 - val_loss: 1.7571 - val_acc: 0.8688\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.0293 - acc: 0.9912 - val_loss: 1.7631 - val_acc: 0.8677\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.0291 - acc: 0.9910 - val_loss: 1.7612 - val_acc: 0.8674\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.0276 - acc: 0.9912 - val_loss: 1.7670 - val_acc: 0.8678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13afab240>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student net - NO distillation - digit 3 omitted in the transfer set\n",
    "hard_omit3 = MNIST_StudentNet(n_hidden=20, T=1)\n",
    "hard_omit3.fit(x_train_omit3, y_train_omit3, \n",
    "               batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 5s 89us/step - loss: 1.0650 - acc: 0.8091 - val_loss: 0.5897 - val_acc: 0.8284\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 2s 37us/step - loss: 0.7366 - acc: 0.9316 - val_loss: 0.5106 - val_acc: 0.8477\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 2s 41us/step - loss: 0.7044 - acc: 0.9451 - val_loss: 0.4765 - val_acc: 0.8564\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 2s 40us/step - loss: 0.6845 - acc: 0.9518 - val_loss: 0.4417 - val_acc: 0.8660\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 3s 47us/step - loss: 0.6720 - acc: 0.9569 - val_loss: 0.4382 - val_acc: 0.8674\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6639 - acc: 0.9604 - val_loss: 0.4226 - val_acc: 0.8732\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 2s 38us/step - loss: 0.6581 - acc: 0.9627 - val_loss: 0.4026 - val_acc: 0.8831\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 2s 34us/step - loss: 0.6541 - acc: 0.9649 - val_loss: 0.3927 - val_acc: 0.8847\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6508 - acc: 0.9671 - val_loss: 0.3997 - val_acc: 0.8803\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6481 - acc: 0.9679 - val_loss: 0.3919 - val_acc: 0.8834\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6456 - acc: 0.9699 - val_loss: 0.3842 - val_acc: 0.8874\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6437 - acc: 0.9709 - val_loss: 0.3827 - val_acc: 0.8876\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6422 - acc: 0.9721 - val_loss: 0.3752 - val_acc: 0.8905\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6405 - acc: 0.9728 - val_loss: 0.3740 - val_acc: 0.8898\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6390 - acc: 0.9742 - val_loss: 0.3741 - val_acc: 0.8910\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6378 - acc: 0.9746 - val_loss: 0.3730 - val_acc: 0.8917\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6368 - acc: 0.9754 - val_loss: 0.3684 - val_acc: 0.8950\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6357 - acc: 0.9766 - val_loss: 0.3732 - val_acc: 0.8910\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 2s 34us/step - loss: 0.6347 - acc: 0.9769 - val_loss: 0.3650 - val_acc: 0.8966\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6339 - acc: 0.9771 - val_loss: 0.3643 - val_acc: 0.8987\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6332 - acc: 0.9782 - val_loss: 0.3601 - val_acc: 0.8973\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6324 - acc: 0.9783 - val_loss: 0.3552 - val_acc: 0.8996\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6317 - acc: 0.9790 - val_loss: 0.3581 - val_acc: 0.9008\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6310 - acc: 0.9795 - val_loss: 0.3563 - val_acc: 0.9014\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6305 - acc: 0.9793 - val_loss: 0.3517 - val_acc: 0.9021\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6299 - acc: 0.9802 - val_loss: 0.3552 - val_acc: 0.9016\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6295 - acc: 0.9802 - val_loss: 0.3533 - val_acc: 0.9031\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6290 - acc: 0.9807 - val_loss: 0.3523 - val_acc: 0.9029\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 3s 48us/step - loss: 0.6284 - acc: 0.9808 - val_loss: 0.3447 - val_acc: 0.9066\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6281 - acc: 0.9813 - val_loss: 0.3456 - val_acc: 0.9072\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6276 - acc: 0.9817 - val_loss: 0.3374 - val_acc: 0.9104\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6274 - acc: 0.9818 - val_loss: 0.3461 - val_acc: 0.9069\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6270 - acc: 0.9819 - val_loss: 0.3390 - val_acc: 0.9121\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6266 - acc: 0.9820 - val_loss: 0.3409 - val_acc: 0.9100\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6264 - acc: 0.9828 - val_loss: 0.3354 - val_acc: 0.9130\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 3s 53us/step - loss: 0.6259 - acc: 0.9825 - val_loss: 0.3328 - val_acc: 0.9152\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 3s 62us/step - loss: 0.6258 - acc: 0.9821 - val_loss: 0.3379 - val_acc: 0.9126\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 2s 40us/step - loss: 0.6255 - acc: 0.9831 - val_loss: 0.3304 - val_acc: 0.9162\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 3s 48us/step - loss: 0.6254 - acc: 0.9830 - val_loss: 0.3317 - val_acc: 0.9163\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6251 - acc: 0.9833 - val_loss: 0.3264 - val_acc: 0.9185\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 2s 36us/step - loss: 0.6248 - acc: 0.9834 - val_loss: 0.3338 - val_acc: 0.9143\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 3s 47us/step - loss: 0.6246 - acc: 0.9837 - val_loss: 0.3267 - val_acc: 0.9212\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 2s 46us/step - loss: 0.6246 - acc: 0.9834 - val_loss: 0.3329 - val_acc: 0.9165\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 3s 50us/step - loss: 0.6242 - acc: 0.9833 - val_loss: 0.3261 - val_acc: 0.9185\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6241 - acc: 0.9838 - val_loss: 0.3290 - val_acc: 0.9169\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6239 - acc: 0.9839 - val_loss: 0.3285 - val_acc: 0.9200\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 2s 37us/step - loss: 0.6237 - acc: 0.9835 - val_loss: 0.3210 - val_acc: 0.9204\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6236 - acc: 0.9838 - val_loss: 0.3303 - val_acc: 0.9155\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6236 - acc: 0.9840 - val_loss: 0.3296 - val_acc: 0.9156\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6235 - acc: 0.9842 - val_loss: 0.3265 - val_acc: 0.9179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1433e84e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student net - WITH distillation - digit 3 omitted in the transfer set\n",
    "soft_omit3 = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "soft_omit3.fit(x_train_omit3, y_train_soft_omit3, \n",
    "               batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 3s 60us/step - loss: 1.1356 - acc: 0.8224 - val_loss: 1.0313 - val_acc: 0.8213\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.7748 - acc: 0.9208 - val_loss: 0.9304 - val_acc: 0.8357\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 2s 34us/step - loss: 0.7281 - acc: 0.9361 - val_loss: 0.8946 - val_acc: 0.8469\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 2s 34us/step - loss: 0.7045 - acc: 0.9451 - val_loss: 0.8583 - val_acc: 0.8546\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 2s 36us/step - loss: 0.6915 - acc: 0.9500 - val_loss: 0.8531 - val_acc: 0.8570\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6825 - acc: 0.9539 - val_loss: 0.8302 - val_acc: 0.8626\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6753 - acc: 0.9567 - val_loss: 0.8156 - val_acc: 0.8660\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6691 - acc: 0.9586 - val_loss: 0.8070 - val_acc: 0.8697\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6642 - acc: 0.9603 - val_loss: 0.7960 - val_acc: 0.8745\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6603 - acc: 0.9618 - val_loss: 0.7887 - val_acc: 0.8765\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6566 - acc: 0.9635 - val_loss: 0.7794 - val_acc: 0.8819\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6538 - acc: 0.9650 - val_loss: 0.7716 - val_acc: 0.8825\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6511 - acc: 0.9658 - val_loss: 0.7600 - val_acc: 0.8877\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6486 - acc: 0.9677 - val_loss: 0.7570 - val_acc: 0.8898\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6466 - acc: 0.9682 - val_loss: 0.7482 - val_acc: 0.8929\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6448 - acc: 0.9689 - val_loss: 0.7401 - val_acc: 0.8956\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6433 - acc: 0.9702 - val_loss: 0.7339 - val_acc: 0.8995\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6417 - acc: 0.9710 - val_loss: 0.7270 - val_acc: 0.9029\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6404 - acc: 0.9719 - val_loss: 0.7244 - val_acc: 0.9052\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6393 - acc: 0.9725 - val_loss: 0.7244 - val_acc: 0.9043\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6383 - acc: 0.9734 - val_loss: 0.7156 - val_acc: 0.9098\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6372 - acc: 0.9741 - val_loss: 0.7224 - val_acc: 0.9052\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6365 - acc: 0.9745 - val_loss: 0.7139 - val_acc: 0.9102\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6357 - acc: 0.9744 - val_loss: 0.7082 - val_acc: 0.9133\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6349 - acc: 0.9749 - val_loss: 0.7036 - val_acc: 0.9148\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6343 - acc: 0.9757 - val_loss: 0.7067 - val_acc: 0.9128\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6338 - acc: 0.9762 - val_loss: 0.6969 - val_acc: 0.9209\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6331 - acc: 0.9763 - val_loss: 0.7006 - val_acc: 0.9197\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 2s 34us/step - loss: 0.6327 - acc: 0.9763 - val_loss: 0.7033 - val_acc: 0.9179\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6323 - acc: 0.9769 - val_loss: 0.7018 - val_acc: 0.9193\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6320 - acc: 0.9774 - val_loss: 0.6972 - val_acc: 0.9210\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6315 - acc: 0.9777 - val_loss: 0.6930 - val_acc: 0.9224\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6311 - acc: 0.9774 - val_loss: 0.6904 - val_acc: 0.9264\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6309 - acc: 0.9780 - val_loss: 0.6873 - val_acc: 0.9275\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6306 - acc: 0.9782 - val_loss: 0.6952 - val_acc: 0.9220\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6303 - acc: 0.9785 - val_loss: 0.6920 - val_acc: 0.9243\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 2s 35us/step - loss: 0.6300 - acc: 0.9785 - val_loss: 0.6901 - val_acc: 0.9254\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6299 - acc: 0.9789 - val_loss: 0.6894 - val_acc: 0.9274\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6295 - acc: 0.9790 - val_loss: 0.6902 - val_acc: 0.9270\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6292 - acc: 0.9789 - val_loss: 0.6903 - val_acc: 0.9265\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6291 - acc: 0.9792 - val_loss: 0.6879 - val_acc: 0.9276\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6288 - acc: 0.9793 - val_loss: 0.6856 - val_acc: 0.9275\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 2s 34us/step - loss: 0.6285 - acc: 0.9795 - val_loss: 0.6855 - val_acc: 0.9291\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6284 - acc: 0.9797 - val_loss: 0.6820 - val_acc: 0.9324\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 2s 36us/step - loss: 0.6283 - acc: 0.9798 - val_loss: 0.6829 - val_acc: 0.9330\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 2s 37us/step - loss: 0.6281 - acc: 0.9797 - val_loss: 0.6858 - val_acc: 0.9291\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6279 - acc: 0.9800 - val_loss: 0.6836 - val_acc: 0.9300\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6277 - acc: 0.9796 - val_loss: 0.6834 - val_acc: 0.9299\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 2s 35us/step - loss: 0.6277 - acc: 0.9801 - val_loss: 0.6827 - val_acc: 0.9308\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6275 - acc: 0.9801 - val_loss: 0.6824 - val_acc: 0.9300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1525cce80>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "T = 5\n",
    "w = 0.5 / (T**2) \n",
    "\n",
    "# apply both hard & soft targets to learn\n",
    "y_hard_soft_train_omit3 = np.concatenate((y_train_omit3, y_train_soft_omit3), axis=1)\n",
    "\n",
    "# student net - WITH distillation & weighted hard soft loss - digit 3 omitted in the transfer set\n",
    "mix_omit3 = Sequential()\n",
    "mix_omit3.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "mix_omit3.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "mix_omit3.add(Dense(10, name='logit'))\n",
    "\n",
    "# y_pred at the end of 10-node dense layer is the logit_pred\n",
    "mix_omit3.compile(loss=lambda y_true, y_pred: avg_mix_loss(y_true, y_pred, w, T), \n",
    "                  optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "mix_omit3.fit(x_train_omit3, y_hard_soft_train_omit3, \n",
    "              batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_hard_soft_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Accuracy on Test set === \n",
      "\n",
      "NO DISTILLATION\n",
      "loss = 1.76703179179905, accuracy = 0.8678, #errors = 1321\n",
      "\n",
      "WITH DISTILLATION\n",
      "loss = 0.3264509074926376, accuracy = 0.9179, #errors = 820\n",
      "\n",
      "WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET\n",
      "loss = 0.6823878009796143, accuracy = 0.93, #errors = 699\n"
     ]
    }
   ],
   "source": [
    "print('=== Overall Accuracy on Test set === \\n')\n",
    "\n",
    "loss, accuracy = hard_omit3.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('NO DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION')\n",
    "loss, accuracy = soft_omit3.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET')\n",
    "loss, accuracy = mix_omit3.evaluate(x_test, y_hard_soft_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Digit-3 Only Accuracy on Test set === \n",
      "\n",
      "NO DISTILLATION\n",
      "loss = 16.111788532521466, accuracy = 0.0, #errors = 10000\n",
      "\n",
      "WITH DISTILLATION\n",
      "loss = 1.2265173893163699, accuracy = 0.4623762377417914, #errors = 5376\n",
      "\n",
      "WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET\n",
      "loss = 1.1009060543362457, accuracy = 0.5722772281948882, #errors = 4277\n"
     ]
    }
   ],
   "source": [
    "print('=== Digit-3 Only Accuracy on Test set === \\n')\n",
    "\n",
    "idx2 = y_test[:,3] == 1\n",
    "x_test_3 = x_test[idx2, :]\n",
    "y_test_3 = y_test[idx2, :]\n",
    "y_test_soft_3 = y_test[idx2,:]\n",
    "y_hard_soft_test_3 = np.concatenate((y_test_3, y_test_soft_3), axis=1)\n",
    "\n",
    "loss, accuracy = hard_omit3.evaluate(x_test_3, y_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('NO DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "loss, accuracy = soft_omit3.evaluate(x_test_3, y_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('WITH DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "loss, accuracy = mix_omit3.evaluate(x_test_3, y_hard_soft_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53869 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "53869/53869 [==============================] - 3s 48us/step - loss: 1.0998 - acc: 0.7858 - val_loss: 0.6146 - val_acc: 0.8251\n",
      "Epoch 2/50\n",
      "53869/53869 [==============================] - 2s 34us/step - loss: 0.7507 - acc: 0.9262 - val_loss: 0.5272 - val_acc: 0.8416\n",
      "Epoch 3/50\n",
      "53869/53869 [==============================] - 2s 36us/step - loss: 0.7094 - acc: 0.9416 - val_loss: 0.4840 - val_acc: 0.8540\n",
      "Epoch 4/50\n",
      "53869/53869 [==============================] - 2s 38us/step - loss: 0.6870 - acc: 0.9500 - val_loss: 0.4469 - val_acc: 0.8631\n",
      "Epoch 5/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6742 - acc: 0.9558 - val_loss: 0.4214 - val_acc: 0.8748\n",
      "Epoch 6/50\n",
      "53869/53869 [==============================] - 2s 38us/step - loss: 0.6658 - acc: 0.9598 - val_loss: 0.4051 - val_acc: 0.8854\n",
      "Epoch 7/50\n",
      "53869/53869 [==============================] - 2s 36us/step - loss: 0.6599 - acc: 0.9621 - val_loss: 0.3867 - val_acc: 0.8928\n",
      "Epoch 8/50\n",
      "53869/53869 [==============================] - 2s 38us/step - loss: 0.6556 - acc: 0.9642 - val_loss: 0.3804 - val_acc: 0.8940\n",
      "Epoch 9/50\n",
      "53869/53869 [==============================] - 2s 41us/step - loss: 0.6520 - acc: 0.9662 - val_loss: 0.3821 - val_acc: 0.8942\n",
      "Epoch 10/50\n",
      "53869/53869 [==============================] - 2s 37us/step - loss: 0.6491 - acc: 0.9675 - val_loss: 0.3679 - val_acc: 0.8996\n",
      "Epoch 11/50\n",
      "53869/53869 [==============================] - 2s 44us/step - loss: 0.6465 - acc: 0.9694 - val_loss: 0.3668 - val_acc: 0.9012\n",
      "Epoch 12/50\n",
      "53869/53869 [==============================] - 2s 34us/step - loss: 0.6444 - acc: 0.9700 - val_loss: 0.3573 - val_acc: 0.9026\n",
      "Epoch 13/50\n",
      "53869/53869 [==============================] - 2s 45us/step - loss: 0.6424 - acc: 0.9714 - val_loss: 0.3566 - val_acc: 0.9045\n",
      "Epoch 14/50\n",
      "53869/53869 [==============================] - 2s 41us/step - loss: 0.6408 - acc: 0.9726 - val_loss: 0.3553 - val_acc: 0.9022\n",
      "Epoch 15/50\n",
      "53869/53869 [==============================] - 2s 46us/step - loss: 0.6396 - acc: 0.9733 - val_loss: 0.3473 - val_acc: 0.9098\n",
      "Epoch 16/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6382 - acc: 0.9739 - val_loss: 0.3515 - val_acc: 0.9056\n",
      "Epoch 17/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6372 - acc: 0.9743 - val_loss: 0.3456 - val_acc: 0.9102\n",
      "Epoch 18/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6363 - acc: 0.9750 - val_loss: 0.3465 - val_acc: 0.9118\n",
      "Epoch 19/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6354 - acc: 0.9758 - val_loss: 0.3428 - val_acc: 0.9082\n",
      "Epoch 20/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6347 - acc: 0.9768 - val_loss: 0.3475 - val_acc: 0.9063\n",
      "Epoch 21/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6339 - acc: 0.9767 - val_loss: 0.3393 - val_acc: 0.9127\n",
      "Epoch 22/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6333 - acc: 0.9770 - val_loss: 0.3372 - val_acc: 0.9130\n",
      "Epoch 23/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6327 - acc: 0.9773 - val_loss: 0.3376 - val_acc: 0.9150\n",
      "Epoch 24/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6324 - acc: 0.9780 - val_loss: 0.3400 - val_acc: 0.9109\n",
      "Epoch 25/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6317 - acc: 0.9784 - val_loss: 0.3281 - val_acc: 0.9184\n",
      "Epoch 26/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6315 - acc: 0.9784 - val_loss: 0.3372 - val_acc: 0.9111\n",
      "Epoch 27/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6310 - acc: 0.9784 - val_loss: 0.3266 - val_acc: 0.9190\n",
      "Epoch 28/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6306 - acc: 0.9793 - val_loss: 0.3357 - val_acc: 0.9133\n",
      "Epoch 29/50\n",
      "53869/53869 [==============================] - 2s 33us/step - loss: 0.6302 - acc: 0.9789 - val_loss: 0.3301 - val_acc: 0.9148\n",
      "Epoch 30/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6299 - acc: 0.9791 - val_loss: 0.3312 - val_acc: 0.9198\n",
      "Epoch 31/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6297 - acc: 0.9794 - val_loss: 0.3286 - val_acc: 0.9211\n",
      "Epoch 32/50\n",
      "53869/53869 [==============================] - 1s 25us/step - loss: 0.6292 - acc: 0.9798 - val_loss: 0.3312 - val_acc: 0.9176\n",
      "Epoch 33/50\n",
      "53869/53869 [==============================] - 2s 42us/step - loss: 0.6290 - acc: 0.9800 - val_loss: 0.3269 - val_acc: 0.9181\n",
      "Epoch 34/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6288 - acc: 0.9802 - val_loss: 0.3299 - val_acc: 0.9162\n",
      "Epoch 35/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6285 - acc: 0.9802 - val_loss: 0.3280 - val_acc: 0.9197\n",
      "Epoch 36/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6282 - acc: 0.9806 - val_loss: 0.3208 - val_acc: 0.9255\n",
      "Epoch 37/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6281 - acc: 0.9805 - val_loss: 0.3314 - val_acc: 0.9182\n",
      "Epoch 38/50\n",
      "53869/53869 [==============================] - 2s 32us/step - loss: 0.6278 - acc: 0.9809 - val_loss: 0.3234 - val_acc: 0.9231\n",
      "Epoch 39/50\n",
      "53869/53869 [==============================] - 1s 28us/step - loss: 0.6276 - acc: 0.9809 - val_loss: 0.3221 - val_acc: 0.9215\n",
      "Epoch 40/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6276 - acc: 0.9809 - val_loss: 0.3181 - val_acc: 0.9262\n",
      "Epoch 41/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6273 - acc: 0.9812 - val_loss: 0.3207 - val_acc: 0.9226\n",
      "Epoch 42/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6272 - acc: 0.9813 - val_loss: 0.3181 - val_acc: 0.9228\n",
      "Epoch 43/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6270 - acc: 0.9816 - val_loss: 0.3158 - val_acc: 0.9254\n",
      "Epoch 44/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6270 - acc: 0.9815 - val_loss: 0.3223 - val_acc: 0.9208\n",
      "Epoch 45/50\n",
      "53869/53869 [==============================] - 1s 26us/step - loss: 0.6266 - acc: 0.9816 - val_loss: 0.3249 - val_acc: 0.9213\n",
      "Epoch 46/50\n",
      "53869/53869 [==============================] - 1s 27us/step - loss: 0.6266 - acc: 0.9815 - val_loss: 0.3230 - val_acc: 0.9253\n",
      "Epoch 47/50\n",
      "53869/53869 [==============================] - 2s 30us/step - loss: 0.6264 - acc: 0.9818 - val_loss: 0.3247 - val_acc: 0.9228\n",
      "Epoch 48/50\n",
      "53869/53869 [==============================] - 2s 29us/step - loss: 0.6264 - acc: 0.9821 - val_loss: 0.3108 - val_acc: 0.9264\n",
      "Epoch 49/50\n",
      "53869/53869 [==============================] - 2s 28us/step - loss: 0.6263 - acc: 0.9818 - val_loss: 0.3197 - val_acc: 0.9218\n",
      "Epoch 50/50\n",
      "53869/53869 [==============================] - 2s 31us/step - loss: 0.6261 - acc: 0.9822 - val_loss: 0.3203 - val_acc: 0.9229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153fe1240>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune bias\n",
    "soft_omit3_bias = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "bias = soft_omit3_bias.layers[2].get_weights()[1]\n",
    "bias[3] = 3.5\n",
    "K.set_value(soft_omit3_bias.layers[2].bias, bias)\n",
    "\n",
    "soft_omit3_bias.fit(x_train_omit3, y_train_soft_omit3, \n",
    "                    batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BIAS TUNED - Overall Accuracy on Test set === \n",
      "\n",
      "loss = 0.32033734719753265, accuracy = 0.9229, #errors = 770\n",
      "\n",
      "=== BIAS TUNED - Digit-3 Only Accuracy on Test set === \n",
      "\n",
      "loss = 1.1231612833419649, accuracy = 0.5257425743459475, #errors = 4742\n"
     ]
    }
   ],
   "source": [
    "print('=== BIAS TUNED - Overall Accuracy on Test set === \\n')\n",
    "loss, accuracy = soft_omit3_bias.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('=== BIAS TUNED - Digit-3 Only Accuracy on Test set === \\n')\n",
    "loss, accuracy = soft_omit3_bias.evaluate(x_test_3, y_test_3, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary performances of distilled student nets which have NEVER seen digit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Omit3 Distillation Performance on Full Test Set ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.8678</td>\n",
       "      <td>1.767032</td>\n",
       "      <td>1321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_tuned</th>\n",
       "      <td>0.9229</td>\n",
       "      <td>0.320337</td>\n",
       "      <td>770.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.9179</td>\n",
       "      <td>0.326451</td>\n",
       "      <td>820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.682388</td>\n",
       "      <td>699.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy      loss  num_error\n",
       "baseline                    0.8678  1.767032     1321.0\n",
       "bias_tuned                  0.9229  0.320337      770.0\n",
       "dense_distilled             0.9179  0.326451      820.0\n",
       "dense_distilled_mix_loss    0.9300  0.682388      699.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Omit3 Distillation Performance on Digit 3 Test Samples ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.111789</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_tuned</th>\n",
       "      <td>0.525743</td>\n",
       "      <td>1.123161</td>\n",
       "      <td>4742.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.462376</td>\n",
       "      <td>1.226517</td>\n",
       "      <td>5376.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.572277</td>\n",
       "      <td>1.100906</td>\n",
       "      <td>4277.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy       loss  num_error\n",
       "baseline                  0.000000  16.111789    10000.0\n",
       "bias_tuned                0.525743   1.123161     4742.0\n",
       "dense_distilled           0.462376   1.226517     5376.0\n",
       "dense_distilled_mix_loss  0.572277   1.100906     4277.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "omit3_summary_overall = {\n",
    "    'baseline': {'loss': 1.76703179179905, 'accuracy': 0.8678, 'num_error': 1321},\n",
    "    'dense_distilled': {'loss': 0.3264509074926376, 'accuracy': 0.9179, 'num_error': 820},\n",
    "    'dense_distilled_mix_loss': {'loss': 0.6823878009796143, 'accuracy': 0.93, 'num_error': 699},\n",
    "    'bias_tuned': {'loss': 0.32033734719753265, 'accuracy': 0.9229, 'num_error': 770}\n",
    "}\n",
    "omit3_summary_3only = {\n",
    "    'baseline': {'loss': 16.111788532521466, 'accuracy': 0.0, 'num_error': 10000},\n",
    "    'dense_distilled': {'loss': 1.2265173893163699, 'accuracy': 0.4623762377417914, 'num_error': 5376},\n",
    "    'dense_distilled_mix_loss': {'loss': 1.1009060543362457, 'accuracy': 0.5722772281948882, 'num_error': 4277},\n",
    "    'bias_tuned': {'loss': 1.1231612833419649, 'accuracy': 0.5257425743459475, 'num_error': 4742}\n",
    "}\n",
    "\n",
    "df_omit3_summary_overall = pd.DataFrame().from_dict(omit3_summary_overall).T\n",
    "df_omit3_summary_3only = pd.DataFrame().from_dict(omit3_summary_3only).T\n",
    "\n",
    "print('=== Omit3 Distillation Performance on Full Test Set ===')\n",
    "display(df_omit3_summary_overall)\n",
    "print()\n",
    "\n",
    "print('=== Omit3 Distillation Performance on Digit 3 Test Samples ===')\n",
    "display(df_omit3_summary_3only)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment with only keeping digit 7 and 8 in the transfer set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 7 and 8 in transfer set\n",
    "idx78 = [True if y[7]==1 or y[8] == 1 else False for y in y_train]\n",
    "\n",
    "x_train_78 = x_train[idx78,:]\n",
    "y_train_soft_78 = y_train_soft[idx78,:]\n",
    "y_train_78 = y_train[idx78,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12116/12116 [==============================] - 2s 161us/step - loss: 0.3170 - acc: 0.9272 - val_loss: 8.2521 - val_acc: 0.1965\n",
      "Epoch 2/50\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.0342 - acc: 0.9889 - val_loss: 9.1068 - val_acc: 0.1973\n",
      "Epoch 3/50\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.0259 - acc: 0.9919 - val_loss: 9.5551 - val_acc: 0.1976\n",
      "Epoch 4/50\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.0220 - acc: 0.9931 - val_loss: 9.8298 - val_acc: 0.1974\n",
      "Epoch 5/50\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.0201 - acc: 0.9931 - val_loss: 10.0648 - val_acc: 0.1980\n",
      "Epoch 6/50\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.0183 - acc: 0.9941 - val_loss: 10.2347 - val_acc: 0.1979\n",
      "Epoch 7/50\n",
      "12116/12116 [==============================] - 1s 99us/step - loss: 0.0177 - acc: 0.9938 - val_loss: 10.3479 - val_acc: 0.1977\n",
      "Epoch 8/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0162 - acc: 0.9948 - val_loss: 10.4804 - val_acc: 0.1980\n",
      "Epoch 9/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0146 - acc: 0.9953 - val_loss: 10.5505 - val_acc: 0.1983\n",
      "Epoch 10/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0140 - acc: 0.9955 - val_loss: 10.6187 - val_acc: 0.1985\n",
      "Epoch 11/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.0148 - acc: 0.9954 - val_loss: 10.6355 - val_acc: 0.1977\n",
      "Epoch 12/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.0123 - acc: 0.9962 - val_loss: 10.7293 - val_acc: 0.1985\n",
      "Epoch 13/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0119 - acc: 0.9960 - val_loss: 10.7953 - val_acc: 0.1981\n",
      "Epoch 14/50\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.0110 - acc: 0.9960 - val_loss: 10.8560 - val_acc: 0.1989\n",
      "Epoch 15/50\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.0102 - acc: 0.9968 - val_loss: 10.8845 - val_acc: 0.1985\n",
      "Epoch 16/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.0084 - acc: 0.9971 - val_loss: 10.9419 - val_acc: 0.1988\n",
      "Epoch 17/50\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 11.0060 - val_acc: 0.1983\n",
      "Epoch 18/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.0073 - acc: 0.9974 - val_loss: 11.0178 - val_acc: 0.1986\n",
      "Epoch 19/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.0067 - acc: 0.9977 - val_loss: 11.0525 - val_acc: 0.1984\n",
      "Epoch 20/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 11.0905 - val_acc: 0.1975\n",
      "Epoch 21/50\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.0059 - acc: 0.9979 - val_loss: 11.1244 - val_acc: 0.1980\n",
      "Epoch 22/50\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 11.1774 - val_acc: 0.1984\n",
      "Epoch 23/50\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.0046 - acc: 0.9982 - val_loss: 11.2052 - val_acc: 0.1985\n",
      "Epoch 24/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 11.2654 - val_acc: 0.1986\n",
      "Epoch 25/50\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 11.2863 - val_acc: 0.1986\n",
      "Epoch 26/50\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.0034 - acc: 0.9988 - val_loss: 11.3389 - val_acc: 0.1983\n",
      "Epoch 27/50\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 11.3731 - val_acc: 0.1986\n",
      "Epoch 28/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 11.3431 - val_acc: 0.1988\n",
      "Epoch 29/50\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.0018 - acc: 0.9998 - val_loss: 11.4389 - val_acc: 0.1984\n",
      "Epoch 30/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 11.4664 - val_acc: 0.1986\n",
      "Epoch 31/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 11.4807 - val_acc: 0.1984\n",
      "Epoch 32/50\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 11.5301 - val_acc: 0.1985\n",
      "Epoch 33/50\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 9.7775e-04 - acc: 1.0000 - val_loss: 11.5446 - val_acc: 0.1987\n",
      "Epoch 34/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 11.5673 - val_acc: 0.1986\n",
      "Epoch 35/50\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 8.9365e-04 - acc: 1.0000 - val_loss: 11.6144 - val_acc: 0.1987\n",
      "Epoch 36/50\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 7.0834e-04 - acc: 1.0000 - val_loss: 11.6400 - val_acc: 0.1987\n",
      "Epoch 37/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 5.6447e-04 - acc: 1.0000 - val_loss: 11.6790 - val_acc: 0.1985\n",
      "Epoch 38/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 5.2345e-04 - acc: 1.0000 - val_loss: 11.6894 - val_acc: 0.1987\n",
      "Epoch 39/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 4.6989e-04 - acc: 1.0000 - val_loss: 11.7391 - val_acc: 0.1986\n",
      "Epoch 40/50\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 5.1136e-04 - acc: 1.0000 - val_loss: 11.7582 - val_acc: 0.1986\n",
      "Epoch 41/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 3.9014e-04 - acc: 1.0000 - val_loss: 11.7727 - val_acc: 0.1987\n",
      "Epoch 42/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 3.4619e-04 - acc: 1.0000 - val_loss: 11.7876 - val_acc: 0.1987\n",
      "Epoch 43/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 3.3569e-04 - acc: 1.0000 - val_loss: 11.7975 - val_acc: 0.1985\n",
      "Epoch 44/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 2.9386e-04 - acc: 1.0000 - val_loss: 11.8095 - val_acc: 0.1985\n",
      "Epoch 45/50\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 2.8390e-04 - acc: 1.0000 - val_loss: 11.8394 - val_acc: 0.1987\n",
      "Epoch 46/50\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 2.5653e-04 - acc: 1.0000 - val_loss: 11.8632 - val_acc: 0.1987\n",
      "Epoch 47/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 2.1209e-04 - acc: 1.0000 - val_loss: 11.8771 - val_acc: 0.1987\n",
      "Epoch 48/50\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 1.8624e-04 - acc: 1.0000 - val_loss: 11.8905 - val_acc: 0.1987\n",
      "Epoch 49/50\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 1.7570e-04 - acc: 1.0000 - val_loss: 11.9116 - val_acc: 0.1987\n",
      "Epoch 50/50\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 1.5753e-04 - acc: 1.0000 - val_loss: 11.9318 - val_acc: 0.1986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153fbe400>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train hard target model with training set omitting digit 3\n",
    "hard_78 = MNIST_StudentNet(n_hidden=20, T=1)\n",
    "hard_78.fit(x_train_78, y_train_78, batch_size=128, epochs=50, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12116/12116 [==============================] - 2s 166us/step - loss: 1.3137 - acc: 0.8350 - val_loss: 2.7221 - val_acc: 0.1943\n",
      "Epoch 2/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.8349 - acc: 0.9833 - val_loss: 2.3942 - val_acc: 0.1964\n",
      "Epoch 3/100\n",
      "12116/12116 [==============================] - 1s 92us/step - loss: 0.7921 - acc: 0.9874 - val_loss: 2.2301 - val_acc: 0.1971\n",
      "Epoch 4/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7810 - acc: 0.9888 - val_loss: 2.1232 - val_acc: 0.1986\n",
      "Epoch 5/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7738 - acc: 0.9893 - val_loss: 2.0344 - val_acc: 0.2018\n",
      "Epoch 6/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7676 - acc: 0.9901 - val_loss: 1.9403 - val_acc: 0.2127\n",
      "Epoch 7/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7628 - acc: 0.9902 - val_loss: 1.8744 - val_acc: 0.2199\n",
      "Epoch 8/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7592 - acc: 0.9904 - val_loss: 1.7867 - val_acc: 0.2264\n",
      "Epoch 9/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7562 - acc: 0.9904 - val_loss: 1.7531 - val_acc: 0.2319\n",
      "Epoch 10/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7533 - acc: 0.9907 - val_loss: 1.7217 - val_acc: 0.2363\n",
      "Epoch 11/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7511 - acc: 0.9912 - val_loss: 1.6749 - val_acc: 0.2551\n",
      "Epoch 12/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7493 - acc: 0.9914 - val_loss: 1.6703 - val_acc: 0.2611\n",
      "Epoch 13/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7476 - acc: 0.9917 - val_loss: 1.6474 - val_acc: 0.2658\n",
      "Epoch 14/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7461 - acc: 0.9917 - val_loss: 1.6107 - val_acc: 0.2834\n",
      "Epoch 15/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7449 - acc: 0.9915 - val_loss: 1.6027 - val_acc: 0.2828\n",
      "Epoch 16/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7438 - acc: 0.9919 - val_loss: 1.5888 - val_acc: 0.2916\n",
      "Epoch 17/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7426 - acc: 0.9922 - val_loss: 1.5615 - val_acc: 0.3042\n",
      "Epoch 18/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7416 - acc: 0.9925 - val_loss: 1.5384 - val_acc: 0.3189\n",
      "Epoch 19/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7408 - acc: 0.9923 - val_loss: 1.5228 - val_acc: 0.3277\n",
      "Epoch 20/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7399 - acc: 0.9923 - val_loss: 1.5058 - val_acc: 0.3397\n",
      "Epoch 21/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7391 - acc: 0.9922 - val_loss: 1.4942 - val_acc: 0.3494\n",
      "Epoch 22/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7383 - acc: 0.9927 - val_loss: 1.4793 - val_acc: 0.3577\n",
      "Epoch 23/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7376 - acc: 0.9929 - val_loss: 1.4694 - val_acc: 0.3685\n",
      "Epoch 24/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7369 - acc: 0.9932 - val_loss: 1.4461 - val_acc: 0.3853\n",
      "Epoch 25/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7361 - acc: 0.9935 - val_loss: 1.4303 - val_acc: 0.3880\n",
      "Epoch 26/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7354 - acc: 0.9938 - val_loss: 1.4123 - val_acc: 0.4031\n",
      "Epoch 27/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7350 - acc: 0.9941 - val_loss: 1.3995 - val_acc: 0.4143\n",
      "Epoch 28/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7343 - acc: 0.9941 - val_loss: 1.3867 - val_acc: 0.4208\n",
      "Epoch 29/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7339 - acc: 0.9940 - val_loss: 1.3710 - val_acc: 0.4240\n",
      "Epoch 30/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7334 - acc: 0.9940 - val_loss: 1.3750 - val_acc: 0.4209\n",
      "Epoch 31/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7331 - acc: 0.9942 - val_loss: 1.3493 - val_acc: 0.4410\n",
      "Epoch 32/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7326 - acc: 0.9945 - val_loss: 1.3333 - val_acc: 0.4527\n",
      "Epoch 33/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7322 - acc: 0.9946 - val_loss: 1.3469 - val_acc: 0.4409\n",
      "Epoch 34/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7320 - acc: 0.9943 - val_loss: 1.3001 - val_acc: 0.4823\n",
      "Epoch 35/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7316 - acc: 0.9943 - val_loss: 1.3247 - val_acc: 0.4549\n",
      "Epoch 36/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7313 - acc: 0.9944 - val_loss: 1.2791 - val_acc: 0.4989\n",
      "Epoch 37/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7310 - acc: 0.9946 - val_loss: 1.2862 - val_acc: 0.4877\n",
      "Epoch 38/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7308 - acc: 0.9944 - val_loss: 1.2711 - val_acc: 0.5010\n",
      "Epoch 39/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7306 - acc: 0.9948 - val_loss: 1.2662 - val_acc: 0.5079\n",
      "Epoch 40/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7304 - acc: 0.9946 - val_loss: 1.2673 - val_acc: 0.5080\n",
      "Epoch 41/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7301 - acc: 0.9948 - val_loss: 1.2551 - val_acc: 0.5102\n",
      "Epoch 42/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7298 - acc: 0.9951 - val_loss: 1.2458 - val_acc: 0.5226\n",
      "Epoch 43/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7297 - acc: 0.9949 - val_loss: 1.2335 - val_acc: 0.5369\n",
      "Epoch 44/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7296 - acc: 0.9950 - val_loss: 1.2453 - val_acc: 0.5225\n",
      "Epoch 45/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7294 - acc: 0.9950 - val_loss: 1.2305 - val_acc: 0.5339\n",
      "Epoch 46/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7292 - acc: 0.9951 - val_loss: 1.2232 - val_acc: 0.5364\n",
      "Epoch 47/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7291 - acc: 0.9952 - val_loss: 1.2312 - val_acc: 0.5298\n",
      "Epoch 48/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7289 - acc: 0.9952 - val_loss: 1.2136 - val_acc: 0.5407\n",
      "Epoch 49/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7289 - acc: 0.9950 - val_loss: 1.2184 - val_acc: 0.5407\n",
      "Epoch 50/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7286 - acc: 0.9951 - val_loss: 1.2098 - val_acc: 0.5449\n",
      "Epoch 51/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7285 - acc: 0.9952 - val_loss: 1.2072 - val_acc: 0.5404\n",
      "Epoch 52/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7284 - acc: 0.9954 - val_loss: 1.1965 - val_acc: 0.5573\n",
      "Epoch 53/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7284 - acc: 0.9955 - val_loss: 1.1991 - val_acc: 0.5518\n",
      "Epoch 54/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7282 - acc: 0.9956 - val_loss: 1.1877 - val_acc: 0.5629\n",
      "Epoch 55/100\n",
      "12116/12116 [==============================] - 0s 35us/step - loss: 0.7281 - acc: 0.9953 - val_loss: 1.1825 - val_acc: 0.5637\n",
      "Epoch 56/100\n",
      "12116/12116 [==============================] - 0s 36us/step - loss: 0.7279 - acc: 0.9958 - val_loss: 1.1849 - val_acc: 0.5545\n",
      "Epoch 57/100\n",
      "12116/12116 [==============================] - 0s 34us/step - loss: 0.7279 - acc: 0.9960 - val_loss: 1.1736 - val_acc: 0.5672\n",
      "Epoch 58/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7277 - acc: 0.9957 - val_loss: 1.1702 - val_acc: 0.5678\n",
      "Epoch 59/100\n",
      "12116/12116 [==============================] - 1s 72us/step - loss: 0.7278 - acc: 0.9957 - val_loss: 1.1507 - val_acc: 0.5857\n",
      "Epoch 60/100\n",
      "12116/12116 [==============================] - 1s 106us/step - loss: 0.7276 - acc: 0.9960 - val_loss: 1.1569 - val_acc: 0.5796\n",
      "Epoch 61/100\n",
      "12116/12116 [==============================] - 1s 94us/step - loss: 0.7274 - acc: 0.9956 - val_loss: 1.1543 - val_acc: 0.5824\n",
      "Epoch 62/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7274 - acc: 0.9960 - val_loss: 1.1489 - val_acc: 0.5867\n",
      "Epoch 63/100\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.7271 - acc: 0.9959 - val_loss: 1.1546 - val_acc: 0.5783\n",
      "Epoch 64/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7272 - acc: 0.9960 - val_loss: 1.1522 - val_acc: 0.5810\n",
      "Epoch 65/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7270 - acc: 0.9960 - val_loss: 1.1494 - val_acc: 0.5796\n",
      "Epoch 66/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7269 - acc: 0.9960 - val_loss: 1.1401 - val_acc: 0.5877\n",
      "Epoch 67/100\n",
      "12116/12116 [==============================] - 1s 60us/step - loss: 0.7268 - acc: 0.9958 - val_loss: 1.1410 - val_acc: 0.5846\n",
      "Epoch 68/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7267 - acc: 0.9960 - val_loss: 1.1471 - val_acc: 0.5758\n",
      "Epoch 69/100\n",
      "12116/12116 [==============================] - 1s 55us/step - loss: 0.7266 - acc: 0.9959 - val_loss: 1.1320 - val_acc: 0.5927\n",
      "Epoch 70/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7266 - acc: 0.9964 - val_loss: 1.1292 - val_acc: 0.5935\n",
      "Epoch 71/100\n",
      "12116/12116 [==============================] - 1s 82us/step - loss: 0.7265 - acc: 0.9964 - val_loss: 1.1204 - val_acc: 0.5972\n",
      "Epoch 72/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7265 - acc: 0.9961 - val_loss: 1.1223 - val_acc: 0.5940\n",
      "Epoch 73/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7263 - acc: 0.9962 - val_loss: 1.1193 - val_acc: 0.5944\n",
      "Epoch 74/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7262 - acc: 0.9965 - val_loss: 1.1082 - val_acc: 0.6142\n",
      "Epoch 75/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7261 - acc: 0.9962 - val_loss: 1.1069 - val_acc: 0.6071\n",
      "Epoch 76/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7261 - acc: 0.9963 - val_loss: 1.1091 - val_acc: 0.5999\n",
      "Epoch 77/100\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.7261 - acc: 0.9967 - val_loss: 1.1006 - val_acc: 0.6103\n",
      "Epoch 78/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7259 - acc: 0.9961 - val_loss: 1.0991 - val_acc: 0.6141\n",
      "Epoch 79/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7259 - acc: 0.9964 - val_loss: 1.1042 - val_acc: 0.6061\n",
      "Epoch 80/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7258 - acc: 0.9960 - val_loss: 1.1070 - val_acc: 0.6044\n",
      "Epoch 81/100\n",
      "12116/12116 [==============================] - 1s 73us/step - loss: 0.7259 - acc: 0.9960 - val_loss: 1.1087 - val_acc: 0.6010\n",
      "Epoch 82/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7256 - acc: 0.9963 - val_loss: 1.0914 - val_acc: 0.6188\n",
      "Epoch 83/100\n",
      "12116/12116 [==============================] - 1s 86us/step - loss: 0.7256 - acc: 0.9965 - val_loss: 1.1047 - val_acc: 0.6018\n",
      "Epoch 84/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7254 - acc: 0.9962 - val_loss: 1.0916 - val_acc: 0.6130\n",
      "Epoch 85/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7254 - acc: 0.9965 - val_loss: 1.1055 - val_acc: 0.5986\n",
      "Epoch 86/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7255 - acc: 0.9965 - val_loss: 1.0815 - val_acc: 0.6292\n",
      "Epoch 87/100\n",
      "12116/12116 [==============================] - 1s 62us/step - loss: 0.7254 - acc: 0.9965 - val_loss: 1.1044 - val_acc: 0.6028\n",
      "Epoch 88/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7253 - acc: 0.9964 - val_loss: 1.0909 - val_acc: 0.6130\n",
      "Epoch 89/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7251 - acc: 0.9967 - val_loss: 1.0916 - val_acc: 0.6120\n",
      "Epoch 90/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7250 - acc: 0.9965 - val_loss: 1.0944 - val_acc: 0.6038\n",
      "Epoch 91/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7251 - acc: 0.9960 - val_loss: 1.0875 - val_acc: 0.6145\n",
      "Epoch 92/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7251 - acc: 0.9965 - val_loss: 1.0903 - val_acc: 0.6117\n",
      "Epoch 93/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7250 - acc: 0.9965 - val_loss: 1.0882 - val_acc: 0.6089\n",
      "Epoch 94/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7250 - acc: 0.9965 - val_loss: 1.0912 - val_acc: 0.6106\n",
      "Epoch 95/100\n",
      "12116/12116 [==============================] - 1s 52us/step - loss: 0.7249 - acc: 0.9965 - val_loss: 1.0715 - val_acc: 0.6286\n",
      "Epoch 96/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7249 - acc: 0.9962 - val_loss: 1.0791 - val_acc: 0.6202\n",
      "Epoch 97/100\n",
      "12116/12116 [==============================] - 1s 62us/step - loss: 0.7248 - acc: 0.9966 - val_loss: 1.0793 - val_acc: 0.6188\n",
      "Epoch 98/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7247 - acc: 0.9962 - val_loss: 1.0711 - val_acc: 0.6213\n",
      "Epoch 99/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7248 - acc: 0.9964 - val_loss: 1.0795 - val_acc: 0.6143\n",
      "Epoch 100/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7246 - acc: 0.9964 - val_loss: 1.0752 - val_acc: 0.6182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15681ada0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train soft target model with training set omitting digit 3\n",
    "soft_78 = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "soft_78.fit(x_train_78, y_train_soft_78, batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12116/12116 [==============================] - 4s 314us/step - loss: 1.4717 - acc: 0.8966 - val_loss: 3.1774 - val_acc: 0.1925\n",
      "Epoch 2/100\n",
      "12116/12116 [==============================] - 1s 110us/step - loss: 0.8514 - acc: 0.9808 - val_loss: 2.7144 - val_acc: 0.1955\n",
      "Epoch 3/100\n",
      "12116/12116 [==============================] - 1s 82us/step - loss: 0.8034 - acc: 0.9891 - val_loss: 2.5803 - val_acc: 0.1967\n",
      "Epoch 4/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7909 - acc: 0.9908 - val_loss: 2.5080 - val_acc: 0.1974\n",
      "Epoch 5/100\n",
      "12116/12116 [==============================] - 1s 75us/step - loss: 0.7839 - acc: 0.9916 - val_loss: 2.4242 - val_acc: 0.1986\n",
      "Epoch 6/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7786 - acc: 0.9922 - val_loss: 2.4065 - val_acc: 0.2003\n",
      "Epoch 7/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7741 - acc: 0.9929 - val_loss: 2.3520 - val_acc: 0.2033\n",
      "Epoch 8/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7705 - acc: 0.9931 - val_loss: 2.3089 - val_acc: 0.2072\n",
      "Epoch 9/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7675 - acc: 0.9936 - val_loss: 2.2714 - val_acc: 0.2103\n",
      "Epoch 10/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7648 - acc: 0.9936 - val_loss: 2.2362 - val_acc: 0.2143\n",
      "Epoch 11/100\n",
      "12116/12116 [==============================] - 1s 70us/step - loss: 0.7627 - acc: 0.9939 - val_loss: 2.2003 - val_acc: 0.2175\n",
      "Epoch 12/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7606 - acc: 0.9941 - val_loss: 2.1660 - val_acc: 0.2246\n",
      "Epoch 13/100\n",
      "12116/12116 [==============================] - 1s 72us/step - loss: 0.7583 - acc: 0.9944 - val_loss: 2.1081 - val_acc: 0.2313\n",
      "Epoch 14/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7564 - acc: 0.9943 - val_loss: 2.0878 - val_acc: 0.2324\n",
      "Epoch 15/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7546 - acc: 0.9946 - val_loss: 2.0283 - val_acc: 0.2476\n",
      "Epoch 16/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7530 - acc: 0.9951 - val_loss: 2.0129 - val_acc: 0.2533\n",
      "Epoch 17/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7512 - acc: 0.9953 - val_loss: 1.9817 - val_acc: 0.2617\n",
      "Epoch 18/100\n",
      "12116/12116 [==============================] - 1s 52us/step - loss: 0.7497 - acc: 0.9954 - val_loss: 1.9586 - val_acc: 0.2733\n",
      "Epoch 19/100\n",
      "12116/12116 [==============================] - 1s 83us/step - loss: 0.7484 - acc: 0.9950 - val_loss: 1.9090 - val_acc: 0.2905\n",
      "Epoch 20/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7470 - acc: 0.9954 - val_loss: 1.8885 - val_acc: 0.3021\n",
      "Epoch 21/100\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.7461 - acc: 0.9960 - val_loss: 1.8663 - val_acc: 0.3118\n",
      "Epoch 22/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7448 - acc: 0.9956 - val_loss: 1.8397 - val_acc: 0.3300\n",
      "Epoch 23/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7438 - acc: 0.9959 - val_loss: 1.8317 - val_acc: 0.3287\n",
      "Epoch 24/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7426 - acc: 0.9957 - val_loss: 1.7921 - val_acc: 0.3515\n",
      "Epoch 25/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7414 - acc: 0.9959 - val_loss: 1.7836 - val_acc: 0.3571\n",
      "Epoch 26/100\n",
      "12116/12116 [==============================] - 1s 52us/step - loss: 0.7402 - acc: 0.9960 - val_loss: 1.7695 - val_acc: 0.3587\n",
      "Epoch 27/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7392 - acc: 0.9957 - val_loss: 1.7414 - val_acc: 0.3790\n",
      "Epoch 28/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7384 - acc: 0.9957 - val_loss: 1.7280 - val_acc: 0.3808\n",
      "Epoch 29/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7377 - acc: 0.9961 - val_loss: 1.7074 - val_acc: 0.3899\n",
      "Epoch 30/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7371 - acc: 0.9963 - val_loss: 1.6971 - val_acc: 0.3960\n",
      "Epoch 31/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7364 - acc: 0.9960 - val_loss: 1.6845 - val_acc: 0.4029\n",
      "Epoch 32/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7361 - acc: 0.9957 - val_loss: 1.6755 - val_acc: 0.4029\n",
      "Epoch 33/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7355 - acc: 0.9964 - val_loss: 1.6645 - val_acc: 0.4110\n",
      "Epoch 34/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7352 - acc: 0.9961 - val_loss: 1.6446 - val_acc: 0.4230\n",
      "Epoch 35/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7347 - acc: 0.9961 - val_loss: 1.6405 - val_acc: 0.4182\n",
      "Epoch 36/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7345 - acc: 0.9965 - val_loss: 1.6334 - val_acc: 0.4176\n",
      "Epoch 37/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7342 - acc: 0.9963 - val_loss: 1.6181 - val_acc: 0.4303\n",
      "Epoch 38/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7339 - acc: 0.9964 - val_loss: 1.6123 - val_acc: 0.4320\n",
      "Epoch 39/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7335 - acc: 0.9964 - val_loss: 1.5968 - val_acc: 0.4409\n",
      "Epoch 40/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7333 - acc: 0.9966 - val_loss: 1.5890 - val_acc: 0.4447\n",
      "Epoch 41/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7330 - acc: 0.9965 - val_loss: 1.5703 - val_acc: 0.4587\n",
      "Epoch 42/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7328 - acc: 0.9961 - val_loss: 1.5716 - val_acc: 0.4483\n",
      "Epoch 43/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7325 - acc: 0.9967 - val_loss: 1.5599 - val_acc: 0.4587\n",
      "Epoch 44/100\n",
      "12116/12116 [==============================] - 2s 137us/step - loss: 0.7323 - acc: 0.9965 - val_loss: 1.5419 - val_acc: 0.4716\n",
      "Epoch 45/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7319 - acc: 0.9967 - val_loss: 1.5337 - val_acc: 0.4799\n",
      "Epoch 46/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7317 - acc: 0.9965 - val_loss: 1.5360 - val_acc: 0.4721\n",
      "Epoch 47/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7314 - acc: 0.9965 - val_loss: 1.5236 - val_acc: 0.4796\n",
      "Epoch 48/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7313 - acc: 0.9966 - val_loss: 1.5133 - val_acc: 0.4893\n",
      "Epoch 49/100\n",
      "12116/12116 [==============================] - 1s 93us/step - loss: 0.7312 - acc: 0.9966 - val_loss: 1.5166 - val_acc: 0.4867\n",
      "Epoch 50/100\n",
      "12116/12116 [==============================] - 1s 64us/step - loss: 0.7310 - acc: 0.9966 - val_loss: 1.5087 - val_acc: 0.4923\n",
      "Epoch 51/100\n",
      "12116/12116 [==============================] - 1s 58us/step - loss: 0.7308 - acc: 0.9968 - val_loss: 1.5069 - val_acc: 0.4873\n",
      "Epoch 52/100\n",
      "12116/12116 [==============================] - 1s 92us/step - loss: 0.7307 - acc: 0.9971 - val_loss: 1.5043 - val_acc: 0.4873\n",
      "Epoch 53/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7306 - acc: 0.9969 - val_loss: 1.4903 - val_acc: 0.5030\n",
      "Epoch 54/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7304 - acc: 0.9967 - val_loss: 1.4883 - val_acc: 0.5033\n",
      "Epoch 55/100\n",
      "12116/12116 [==============================] - 1s 60us/step - loss: 0.7302 - acc: 0.9966 - val_loss: 1.4820 - val_acc: 0.5019\n",
      "Epoch 56/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7302 - acc: 0.9965 - val_loss: 1.4820 - val_acc: 0.5042\n",
      "Epoch 57/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7299 - acc: 0.9967 - val_loss: 1.4840 - val_acc: 0.5008\n",
      "Epoch 58/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7298 - acc: 0.9969 - val_loss: 1.4815 - val_acc: 0.5040\n",
      "Epoch 59/100\n",
      "12116/12116 [==============================] - 1s 81us/step - loss: 0.7298 - acc: 0.9969 - val_loss: 1.4737 - val_acc: 0.5095\n",
      "Epoch 60/100\n",
      "12116/12116 [==============================] - 1s 66us/step - loss: 0.7296 - acc: 0.9967 - val_loss: 1.4726 - val_acc: 0.5099\n",
      "Epoch 61/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7295 - acc: 0.9968 - val_loss: 1.4675 - val_acc: 0.5117\n",
      "Epoch 62/100\n",
      "12116/12116 [==============================] - 1s 98us/step - loss: 0.7294 - acc: 0.9966 - val_loss: 1.4583 - val_acc: 0.5168\n",
      "Epoch 63/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7292 - acc: 0.9969 - val_loss: 1.4433 - val_acc: 0.5330\n",
      "Epoch 64/100\n",
      "12116/12116 [==============================] - 1s 96us/step - loss: 0.7290 - acc: 0.9970 - val_loss: 1.4454 - val_acc: 0.5273\n",
      "Epoch 65/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7289 - acc: 0.9970 - val_loss: 1.4350 - val_acc: 0.5345\n",
      "Epoch 66/100\n",
      "12116/12116 [==============================] - 1s 78us/step - loss: 0.7287 - acc: 0.9969 - val_loss: 1.4380 - val_acc: 0.5307\n",
      "Epoch 67/100\n",
      "12116/12116 [==============================] - 1s 77us/step - loss: 0.7287 - acc: 0.9967 - val_loss: 1.4179 - val_acc: 0.5501\n",
      "Epoch 68/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7283 - acc: 0.9967 - val_loss: 1.4233 - val_acc: 0.5421\n",
      "Epoch 69/100\n",
      "12116/12116 [==============================] - 1s 79us/step - loss: 0.7283 - acc: 0.9969 - val_loss: 1.4262 - val_acc: 0.5348\n",
      "Epoch 70/100\n",
      "12116/12116 [==============================] - 1s 73us/step - loss: 0.7281 - acc: 0.9970 - val_loss: 1.4106 - val_acc: 0.5548\n",
      "Epoch 71/100\n",
      "12116/12116 [==============================] - 1s 84us/step - loss: 0.7278 - acc: 0.9971 - val_loss: 1.3971 - val_acc: 0.5590\n",
      "Epoch 72/100\n",
      "12116/12116 [==============================] - 1s 75us/step - loss: 0.7277 - acc: 0.9970 - val_loss: 1.3877 - val_acc: 0.5645\n",
      "Epoch 73/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7276 - acc: 0.9969 - val_loss: 1.3855 - val_acc: 0.5631\n",
      "Epoch 74/100\n",
      "12116/12116 [==============================] - 1s 88us/step - loss: 0.7275 - acc: 0.9970 - val_loss: 1.3854 - val_acc: 0.5640\n",
      "Epoch 75/100\n",
      "12116/12116 [==============================] - 1s 91us/step - loss: 0.7273 - acc: 0.9971 - val_loss: 1.3775 - val_acc: 0.5673\n",
      "Epoch 76/100\n",
      "12116/12116 [==============================] - 1s 80us/step - loss: 0.7272 - acc: 0.9970 - val_loss: 1.3728 - val_acc: 0.5705\n",
      "Epoch 77/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7271 - acc: 0.9969 - val_loss: 1.3608 - val_acc: 0.5816\n",
      "Epoch 78/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7270 - acc: 0.9969 - val_loss: 1.3653 - val_acc: 0.5753\n",
      "Epoch 79/100\n",
      "12116/12116 [==============================] - 1s 84us/step - loss: 0.7269 - acc: 0.9971 - val_loss: 1.3564 - val_acc: 0.5770\n",
      "Epoch 80/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7267 - acc: 0.9973 - val_loss: 1.3536 - val_acc: 0.5835\n",
      "Epoch 81/100\n",
      "12116/12116 [==============================] - 1s 95us/step - loss: 0.7267 - acc: 0.9971 - val_loss: 1.3424 - val_acc: 0.5913\n",
      "Epoch 82/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7265 - acc: 0.9971 - val_loss: 1.3438 - val_acc: 0.5857\n",
      "Epoch 83/100\n",
      "12116/12116 [==============================] - 1s 94us/step - loss: 0.7265 - acc: 0.9973 - val_loss: 1.3459 - val_acc: 0.5789\n",
      "Epoch 84/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7265 - acc: 0.9974 - val_loss: 1.3412 - val_acc: 0.5856\n",
      "Epoch 85/100\n",
      "12116/12116 [==============================] - 1s 75us/step - loss: 0.7263 - acc: 0.9974 - val_loss: 1.3393 - val_acc: 0.5878\n",
      "Epoch 86/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7264 - acc: 0.9970 - val_loss: 1.3284 - val_acc: 0.5929\n",
      "Epoch 87/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7262 - acc: 0.9972 - val_loss: 1.3272 - val_acc: 0.5973\n",
      "Epoch 88/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7262 - acc: 0.9973 - val_loss: 1.3272 - val_acc: 0.5950\n",
      "Epoch 89/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7262 - acc: 0.9974 - val_loss: 1.3172 - val_acc: 0.6009\n",
      "Epoch 90/100\n",
      "12116/12116 [==============================] - 1s 71us/step - loss: 0.7260 - acc: 0.9972 - val_loss: 1.3208 - val_acc: 0.5956\n",
      "Epoch 91/100\n",
      "12116/12116 [==============================] - 1s 64us/step - loss: 0.7259 - acc: 0.9974 - val_loss: 1.3108 - val_acc: 0.6095\n",
      "Epoch 92/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7259 - acc: 0.9973 - val_loss: 1.3163 - val_acc: 0.5970\n",
      "Epoch 93/100\n",
      "12116/12116 [==============================] - 1s 76us/step - loss: 0.7259 - acc: 0.9973 - val_loss: 1.3147 - val_acc: 0.6037\n",
      "Epoch 94/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7258 - acc: 0.9970 - val_loss: 1.3103 - val_acc: 0.6069\n",
      "Epoch 95/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7257 - acc: 0.9975 - val_loss: 1.3087 - val_acc: 0.6069\n",
      "Epoch 96/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7256 - acc: 0.9972 - val_loss: 1.2975 - val_acc: 0.6186\n",
      "Epoch 97/100\n",
      "12116/12116 [==============================] - 1s 69us/step - loss: 0.7257 - acc: 0.9971 - val_loss: 1.3142 - val_acc: 0.6010\n",
      "Epoch 98/100\n",
      "12116/12116 [==============================] - 1s 62us/step - loss: 0.7256 - acc: 0.9970 - val_loss: 1.3085 - val_acc: 0.6089\n",
      "Epoch 99/100\n",
      "12116/12116 [==============================] - 1s 67us/step - loss: 0.7255 - acc: 0.9974 - val_loss: 1.3045 - val_acc: 0.6123\n",
      "Epoch 100/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7254 - acc: 0.9974 - val_loss: 1.2919 - val_acc: 0.6241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15519a668>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "T = 5\n",
    "w = 0.5 / (T**2) \n",
    "\n",
    "# apply both hard & soft targets to learn\n",
    "y_hard_soft_train_78 = np.concatenate((y_train_78, y_train_soft_78), axis=1)\n",
    "\n",
    "# student net - WITH distillation & weighted hard soft loss - digit 3 omitted in the transfer set\n",
    "mix_78 = Sequential()\n",
    "mix_78.add(Dense(n_hidden, name='hidden_1', input_shape=(28*28, ), activation='relu'))\n",
    "mix_78.add(Dense(n_hidden, name='hidden_2', activation='relu'))\n",
    "mix_78.add(Dense(10, name='logit'))\n",
    "\n",
    "# y_pred at the end of 10-node dense layer is the logit_pred\n",
    "mix_78.compile(loss=lambda y_true, y_pred: avg_mix_loss(y_true, y_pred, w, T), \n",
    "               optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "mix_78.fit(x_train_78, y_hard_soft_train_78, \n",
    "           batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_hard_soft_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12116 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12116/12116 [==============================] - 2s 158us/step - loss: 1.1857 - acc: 0.9439 - val_loss: 2.7235 - val_acc: 0.1951\n",
      "Epoch 2/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.8168 - acc: 0.9835 - val_loss: 2.4055 - val_acc: 0.1966\n",
      "Epoch 3/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7902 - acc: 0.9879 - val_loss: 2.2500 - val_acc: 0.1972\n",
      "Epoch 4/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7781 - acc: 0.9889 - val_loss: 2.1407 - val_acc: 0.1979\n",
      "Epoch 5/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7688 - acc: 0.9896 - val_loss: 1.9956 - val_acc: 0.1990\n",
      "Epoch 6/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7621 - acc: 0.9907 - val_loss: 1.9205 - val_acc: 0.2064\n",
      "Epoch 7/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7574 - acc: 0.9910 - val_loss: 1.8677 - val_acc: 0.2155\n",
      "Epoch 8/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7539 - acc: 0.9914 - val_loss: 1.8046 - val_acc: 0.2334\n",
      "Epoch 9/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7506 - acc: 0.9917 - val_loss: 1.7750 - val_acc: 0.2409\n",
      "Epoch 10/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7482 - acc: 0.9920 - val_loss: 1.7308 - val_acc: 0.2534\n",
      "Epoch 11/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7460 - acc: 0.9920 - val_loss: 1.6992 - val_acc: 0.2588\n",
      "Epoch 12/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7443 - acc: 0.9926 - val_loss: 1.6775 - val_acc: 0.2617\n",
      "Epoch 13/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7426 - acc: 0.9925 - val_loss: 1.6517 - val_acc: 0.2693\n",
      "Epoch 14/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7417 - acc: 0.9931 - val_loss: 1.6111 - val_acc: 0.2863\n",
      "Epoch 15/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7401 - acc: 0.9931 - val_loss: 1.5931 - val_acc: 0.2908\n",
      "Epoch 16/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7391 - acc: 0.9933 - val_loss: 1.5818 - val_acc: 0.2927\n",
      "Epoch 17/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7383 - acc: 0.9932 - val_loss: 1.5364 - val_acc: 0.3159\n",
      "Epoch 18/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7373 - acc: 0.9936 - val_loss: 1.5101 - val_acc: 0.3225\n",
      "Epoch 19/100\n",
      "12116/12116 [==============================] - 1s 65us/step - loss: 0.7366 - acc: 0.9937 - val_loss: 1.4798 - val_acc: 0.3437\n",
      "Epoch 20/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7358 - acc: 0.9941 - val_loss: 1.4584 - val_acc: 0.3529\n",
      "Epoch 21/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7352 - acc: 0.9940 - val_loss: 1.4392 - val_acc: 0.3604\n",
      "Epoch 22/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7344 - acc: 0.9941 - val_loss: 1.4249 - val_acc: 0.3673\n",
      "Epoch 23/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7338 - acc: 0.9944 - val_loss: 1.4007 - val_acc: 0.3840\n",
      "Epoch 24/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7333 - acc: 0.9944 - val_loss: 1.3750 - val_acc: 0.4071\n",
      "Epoch 25/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7328 - acc: 0.9943 - val_loss: 1.3739 - val_acc: 0.3989\n",
      "Epoch 26/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7324 - acc: 0.9947 - val_loss: 1.3493 - val_acc: 0.4254\n",
      "Epoch 27/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7319 - acc: 0.9946 - val_loss: 1.3388 - val_acc: 0.4246\n",
      "Epoch 28/100\n",
      "12116/12116 [==============================] - 0s 37us/step - loss: 0.7314 - acc: 0.9944 - val_loss: 1.3160 - val_acc: 0.4396\n",
      "Epoch 29/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7311 - acc: 0.9946 - val_loss: 1.3113 - val_acc: 0.4490\n",
      "Epoch 30/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7307 - acc: 0.9944 - val_loss: 1.3019 - val_acc: 0.4496\n",
      "Epoch 31/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7303 - acc: 0.9945 - val_loss: 1.2916 - val_acc: 0.4579\n",
      "Epoch 32/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7300 - acc: 0.9947 - val_loss: 1.2824 - val_acc: 0.4624\n",
      "Epoch 33/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7298 - acc: 0.9945 - val_loss: 1.2597 - val_acc: 0.4820\n",
      "Epoch 34/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7296 - acc: 0.9950 - val_loss: 1.2578 - val_acc: 0.4777\n",
      "Epoch 35/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7292 - acc: 0.9953 - val_loss: 1.2347 - val_acc: 0.5060\n",
      "Epoch 36/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7291 - acc: 0.9951 - val_loss: 1.2388 - val_acc: 0.4992\n",
      "Epoch 37/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7287 - acc: 0.9951 - val_loss: 1.2439 - val_acc: 0.4876\n",
      "Epoch 38/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7286 - acc: 0.9951 - val_loss: 1.2358 - val_acc: 0.4965\n",
      "Epoch 39/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7284 - acc: 0.9947 - val_loss: 1.2146 - val_acc: 0.5168\n",
      "Epoch 40/100\n",
      "12116/12116 [==============================] - 1s 68us/step - loss: 0.7282 - acc: 0.9953 - val_loss: 1.2164 - val_acc: 0.5105\n",
      "Epoch 41/100\n",
      "12116/12116 [==============================] - 1s 63us/step - loss: 0.7281 - acc: 0.9954 - val_loss: 1.2115 - val_acc: 0.5148\n",
      "Epoch 42/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7279 - acc: 0.9957 - val_loss: 1.1935 - val_acc: 0.5381\n",
      "Epoch 43/100\n",
      "12116/12116 [==============================] - 1s 59us/step - loss: 0.7279 - acc: 0.9953 - val_loss: 1.1997 - val_acc: 0.5235\n",
      "Epoch 44/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7276 - acc: 0.9950 - val_loss: 1.1819 - val_acc: 0.5383\n",
      "Epoch 45/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7274 - acc: 0.9953 - val_loss: 1.1809 - val_acc: 0.5380\n",
      "Epoch 46/100\n",
      "12116/12116 [==============================] - 1s 74us/step - loss: 0.7273 - acc: 0.9955 - val_loss: 1.1855 - val_acc: 0.5288\n",
      "Epoch 47/100\n",
      "12116/12116 [==============================] - 1s 53us/step - loss: 0.7272 - acc: 0.9954 - val_loss: 1.1608 - val_acc: 0.5568\n",
      "Epoch 48/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7271 - acc: 0.9951 - val_loss: 1.1711 - val_acc: 0.5418\n",
      "Epoch 49/100\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.7270 - acc: 0.9954 - val_loss: 1.1627 - val_acc: 0.5519\n",
      "Epoch 50/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7268 - acc: 0.9954 - val_loss: 1.1500 - val_acc: 0.5686\n",
      "Epoch 51/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7267 - acc: 0.9957 - val_loss: 1.1531 - val_acc: 0.5630\n",
      "Epoch 52/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7267 - acc: 0.9954 - val_loss: 1.1511 - val_acc: 0.5603\n",
      "Epoch 53/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7265 - acc: 0.9951 - val_loss: 1.1462 - val_acc: 0.5631\n",
      "Epoch 54/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7264 - acc: 0.9955 - val_loss: 1.1280 - val_acc: 0.5838\n",
      "Epoch 55/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7263 - acc: 0.9955 - val_loss: 1.1308 - val_acc: 0.5820\n",
      "Epoch 56/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7262 - acc: 0.9957 - val_loss: 1.1296 - val_acc: 0.5783\n",
      "Epoch 57/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7261 - acc: 0.9958 - val_loss: 1.1392 - val_acc: 0.5654\n",
      "Epoch 58/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7259 - acc: 0.9959 - val_loss: 1.1230 - val_acc: 0.5842\n",
      "Epoch 59/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7258 - acc: 0.9957 - val_loss: 1.1268 - val_acc: 0.5737\n",
      "Epoch 60/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7258 - acc: 0.9956 - val_loss: 1.1162 - val_acc: 0.5952\n",
      "Epoch 61/100\n",
      "12116/12116 [==============================] - 0s 38us/step - loss: 0.7257 - acc: 0.9960 - val_loss: 1.1146 - val_acc: 0.5917\n",
      "Epoch 62/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7256 - acc: 0.9958 - val_loss: 1.1011 - val_acc: 0.6008\n",
      "Epoch 63/100\n",
      "12116/12116 [==============================] - 1s 41us/step - loss: 0.7255 - acc: 0.9960 - val_loss: 1.1063 - val_acc: 0.5917\n",
      "Epoch 64/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7255 - acc: 0.9960 - val_loss: 1.1095 - val_acc: 0.5871\n",
      "Epoch 65/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7254 - acc: 0.9961 - val_loss: 1.0997 - val_acc: 0.6020\n",
      "Epoch 66/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7253 - acc: 0.9962 - val_loss: 1.0974 - val_acc: 0.6036\n",
      "Epoch 67/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7252 - acc: 0.9961 - val_loss: 1.1014 - val_acc: 0.5965\n",
      "Epoch 68/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7251 - acc: 0.9965 - val_loss: 1.0910 - val_acc: 0.6101\n",
      "Epoch 69/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7251 - acc: 0.9960 - val_loss: 1.0862 - val_acc: 0.6142\n",
      "Epoch 70/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7250 - acc: 0.9963 - val_loss: 1.0899 - val_acc: 0.6071\n",
      "Epoch 71/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7249 - acc: 0.9963 - val_loss: 1.0895 - val_acc: 0.6066\n",
      "Epoch 72/100\n",
      "12116/12116 [==============================] - 1s 49us/step - loss: 0.7249 - acc: 0.9966 - val_loss: 1.0904 - val_acc: 0.6071\n",
      "Epoch 73/100\n",
      "12116/12116 [==============================] - 1s 57us/step - loss: 0.7248 - acc: 0.9965 - val_loss: 1.0833 - val_acc: 0.6128\n",
      "Epoch 74/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7248 - acc: 0.9965 - val_loss: 1.0821 - val_acc: 0.6143\n",
      "Epoch 75/100\n",
      "12116/12116 [==============================] - 1s 43us/step - loss: 0.7247 - acc: 0.9966 - val_loss: 1.0708 - val_acc: 0.6232\n",
      "Epoch 76/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7246 - acc: 0.9967 - val_loss: 1.0802 - val_acc: 0.6112\n",
      "Epoch 77/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7245 - acc: 0.9963 - val_loss: 1.0816 - val_acc: 0.6074\n",
      "Epoch 78/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7246 - acc: 0.9966 - val_loss: 1.0781 - val_acc: 0.6122\n",
      "Epoch 79/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7244 - acc: 0.9965 - val_loss: 1.0716 - val_acc: 0.6245\n",
      "Epoch 80/100\n",
      "12116/12116 [==============================] - 1s 61us/step - loss: 0.7246 - acc: 0.9965 - val_loss: 1.0750 - val_acc: 0.6102\n",
      "Epoch 81/100\n",
      "12116/12116 [==============================] - 1s 54us/step - loss: 0.7244 - acc: 0.9965 - val_loss: 1.0701 - val_acc: 0.6215\n",
      "Epoch 82/100\n",
      "12116/12116 [==============================] - 1s 50us/step - loss: 0.7243 - acc: 0.9965 - val_loss: 1.0655 - val_acc: 0.6270\n",
      "Epoch 83/100\n",
      "12116/12116 [==============================] - 1s 47us/step - loss: 0.7242 - acc: 0.9966 - val_loss: 1.0671 - val_acc: 0.6170\n",
      "Epoch 84/100\n",
      "12116/12116 [==============================] - 1s 46us/step - loss: 0.7242 - acc: 0.9968 - val_loss: 1.0557 - val_acc: 0.6298\n",
      "Epoch 85/100\n",
      "12116/12116 [==============================] - 1s 51us/step - loss: 0.7242 - acc: 0.9965 - val_loss: 1.0649 - val_acc: 0.6263\n",
      "Epoch 86/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7243 - acc: 0.9965 - val_loss: 1.0625 - val_acc: 0.6266\n",
      "Epoch 87/100\n",
      "12116/12116 [==============================] - 1s 56us/step - loss: 0.7242 - acc: 0.9962 - val_loss: 1.0639 - val_acc: 0.6226\n",
      "Epoch 88/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7241 - acc: 0.9968 - val_loss: 1.0573 - val_acc: 0.6279\n",
      "Epoch 89/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7241 - acc: 0.9967 - val_loss: 1.0604 - val_acc: 0.6205\n",
      "Epoch 90/100\n",
      "12116/12116 [==============================] - 1s 42us/step - loss: 0.7241 - acc: 0.9968 - val_loss: 1.0581 - val_acc: 0.6277\n",
      "Epoch 91/100\n",
      "12116/12116 [==============================] - 1s 48us/step - loss: 0.7240 - acc: 0.9965 - val_loss: 1.0475 - val_acc: 0.6412\n",
      "Epoch 92/100\n",
      "12116/12116 [==============================] - 0s 40us/step - loss: 0.7239 - acc: 0.9964 - val_loss: 1.0507 - val_acc: 0.6291\n",
      "Epoch 93/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7239 - acc: 0.9969 - val_loss: 1.0493 - val_acc: 0.6455\n",
      "Epoch 94/100\n",
      "12116/12116 [==============================] - 1s 44us/step - loss: 0.7239 - acc: 0.9969 - val_loss: 1.0559 - val_acc: 0.6333\n",
      "Epoch 95/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7238 - acc: 0.9967 - val_loss: 1.0523 - val_acc: 0.6356\n",
      "Epoch 96/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7238 - acc: 0.9965 - val_loss: 1.0415 - val_acc: 0.6445\n",
      "Epoch 97/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7238 - acc: 0.9967 - val_loss: 1.0436 - val_acc: 0.6428\n",
      "Epoch 98/100\n",
      "12116/12116 [==============================] - 0s 39us/step - loss: 0.7237 - acc: 0.9969 - val_loss: 1.0450 - val_acc: 0.6358\n",
      "Epoch 99/100\n",
      "12116/12116 [==============================] - 0s 41us/step - loss: 0.7237 - acc: 0.9969 - val_loss: 1.0400 - val_acc: 0.6423\n",
      "Epoch 100/100\n",
      "12116/12116 [==============================] - 1s 45us/step - loss: 0.7237 - acc: 0.9969 - val_loss: 1.0510 - val_acc: 0.6302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1591e5f60>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_78_bias = MNIST_StudentNet(n_hidden=20, T=3)\n",
    "bias = soft_78_bias.layers[2].get_weights()[1]\n",
    "bias[7] = 1\n",
    "bias[8] = 1\n",
    "K.set_value(soft_78_bias.layers[2].bias, bias)\n",
    "\n",
    "soft_78_bias.fit(x_train_78, y_train_soft_78, \n",
    "                 batch_size=128, epochs=100,verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Accuracy on Test set === \n",
      "\n",
      "NO DISTILLATION\n",
      "loss = 11.931765022277832, accuracy = 0.1986, #errors = 8014\n",
      "\n",
      "WITH DISTILLATION\n",
      "loss = 1.07515452003479, accuracy = 0.6182, #errors = 3818\n",
      "\n",
      "WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET\n",
      "loss = 1.2918906003952026, accuracy = 0.6241, #errors = 3759\n",
      "\n",
      "WITH DISTILLATION & BIAS TUNED\n",
      "loss = 1.050957187271118, accuracy = 0.6302, #errors = 3698\n"
     ]
    }
   ],
   "source": [
    "print('=== Overall Accuracy on Test set === \\n')\n",
    "\n",
    "loss, accuracy = hard_78.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('NO DISTILLATION')\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION')\n",
    "loss, accuracy = soft_78.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION & WEIGHTED HARD-SOFT TARGET')\n",
    "loss, accuracy = mix_78.evaluate(x_test, y_hard_soft_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n",
    "print()\n",
    "\n",
    "print('WITH DISTILLATION & BIAS TUNED')\n",
    "loss, accuracy = soft_78_bias.evaluate(x_test, y_test, verbose=0)\n",
    "num_errors = int((1 - accuracy) * len(x_test))\n",
    "print('loss = {}, accuracy = {}, #errors = {}'.format(loss, accuracy, num_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary performaces of distilled student nets which have ONLY SEEN digit 7 & 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Only 7 & 8 Distillation Performance on Full Test Set ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>num_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.1986</td>\n",
       "      <td>11.931765</td>\n",
       "      <td>8014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_tuned</th>\n",
       "      <td>0.6302</td>\n",
       "      <td>1.050957</td>\n",
       "      <td>3698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled</th>\n",
       "      <td>0.6182</td>\n",
       "      <td>1.075155</td>\n",
       "      <td>3818.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_distilled_mix_loss</th>\n",
       "      <td>0.6241</td>\n",
       "      <td>1.291891</td>\n",
       "      <td>3759.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy       loss  num_error\n",
       "baseline                    0.1986  11.931765     8014.0\n",
       "bias_tuned                  0.6302   1.050957     3698.0\n",
       "dense_distilled             0.6182   1.075155     3818.0\n",
       "dense_distilled_mix_loss    0.6241   1.291891     3759.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep78_summary = {\n",
    "    'baseline': {'loss': 11.931765022277832, 'accuracy': 0.1986, 'num_error': 8014},\n",
    "    'dense_distilled': {'loss': 1.07515452003479, 'accuracy': 0.6182, 'num_error': 3818},\n",
    "    'dense_distilled_mix_loss': {'loss': 1.2918906003952026, 'accuracy': 0.6241, 'num_error': 3759},\n",
    "    'bias_tuned': {'loss': 1.050957187271118, 'accuracy': 0.6302, 'num_error': 3698}\n",
    "}\n",
    "\n",
    "print('=== Only 7 & 8 Distillation Performance on Full Test Set ===')\n",
    "df_keep78_summary = pd.DataFrame().from_dict(keep78_summary).T\n",
    "display(df_keep78_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
